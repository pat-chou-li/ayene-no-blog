import{_ as c}from"./ValaxyMain.vue_vue_type_style_index_0_lang.BroTQeye.js";import"./chunks/@vueuse/motion.CDGXpttC.js";import{e as E,u as g,a as u}from"./chunks/vue-router.DlUPz4t8.js";import{aa as m,ap as a,ag as t,af as i,ai as s,O as y,ab as f,a1 as b}from"./framework.BWK1Zvdg.js";import"./app.5Pc5wqE0.js";import"./chunks/dayjs.BldX5ftQ.js";import"./chunks/vue-i18n.lF0SUo7_.js";import"./chunks/pinia.BYsrTFz1.js";/* empty css                    */import"./chunks/nprogress.DxEoPAls.js";import"./YunComment.vue_vue_type_style_index_0_lang.BTzG1k4p.js";import"./index.C5okkQwF.js";import"./YunPageHeader.vue_vue_type_script_setup_true_lang.DB3O5jFY.js";import"./post.B8Hql8K0.js";const L="/ayene-no-blog/assets/2-0.B-kP4mJM.jpg",A="/ayene-no-blog/assets/2-1.C_UGwZOT.jpg",M="/ayene-no-blog/assets/2-2.BaGuuOEh.jpg",_="/ayene-no-blog/assets/3-1.B2OYP1ye.png",D="/ayene-no-blog/assets/image-20250720224305641.BY_JyZU4.png",F="/ayene-no-blog/assets/image-20250720224441025.IsENbXzA.png",B="/ayene-no-blog/assets/jumpstart-fm-rag.DJ7Uanfr.jpg",v="/ayene-no-blog/assets/7-3-Agent_E5_B7_A5_E4_BD_9C_E5_8E_9F_E7_90_86.BATTr5fI.jpeg",P=E("/posts/随笔/LLM相关",async n=>JSON.parse('{"title":"LLM相关","description":"","frontmatter":{"title":"LLM相关","date":"2025.07.20","categories":"LLM","tags":["LLM"]},"headers":[],"relativePath":"pages/posts/随笔/LLM相关.md","lastUpdated":1755853346000}'),{lazy:(n,e)=>n.name===e.name}),H={__name:"LLM相关",setup(n,{expose:e}){const{data:h}=P(),p=u(),k=g(),r=Object.assign(k.meta.frontmatter||{},h.value?.frontmatter||{});return p.currentRoute.value.data=h.value,b("valaxy:frontmatter",r),globalThis.$frontmatter=r,e({frontmatter:{title:"LLM相关",date:"2025.07.20",categories:"LLM",tags:["LLM"]}}),(l,o)=>{const d=c;return f(),m(d,{frontmatter:y(r)},{"main-content-md":a(()=>[...o[0]||(o[0]=[i("h2",{id:"一-llm训练流程",tabindex:"-1"},[s("一. LLM训练流程 "),i("a",{class:"header-anchor",href:"#一-llm训练流程","aria-label":'Permalink to "一. LLM训练流程"'},"​")],-1),i("figure",null,[i("img",{src:L,alt:"alt text",loading:"lazy",decoding:"async"})],-1),i("p",null,"训练出LLM一般需要经过Pretrain、SFT、RLHF三个阶段。",-1),i("p",null,"pretrain",-1),i("p",null,"sft",-1),i("p",null,"rlhf",-1),i("h3",{id:"pretrain",tabindex:"-1"},[s("pretrain "),i("a",{class:"header-anchor",href:"#pretrain","aria-label":'Permalink to "pretrain"'},"​")],-1),i("ul",null,[i("li",null,"模型架构：各类基于transformer的decoder-only架构"),i("li",null,"任务制定：CLM为主"),i("li",null,"计算需求：分布式训练方法"),i("li",null,"数据需求：互联网上的海量文本")],-1),i("h4",{id:"任务制定",tabindex:"-1"},[s("任务制定 "),i("a",{class:"header-anchor",href:"#任务制定","aria-label":'Permalink to "任务制定"'},"​")],-1),i("p",null,"pretrain的数据集自然是互联网上的海量文本，但是既然要训练，自然需要一个任务，并且根据这项任务对数据进行处理，现在最常用的方法是CLM（Causal Language Model，因果语言模型）。",-1),i("p",null,"CLM：根据前面所有的token来预测下一个token。（现在的主流，Lamma，gpt，qwen等）",-1),i("div",{class:"language-tex"},[i("button",{title:"Copy code",class:"copy"}),i("span",{class:"lang"},"tex"),i("pre",{class:"shiki shiki-themes github-light github-dark vp-code"},[i("code",{"v-pre":""},[i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"input: 今天天气")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"output: 今天天气很")]),s(`
`),i("span",{class:"line"}),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"input: 今天天气很")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"output：今天天气很好")])])]),i("button",{class:"code-block-unfold-btn"})],-1),i("p",null,"在实践上，在计算transformer的时候通过掩码注意力机制，将后续的注意力权重调整为0，使得每个token只能关注前面的token。（即在训练时对模型加mask，并不是对输入加mask使得文本不可见）",-1),i("div",{class:"language-mark"},[i("button",{title:"Copy code",class:"copy"}),i("span",{class:"lang"},"mark"),i("pre",{class:"shiki shiki-themes github-light github-dark vp-code"},[i("code",{"v-pre":""},[i("span",{class:"line"},[i("span",null," 【MASK】【MASK】【MASK】【MASK】")]),s(`
`),i("span",{class:"line"},[i("span",null,"    I   【MASK】 【MASK】【MASK】")]),s(`
`),i("span",{class:"line"},[i("span",null,"    I     like  【MASK】【MASK】")]),s(`
`),i("span",{class:"line"},[i("span",null,"    I     like    you  【MASK】")]),s(`
`),i("span",{class:"line"},[i("span",null,"    I     like    you   .")])])]),i("button",{class:"code-block-unfold-btn"})],-1),i("p",null,"其他并不主流的方式，但仍然存在",-1),i("p",null,"MLM：随机屏蔽部分token，让模型完成类似完形填空的任务。",-1),i("div",{class:"language-markdown"},[i("button",{title:"Copy code",class:"copy"}),i("span",{class:"lang"},"markdown"),i("pre",{class:"shiki shiki-themes github-light github-dark vp-code"},[i("code",{"v-pre":""},[i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"输入：I <"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"MASK"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"> you because you are <"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"MASK"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},">")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"输出：<"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"MASK"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"> - love; <"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"MASK"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"> - wonderful")])])]),i("button",{class:"code-block-unfold-btn"})],-1),i("p",null,"NSP：输入两个句子，判断是否构成上下文。",-1),i("h4",{id:"计算需求",tabindex:"-1"},[s("计算需求 "),i("a",{class:"header-anchor",href:"#计算需求","aria-label":'Permalink to "计算需求"'},"​")],-1),i("p",null,[s("大模型的显存开销显然是单张GPU无法接受的，在实践中，需要将模型参数、模型梯度、Adam状态参数等等进行分片，例如可以将两个batch放到两张gpu上，然后通过通信共享梯度，完成类似batch_size的并行训练，被称为"),i("strong",null,"Data Parallelism（数据并行）"),s("：")],-1),i("img",{src:A,alt:"alt text",style:{zoom:"67%"}},null,-1),i("p",null,"例如将模型参数本身放到不同的GPU上：",-1),i("img",{src:M,alt:"alt text",style:{zoom:"50%"}},null,-1),i("p",null,"显而易见的，越是高的分片，带来越高的通信开销，因此哪些内容适合分片，哪些内容适合所有GPU上对齐。",-1),i("p",null,"除了分片以外，还有很多手段降低显存开销",-1),i("ul",null,[i("li",null,"CPU-offline：将部分计算、缓存转移至CPU"),i("li",null,"算子优化：针对Transformer优化矩阵乘法等算子，减少中间矩阵，降低显存开销"),i("li",null,"量化、压缩：例如对梯度进行压缩后再通信，以梯度精度损失为代价降低通信"),i("li",null,"…")],-1),i("h4",{id:"数据需求",tabindex:"-1"},[s("数据需求 "),i("a",{class:"header-anchor",href:"#数据需求","aria-label":'Permalink to "数据需求"'},"​")],-1),i("p",null,"如何从互联网中获取高质量语料用于训练？（tips：目前各公司并不完全开源其高质量语料库）",-1),i("ul",null,[i("li",null,"文档准备：通过爬虫、简单的URL过滤、语种过滤等，收集大量文本数据。"),i("li",null,[s("语料过滤：去除低质量、无意义、有害内容，如乱码广告等，方法包括： "),i("ul",null,[i("li",null,"通过高质量语料库训练一个文本分类器"),i("li",null,"人工标定")])]),i("li",null,"语料去重：hash计算相似性、模式匹配等")],-1),i("h3",{id:"sft-监督微调",tabindex:"-1"},[s("SFT（监督微调） "),i("a",{class:"header-anchor",href:"#sft-监督微调","aria-label":'Permalink to "SFT（监督微调）"'},"​")],-1),i("h4",{id:"指令微调",tabindex:"-1"},[s("指令微调 "),i("a",{class:"header-anchor",href:"#指令微调","aria-label":'Permalink to "指令微调"'},"​")],-1),i("p",null,"大模型SFT与传统中对模型续训、微调的方法并不相同。",-1),i("p",null,"传统上，我们想让一个模型能完成特定的下游任务的任务，例如要完成机器翻译，我们就提供机器翻译的数据集，修改模型的输出，然后进行续训。",-1),i("p",null,"但是LLM微调时，为了增强其泛化性，采用的是指令微调的方式，输入是各种指令，输出则是相应的回复，数据集类似于：",-1),i("div",{class:"language-json"},[i("button",{title:"Copy code",class:"copy"}),i("span",{class:"lang"},"json"),i("pre",{class:"shiki shiki-themes github-light github-dark vp-code"},[i("code",{"v-pre":""},[i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"input : {")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#B31D28","--shiki-light-font-style":"italic","--shiki-dark":"#FDAEB7","--shiki-dark-font-style":"italic"}},"	system"),i("span",{style:{"--shiki-light":"#B31D28","--shiki-light-font-style":"italic","--shiki-dark":"#FDAEB7","--shiki-dark-font-style":"italic"}}," prompt"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},": "),i("span",{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"}},'"你是一个helpful assistant"'),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},",")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#B31D28","--shiki-light-font-style":"italic","--shiki-dark":"#FDAEB7","--shiki-dark-font-style":"italic"}},"    user"),i("span",{style:{"--shiki-light":"#B31D28","--shiki-light-font-style":"italic","--shiki-dark":"#FDAEB7","--shiki-dark-font-style":"italic"}}," prompt"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},": "),i("span",{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"}},'"告诉我今天的天气预报"'),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},",")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#B31D28","--shiki-light-font-style":"italic","--shiki-dark":"#FDAEB7","--shiki-dark-font-style":"italic"}},"    assistant"),i("span",{style:{"--shiki-light":"#B31D28","--shiki-light-font-style":"italic","--shiki-dark":"#FDAEB7","--shiki-dark-font-style":"italic"}}," prompt"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},": "),i("span",{style:{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"}},'"根据天气预报，今天天气是晴转多云，最高温度26摄氏度，最低温度9摄氏度，昼夜温差大，请注意保暖哦"')]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"}")])])]),i("button",{class:"code-block-unfold-btn"})],-1),i("p",null,"输入的是一条用户指令，大部分时候会在user prompt前拼接一条system prompt，而assistant prompt就是输出，也就是大模型计算损失时的gt。",-1),i("p",null,"以上是单次会话的训练实例，为了让LLM能够执行多轮对话，SFT中也进行多轮对话的训练，其输入输出为：",-1),i("div",{class:"language-markdown"},[i("button",{title:"Copy code",class:"copy"}),i("span",{class:"lang"},"markdown"),i("pre",{class:"shiki shiki-themes github-light github-dark vp-code"},[i("code",{"v-pre":""},[i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}}," input=<"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"prompt_1"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"><"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"completion_1"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"><"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"prompt_2"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"><"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"completion_2"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"><"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"prompt_3"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"><"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"completion_3"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},">")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}}," output=["),i("span",{style:{"--shiki-light":"#032F62","--shiki-light-text-decoration":"underline","--shiki-dark":"#DBEDFF","--shiki-dark-text-decoration":"underline"}},"MASK"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"]<"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"completion_1"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},">["),i("span",{style:{"--shiki-light":"#032F62","--shiki-light-text-decoration":"underline","--shiki-dark":"#DBEDFF","--shiki-dark-text-decoration":"underline"}},"MASK"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"]<"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"completion_2"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},">["),i("span",{style:{"--shiki-light":"#032F62","--shiki-light-text-decoration":"underline","--shiki-dark":"#DBEDFF","--shiki-dark-text-decoration":"underline"}},"MASK"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"]<"),i("span",{style:{"--shiki-light":"#22863A","--shiki-dark":"#85E89D"}},"completion_3"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},">")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}}," ")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}}," ["),i("span",{style:{"--shiki-light":"#032F62","--shiki-light-text-decoration":"underline","--shiki-dark":"#DBEDFF","--shiki-dark-text-decoration":"underline"}},"MASK"),i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"]是指不参与loss计算的部分")])])]),i("button",{class:"code-block-unfold-btn"})],-1),i("h4",{id:"高效微调",tabindex:"-1"},[s("高效微调 "),i("a",{class:"header-anchor",href:"#高效微调","aria-label":'Permalink to "高效微调"'},"​")],-1),i("p",null,"其实可以发现，前面的pretrain和指令微调，都可以理解为正在训练大模型的通用能力，指令微调也需要极高的训练成本，到端侧部署应用时，通常使用的是高效微调方案。",-1),i("h5",{id:"adapter-tuning",tabindex:"-1"},[s("Adapter Tuning "),i("a",{class:"header-anchor",href:"#adapter-tuning","aria-label":'Permalink to "Adapter Tuning"'},"​")],-1),i("figure",null,[i("img",{src:_,alt:"alt text",loading:"lazy",decoding:"async"})],-1),i("p",null,"在transformer层之间插入一个小型adapter，冻结原有参数，仅训练这些参数。Feedforwarddown-project会将输入维度大幅降低，并在输出时还原。",-1),i("h5",{id:"lora",tabindex:"-1"},[s("Lora "),i("a",{class:"header-anchor",href:"#lora","aria-label":'Permalink to "Lora"'},"​")],-1),i("p",null,[s("原文："),i("a",{href:"https://arxiv.org/pdf/2106.09685",target:"_blank",rel:"noreferrer"},"https://arxiv.org/pdf/2106.09685")],-1),i("figure",null,[i("img",{src:D,alt:"image-20250720224305641",loading:"lazy",decoding:"async"})],-1),i("figure",null,[i("img",{src:F,alt:"image-20250720224441025",loading:"lazy",decoding:"async"})],-1),i("p",null,"如果仅有左侧的模型，就是全量微调。",-1),i("p",null,"现在将左侧模型冻结，梯度传导到右侧的A和B矩阵。这里解释一下A和B是什么。",-1),i("p",null,"理论上原始参数的梯度是一个较大的矩阵∆W，现在将该矩阵分解为r更低的两个矩阵B和A表示，由于B和A的r更小，更新的参数也大量减小了。",-1),i("p",null,"图上AB的值为初始化的值，即A用高斯初始化，B用0初始化。",-1),i("p",null,"可以看出lora也是一种adapter，但是是并行的加入新参数，通过矩阵分解降低参数更新量。",-1),i("h3",{id:"rlhf",tabindex:"-1"},[s("RLHF "),i("a",{class:"header-anchor",href:"#rlhf","aria-label":'Permalink to "RLHF"'},"​")],-1),i("p",null,"（Reinforcement Learning from Human Feedback， 人类反馈强化学习）",-1),i("p",null,"这一步是为了保证回答能够对齐人类的价值观，使得输出更符合人类偏好，分为两个步骤，训练RM和PPO训练",-1),i("p",null,"RM（Reward Model）",-1),i("p",null,"这一步在训练一个奖励模型，定量评判模型输出的回答在人类看来是否质量不错，即",-1),i("p",null,"输入：【prompt，模型的回答】",-1),i("p",null,"输出：评分（符合人类偏好的程度）",-1),i("p",null,[s("训练RW的数据则来自于：对于同一个prompt，由LLM不断生成不同的回答，然后人工对这些回答的偏好进行"),i("strong",null,"排序"),s("。")],-1),i("hr",null,null,-1),i("p",null,"将RW训练好之后，以RW的评分为标准，不断对LLM进行人类偏好的训练，常用的方式是PPO训练（Proximal Policy Optimization），一种强化学习方法。",-1),i("h2",{id:"二、大模型应用",tabindex:"-1"},[s("二、大模型应用 "),i("a",{class:"header-anchor",href:"#二、大模型应用","aria-label":'Permalink to "二、大模型应用"'},"​")],-1),i("h3",{id:"rag",tabindex:"-1"},[s("RAG "),i("a",{class:"header-anchor",href:"#rag","aria-label":'Permalink to "RAG"'},"​")],-1),i("p",null,"（检索增强生成，Retrieve Augment Generation）",-1),i("p",null,"如果询问LLM专业领域的知识/需要时效性的知识，LLM经常会产生幻觉，捏造文献和一些知识，一种常见的方法是检索专业领域的知识，并且将对应的内容随user prompt一起输入给LLM，从而提升内容的准确性。",-1),i("figure",null,[i("img",{src:B,alt:"img",loading:"lazy",decoding:"async"})],-1),i("ul",null,[i("li",null,"构建知识库：收集文档，将其chunking，即按一种策略将文档分块，将每块文本embedding成向量，存进向量数据库（以向量为index的数据库，支持相似性搜索）中。"),i("li",null,"检索：在输入prompt后，根据prompt从向量DB中检索top-k个context，和prompt拼接后再让LLM生成回答")],-1),i("h3",{id:"agent",tabindex:"-1"},[s("Agent "),i("a",{class:"header-anchor",href:"#agent","aria-label":'Permalink to "Agent"'},"​")],-1),i("p",null,"Agent，即在LLM问答推理能力的基础上，增加规划（例如评估任务难度，选择工具函数，分解任务目标…）、记忆（保留历史上下文）和使用工具函数的能力。",-1),i("figure",null,[i("img",{src:v,alt:"alt text",loading:"lazy",decoding:"async"})],-1),i("p",null,"上述规划能力，来源可能是prompt工程（比如要求大模型分解任务），可能来源于SFT（标注子任务数据集）",-1),i("p",null,"例如，一种方法叫做few-shot CoT，提供给大模型一个示例，再让其分解对应领域的问题，就可以有比较好的效果",-1),i("div",{class:"language-markdown"},[i("button",{title:"Copy code",class:"copy"}),i("span",{class:"lang"},"markdown"),i("pre",{class:"shiki shiki-themes github-light github-dark vp-code"},[i("code",{"v-pre":""},[i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"问题：罗杰有5个网球，他又买了两盒网球，每盒有3个。他现在总共有多少个网球？")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"思维链：")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"罗杰一开始有5个网球；  ")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"购买了两盒，每盒3个，共买了2×3=6个网球；  ")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"因此总数是5（原有） + 6（新购） = 11个网球。  ")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"答案是11。  ")]),s(`
`),i("span",{class:"line"}),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"请根据以上思考方法，解决下述问题：")]),s(`
`),i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"}},"艾米需要4分钟爬到滑梯顶部，滑下需1分钟。滑梯开放15分钟，她能滑多少次？")])])]),i("button",{class:"code-block-unfold-btn"})],-1),i("p",null,"规划完成后，LLM会主动调用某些函数，函数调用的能力，则来自于人类自行部署微服务到云端 \\ 本体定义函数，通过mcp协议或直接通信，让LLM输出符合函数输入格式的参数，调用函数并返回，再添加到LLM后续输出中。",-1)])]),"main-header":a(()=>[t(l.$slots,"main-header")]),"main-header-after":a(()=>[t(l.$slots,"main-header-after")]),"main-nav":a(()=>[t(l.$slots,"main-nav")]),"main-content-before":a(()=>[t(l.$slots,"main-content-before")]),"main-content":a(()=>[t(l.$slots,"main-content")]),"main-content-after":a(()=>[t(l.$slots,"main-content-after")]),"main-nav-before":a(()=>[t(l.$slots,"main-nav-before")]),"main-nav-after":a(()=>[t(l.$slots,"main-nav-after")]),comment:a(()=>[t(l.$slots,"comment")]),footer:a(()=>[t(l.$slots,"footer")]),aside:a(()=>[t(l.$slots,"aside")]),"aside-custom":a(()=>[t(l.$slots,"aside-custom")]),default:a(()=>[t(l.$slots,"default")]),_:3},8,["frontmatter"])}}};export{H as default,P as usePageData};
