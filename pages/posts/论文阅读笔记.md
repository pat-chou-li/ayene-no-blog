---
title: 论文阅读笔记，各种各样的

date: 2023.11.1

categories: 阅读笔记

tags:

 - 图形学
 - NeRF
 - 3DGS
---

## nerfren（2022 CVPR）

- Category：光照反射
- Project: https://bennyguo.github.io/nerfren/
- Code: https://github.com/bennyguo/nerfren
- Paper: https://arxiv.org/pdf/2111.15234.pdf

### Motivation

原来的nerf：

- 缺乏对反射的建模，对于镜子反射的物体，在体渲染的模型下，相当于在镜子里存在一个虚拟的空间，从而导致错误的深度估计，**同时会将反射的物体建模为半透明的，从而导致雾状的几何**。

  > 其实我觉得这个很符合直觉，体渲染并不是真正的光线追踪，不能做到反射的光线也只能用虚拟的空间来欺骗视觉了

  - 先理解什么是雾状，将其和一般的半透明（如一个泡泡）区分开来，在一块区域中，每个采样点都有一些几何存在，但都是半透明的，看见的就是雾状，而一个泡泡看起来很清晰，是因为它内部是不存在发光的采样点的，而只在表面存在，这个是透明。

  - 在ref-nerf中提到，NeRF倾向于在物体内部使用各向同性发光点来“伪造”镜面反射，而不是由表面点发射的视相关辐射，导致物体具有半透明性或雾状壳。

  <img src="./assets/image-20231123164551426-1700729325052-18-1700729333233-20-1701848684483-1.png" alt="image-20231123164551426" style="zoom:50%;" />
  
  - ↑可视化的density，在正确的物理世界中，镜子应该只在表面有密度，而nerf中是认为镜子有一个半透明的表面，然后在镜子内部的不同深度出现了发光点，从而是雾状的。
    - 这样还会导致一个问题，这样训练出来的nerf，在镜子的交界附近也能看到那个镜子中的虚拟世界，也就是所谓伪影。

  <img src="./assets/image-20231118152612523-1700729317192-16-1701848694294-3.png" alt="image-20231118152612523" style="zoom: 80%;" />
  
  - 所以问题就回到如何让nerf学习到基于表面的反射，如果用原来的nerf结构，最后的结果应该是：最后的镜子上有很高的density，即认为镜子是不透明物体，同时镜子平面上每个采样点的c来表示镜面反射，通过一些光学模型作为几何先验进行正则化，或者是修改模型等等，这篇文章就在解决这个问题。

### Method

- 将神经辐射场分解为独立的transmitted部分和reflected部分，同时学习一个reflection fraction map $\beta$，最后image由两者通过各自的辐射场渲染，最后相加得到，即

$$
I = I_t+\beta*I_r
$$

- 当然，it is highly under-constrained，如果不更改nerf就简单的将其分解为两个场，其实就是做一个无监督任务，那么会有无穷多种二分类的分解方法最后合成得到正确的视图。常见的有：分解为full场（渲染完整视图）和empty，分解为2个full场，以及两者之间。那么自然就要对训练做出约束了，基于以下三个假设：

  - $\beta$只应该和transmitted有关，因为$\beta$其实表示了物质的材料（我理解是反射率），而不应该和被它反射的物体有关系
  - transmitted的深度图应该是局部平滑的，因为现实中大部分反射体都是光滑平面（疑问：可是这个场也要用来重建其他物体，从case来看就是只盯着镜面重建了，但是case中镜面与其他物体的交界处好像重建的也还行）
  - reflected部分只需要简单的几何，因为大部分情况下我们只能从有限的观察方向看到反射图像。（意思是正常的三维重建我们追求360度全方位重建，但是重建出镜面中反射的物体我们只能看见一部分）

- 自然，三个假设做出三种约束

  - 设计特定的网络结构，分解成两个场，这里的α（reflection fraction map）只和transmitted有关

    - 整个镜面反射r场都和视线无关是一个比较反直觉的设计，有些不能理解。在原来的折射场中保持了方向是为了保留高光。在我看来是完全抛弃了物理假设，也就是图形学中的PBR公式，BRDF项等一系列物理假设，把镜子中的图像认为是藏在镜子中的另一个世界，不同视角带来的差异是因为摄像机在不同位置带来的，而并不是由于反射的光来源于不同位置造成的。比如看一个鼠标，从正面，上面，左边的三视图看到的是不同的鼠标图像，这是视线无关的场，场中某个点就是鼠标的组成部分，不会因为视角变化而导致鼠标本身的样子发生了变化，但鼠标上的高光真的是会因为你视角不一样而发生改变的。

    <img src="./assets/image-20231118153245250-1700729315140-14-1701848705589-5.png" alt="image-20231118153245250" style="zoom: 80%;" />

  - 应用深度平滑先验

    - p是某像素，q是周围的8个像素，t*是估计深度（通过体渲染时采样点的深度加权求和得到），pq的深度差距越大以及颜色差距越大，都会导致Loss的增大，保证transmitted场的深度局部平滑

  ![image-20231118160816671](./assets/image-20231118160816671-1701848707092-7.png)

  - 双向深度一致性约束（bidirectional depth consistency constraint）

    镜面反射场最好描述了一个什么物体？作者的推导逻辑是，因为只有在很少的视角能观测->简单的几何，一种光线只能击中一个表面的几何->应该是一个不透明表面，一个贴了纹理的壳

    > 其实个人感觉故事讲得不好，这个约束在于将表面约束的足够薄，不应该由只能从正面看镜子来导出，其实将镜面反射简化为一张2D纹理是图形学渲染中的常见手段，但是确实不是很物理也不好讲故事，我猜作者是受此启发的，但是故事没太讲好，或者是我没get到，或者说从ref-nerf受启发也很合理，要将发光体约束在表面就要使得物体足够薄]

    - 定义了一个**反向**的深度，也就是从最**远**采样点的深度加权求和得到

    - 定义了一个**正向**的深度，也就是从最**近**采样点的深度加权求和得到

    - 这里比较容易让人产生疑惑，从近到远求和以及从远到近求和结果有什么区别？其实权重不一样，同一个采样点，从近到远求和的权重取决于它之前的点的不透明度，而另一个取决于它之后的点的不透明度，因此如果有两个表面，正向深度就会更偏向于前面的表面，因为后面的表面权重被前面的表面大幅降低了。![image-20231118164117531](./assets/image-20231118164117531-1700729312546-11-1701848708687-9.png)

    - 来看一下不同的几何下正向深度和反向深度长什么样，横轴是采样点的距离，蓝线的纵轴表示密度，所以a表示遇到了两个表面，b是雾，c则是理想的镜面反射体——一张贴了纹理的固体。那么作者提出的约束就很简单，让两个深度足够近。
    
      ![image-20231118174744343](./assets/image-20231118174744343-1701848710428-11.png)

- 另外，对于有挑战性的场景，比如镜子，可以手动的提供mask，来使得场景被更正确的分解。（其实感觉这个是不是有点过分了，破坏了end to end，加入了手工的方法来大幅提升精度，不过某些先语义分割再nerf的方法好像也殊途同归，我训练个专门识别反射体的segment的cnn/transformer作为预输入语义是否有一定效果？就是novelty几乎没有，还会有个臃肿的模型，不过发散的讲，更多种分解场的方法，以及不同对应的预输入？）

  - 具体是，利用提供的mask图作为$\beta$的约束，要在mask区域$\beta$尽可能大，而其他区域尽可能小。

<img src="./assets/IMG_9445-1701848735342-13.JPG" alt="IMG_9445" />

![image-20231118150053296](./assets/image-20231118150053296-1701848737166-15.png)



### Limitation

- 这篇文章太注重平面反射的建模了，自然会产生一个问题：对于非平面的镜面反射不能有很好的效果，比如下图的弯曲镜面![image-20231118181253580](./assets/image-20231118181253580-1701848749616-17.png)

- 另一个是没有模拟出菲涅尔效应，这里作者只是提了一下，没有给出failure case，其实个人感觉反射系数$\beta$近似了一种菲涅尔系数，不过没有进行建模和约束，仅靠MLP学习，所以大概率学到的不是物理意义上的菲涅尔。

- 虽然文章没写，但是感觉过于精细的调参也是一个缺点，它要在早期屏蔽view direction以及它自己新增的几何约束，才能出比较好的效果，否则基本就是分解失败

![image-20231120191622445](./assets/image-20231120191622445-1701848750885-19.png)

## Ref-Nerf（2021 CVPR）

- Project: https://dorverbin.github.io/refnerf/
- Code: https://github.com/google-research/multinerf
- Paper: https://arxiv.org/pdf/2112.03907.pdf

### motivation

- 以view direction为MLP的输入，不方便进行插值，因为radiance function关于view direction变化太快
- nerf倾向于用在物体内部的各向同性的发光点来伪造镜面反射，导致物体呈现出半透明或者雾状。也因此导致法向量充满噪声，难以使用。

### contribution

- 重新参数化nerf，将view direction替换为出射方向

- 提出一种IPE，使得即使分开建模漫反射和镜面反射时，radiance function在不同的纹理和材质下仍可以平滑插值（还没看mip-Nerf，先TODO，有球谐函数和高斯的知识，也可能和3DGS有共通之处）
- 一种正则化方法，使得体积密度能够集中于表面，从而优化法向量的精确度

### method

- 重新参数化nerf，将view direction替换为出射方向
  - 用一个四个平行光的简单场景为例，黑色的是可见（但是没有光）部分，棋盘格是不可见部分。
  - $\omega_r$表示出射方向，永远指向平行光所以不随着x变动，因此第二行的图中这些光都是平行线，如果我们要预测$\omega_r$不取在这四个平行光所在方向的参数，Directional MLP只需要进行平滑的插值。
  - 下方的$\hat{n}, ρ, c_d$表示ref-Nerf对这三个量进行了建模，将它从镜面反射中分离开来，最明显的变化是使得右边第二列，相比于第一列，让Directional MLP不需要对漫反射进行插值。

<img src="./assets/image-20231123144423362-1701848752833-21.png" alt="image-20231123144423362" style="zoom:50%;" />

- 网络的框架，ρ表示粗糙度，和IDE有关，法向量和视线方向用来算出反射方向$\omega_r$，以及位置$x$通过空间MLP编码后得到的特征向量$b$,还有$n·d$(为了模拟菲涅尔项，以及其他可能的BRDF函数)来计算高光反射的颜色，最后和漫反射颜色一起通过色调映射输出srgb颜色。[色调映射：将颜色从线性空间转换到srgb并且将颜色范围限制到[0,1]]

![](./assets/image-20231123150232055-1701848754511-23.png)

- 提出一种IPE，使得即使分开建模漫反射和镜面反射时，radiance function在不同的纹理和材质下仍可以平滑插值（还没看mip-Nerf，先TODO，有球谐函数和高斯的知识，也可能和3DGS有共通之处）

- 一种正则化方法，使得体积密度能够集中于表面，从而优化法向量的精确度

  - 针对体密度梯度计算出的法向量充满噪声，不够平滑的问题，提出第一个正则化，就是在MLP输出另外一个法向量，要求这两个法向量足够接近。作者提到：MLP预测的法线更加平滑，这和vanilla NeRF提到的位置编码来自于同一理论，论文Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains中提到MLP的这一特性，以及克服MLP趋向于学习低频特征而不擅长学习高频特征的方法，包括原始NeRF中的位置编码。当然，这里是反过来利用MLP学习低频特征的特性，来预测一个尽可能平滑的法线。

  ![image-20231123160605760](./assets/image-20231123160605760-1701848756682-25.png)

  - 第二个问题是针对NeRF总喜欢用一些物体表面后的发光点的问题，提出要让高可见度(也就是$w_i$高)和光线同向的法线受到惩罚，这样物体存在于物体背面又能够发光被我们看见的物体就会减少。通俗的讲，让NeRF倾向于让面对光线的那一部分表面来表达物体的颜色，而不要让物体表面变成半透明，而让里面的物体发光来解释所看到的颜色。

  ![image-20231123161646209](./assets/image-20231123161646209-1701848758427-27.png)

## Nerf2Mesh（2023 ICCV）

- Category：nerf2mesh, nerf-texture
- Project: https://me.kiui.moe/nerf2mesh/
- Code: https://github.com/ashawkey/nerf2mesh
- Paper: https://arxiv.org/pdf/2303.02091.pdf

好像不是很看得懂，尽力吧

这个翻译的不错：https://blog.csdn.net/m0_50910915/article/details/131823539

这个是论文思路解释：https://blog.csdn.net/qq_40514113/article/details/129759065

### motivation

关于深度学习重建mesh的方法中，

- mobile-nerf重建的mesh质量不佳，而且纹理是在特征空间中而不是RGB空间中，这使得纹理编辑成为难题
- SDF相关的工作中，提取了过度平滑的几何，难以model thin structures。
- 通过Marching Cubes产生的网格有过多的冗余顶点和表面
- NVdiffrec使用可微光栅化来优化可变形四边形网格，但只能用于对象级的重建，并且在复杂的多边形上失败

### contribution

- 提出了Nerf2Mesh，从多视角RGB图像中提取网格，细化从Nerf中提取的粗网格，实现几何和外观的联合优化。
- 提出了一种迭代式的网格refine算法，能够自适应的调整表面密度，根据重投影的二维图像误差对复杂表面进行细分，对简单表面进行抽取（decimated）（???）
- 与最近方法相比，更好的网格质量，更小的网格尺寸，更好的渲染质量。

### Realted Work

Surface mesh for Scene Reconstruction方面不是很看得懂，直接看从NeRF中提取网格的工作吧。

Nerf使用体积密度场表示几何，并不形成确定的表面，因此在提取表面网格上也受到限制。一种流行的方法是学习SDF，但是SDF的表面太过平滑，无法学习thin construction，还提到了SAMURAI，mobielNerf，以及提到两篇工作发现**指数密度激活函数**可以帮助集中密度，形成更好的表面。

### Method

![img](./assets/01815b7515bd48249535fc5064994770.png)

先训练一个grid-based的NeRF（InstantNGP），分为几何和外观联合优化

- 几何上，先用Marching Cubes提取粗网格，然后通过文中提出的算法细化网格
- 外观上，通过颜色网格学习的，并分解为漫反射和镝面反射项。收敛后，我们可以导出精细网格，展开其UV坐标并烘焙纹理。

#### stage1

大致的流程图里已经很清楚了，不过文中提到$f_s$可以被烘焙为纹理，$MLP_2$可以在fraa shader中实现，因此镜面反射可以被导出和渲染，很难理解这件事，特征是什么被提取为纹理的，着色器里怎么写MLP等等，感觉在这一领域欠缺了很多知识。

另外，本文的方法直接将光照烘焙为纹理，因为估计环境光具有挑战性，而且很可能会导致渲染质量降低。

当然，为了约束分解，还加入了L2正则化，原文提到这是为了促进漫反射和镜面反射的分解，于是对镜面反射应用L2正则化。这里复习下L1和L2，L1正则化趋向于让模型获得稀疏解，即在某些权重上为0，L2则让模型趋向于获得较为平滑的解。

> 也就是说这边是让不同位置上的高光反射不过于强烈，可以让高光的$c_s$平滑一些……为什么促进了分解？已经完全看不懂了
>
> 你用L1正则化我还能理解，但是文中也没有什么详细解释，只能你说是那就是了

![image-20231207153921810](./assets/image-20231207153921810.png)

为了使得表面更加锐利，采用交叉熵正则化。

兴许这个还是比较好理解的，交叉熵正则化鼓励权重的稀疏分布，也就是说让体渲染上每个点的体密度尽可能大，而不是形成雾状，这样就使得体密度集中在表面上了。

> 求求作者解释一下吧，靠猜太累了，当然也可能是我读的论文太少了，这个作者认为不需要解释

![image-20231207155808414](./assets/image-20231207155808414.png)

#### stage2

- Appearance refinement：通过nvdiffrast进行可微渲染，仍然可以使用img逐像素的损失，来进行外观优化。（按文章脉络应该是在优化纹理，按图来说应该还是在优化$MLP_1，MLP_2$的参数）

- Iterative mesh refinement.

  - 顶点优化比较容易理解，对于每个顶点设置一个偏移量$\triangle v_i$，通过可微渲染反向传播image-space loss gradients（NVdiffrec提出）来优化这个偏移量。
  - 面（face）优化就比较复杂，因为网格面不可微。本文提出的训练策略是将2D图像渲染误差重投影到对应的网格面上，累积网格面的误差，然后给出一个阈值，高于这个误差的做网格细化，低于的做网格抽取并重新网格化来降低网格密度。

  > 复习一下obj格式吧，顶点用三维向量描述顶点位置，面则是由顶点索引组成，表示这个面由哪几个顶点链接得到，注意顶点索引顺序是有影响的，这会决定面的方向
  >
  > 网格抽取（decimate）似乎是一个图形学上几何的相关知识，读不懂还是图形基础不够

网格更新之后就重新初始化顶点便宜和面误差

- Unbounded scene.

无界场景优化，没有读过相关文章，略过了。

#### Mesh Exportation

将优化后的精细网格$M_{fine}$用XAtlas解析UV坐标，然后烘焙漫反射颜色$c_d$和镜面反射特征$f_s$，分为生成$I_d, I_s$

为了渲染镜面反射颜色，采用mobileNeRF中的方法，导出$MLP_2$的权重并合并到frag shader中

### Limitation

烘焙光照，无法relight，基于单通道光栅化，不能处理半透明

> 读完了，但是和没读一样，不过还是有收获
>
> - get一种约束体密度到表面的方法，可以对体渲染公式中的$\alpha_i$做交叉熵损失
> - 在网格重建方面还是缺少了很多的基础知识，比如MarchingCube，另外意识到NeRF可能不是网格重建的主要方法，SDF可能更加主流。
> - 初步看到了可微渲染是什么东西，了解到一种可微渲染框架nvdiffrast
>
> 以后如果做相关方向，可以再回来看看这篇文章。

## **pixelNeRF**

**pixelNeRF: Neural Radiance Fields from One or Few Images**

*Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa*
CVPR 2021, 3 Dec 2020

[[arXiv](https://arxiv.org/abs/2012.02190)] [[Project](https://alexyu.net/pixelnerf/)] [[Github](https://github.com/sxyu/pixel-nerf)]

这篇文章给出了PixelNeRF的简易实现：https://zhuanlan.zhihu.com/p/550890576

应该算是NeRF早期的Few-shot工作，引用量也比较高，就当做稀疏视角重建的启蒙作看吧

### Realted Work

没有提NeRF相关的工作，都是之前的稀疏重建，提到了同期工作GRF。

### Method

motivation比较简单就合在method里，简单来说就是之前的NeRF只使用原来的图像进行训练，没有很好的利用原来图像的所有信息，所以当只有稀疏视角的时候新视角合成就有比较多的artifacts

这篇文章提出要用CNN提取输入的图像，形成一个feature volume，在推理的时候也使用这张特征图（额所以你为什么不叫feature map要叫volume），就能更充分的利用原来图片的信息了。

CNN Encoder由预训练的res-net34组成。

![image-20231215211057871](./assets/image-20231215211057871.png)

思想还是比较简单的，以单视角输入为例，将input view经过CNN Encoder得到一个Volume feature，即每个像素都有一个feature，即pixel-aligned，记作W，当需要进行新视角合成的时候，采样点从世界坐标映射到input view所在的view space，然后由最近的四个像素经双线性插值得到一个特征向量，将这个特征向量输入以下的网络

<img src="./assets/image-20231215211524634.png" alt="image-20231215211524634" style="zoom: 50%;" />

值得注意的是当多视角输入的时候，得到的V是经过一个平均的后输入解下去的MLP的，这点应该还有优化空间，不同视角下对于空间中同一个点训练出来的特征应该是具有不同意义以及重要性的，比如要新合成的视角和正面比较近，那么背面的视图应该拥有较低的权重。

另外输入的时候直接使用view direction，没有经过位置编码，也不是在中途加入的，原文表示

- view direction可以作为不同视图的相关性和定位的信息
- 当新视角和已有视角更接近的时候，可以更依赖已有视角，否则应该更依赖学习出的prior

> 我怎么感觉还是没解释为什么不中途加入呢

## **RegNeRF**

RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs

*Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, Noha Radwan*
CVPR 2022, 1 Dec 2021
[[arXiv](https://arxiv.org/abs/2112.00724)] [[Project](https://m-niemeyer.github.io/regnerf/index.html)] [[Code](https://github.com/google-research/google-research/tree/master/regnerf)] [[Notes](https://github.com/yangjiheng/nerf_and_beyond_docs/blob/main/paper_discussions/RegNeRF.md)]

总之先来两篇不错的阅读笔记：

顺便了解下李代数：https://zhuanlan.zhihu.com/p/532973564

fewshot工作总结：https://zhuanlan.zhihu.com/p/617570383

### motivation

- 一些工作（MVSNeRF，IBRNet，GRF，PixelNerf）需要昂贵的预训练，通过“amortized inference”（摊余推断）与fine-tune来完成稀疏视角重建。尽管这些模型取得了令人期待的结果，但通过捕捉或渲染许多不同场景来获取必要的预训练数据可能成本过高。此外，这些技术在测试时可能无法很好地推广到新领域，并且可能由于稀疏输入数据的固有模糊性而产生模糊的伪影。

> gpt:
> "摊余推断"（amortized inference）是指在模型的训练阶段进行的推断过程，其中模型被训练以在给定一些输入时，能够有效地生成输出，而不需要每次都进行全面的推断。这种方法的目标是通过训练模型来学习一种映射，使其在整个输入空间上都能产生良好的推断结果。
>
> 在文中提到的上下文中，摊余推断是指在测试时，通过使用已经训练好的模型，可以从仅有少量输入图像生成新颖的视图，而无需每次都重新进行完整的模型推断。这种方法有助于提高模型的效率，并使其更容易应用于实际场景，尤其是对于需要快速推断的应用。

> 还知道了一个常识：稀疏视角下有一个固有问题，就是会出现模糊和伪影，这里是在说这种做法还是没有解决模糊和伪影问题。

- 另一些工作是通过添加一些正则化，重新训练整个场景（例如引入深度监督的nerf，这里指Depth-supervised NeRF: Fewer Views and Faster Training for Free，以及引入clip的dietNeRF），但是现有方法要么过于依赖并非始终可用的外部信号（意思就是depth不是什么时候都能拿到的），要么只在低分辨率场景下（只提供high-level的信息，这里是针对clip）运行。

### contribution

- patch-based regularizer：用于新视角的深度图，可以减少伪影、增加几何质量
- normalzing flow model：用于新视角的颜色，通过最大化渲染patch的对数似然（？），来避免颜色在不同viewpoint下的差异（消融实验来看是个没用的创新点，还很难看懂，这里直接跳过了）
- **annealing strategy for sampling points：沿采样点的退火策略，首先在小范围内对场景内容进行采样，然后扩展到完整的场景边界，保证训练早期不出现分歧。**

### method

#### 深度平滑：

![image-20240106173912019](./assets/image-20240106173912019.png)

从规定的可能的相机位姿矩阵中采样光线$r_{ij}$，在patch范围内进行深度平滑，深度计算方法采用NeRF原代码中的方法。

> 很常见的方法了，但是一百种NeRF改进有一百个实现，也有各种各样的理由，可以考虑作为一个小trick。

#### 退火策略（Sampling Space Annealing）

![image-20240113161838002](./assets/image-20240113161838002.png)

作者观察到，在稀疏输入的情况下，NeRF容易收敛到相机的近平面上，这虽然还原了输入视角，但是缺乏3D一致性，对新视角的合成就很不友好。

> $i:迭代次数，t_n:近平面，t_f:远平面，t_m:中点$

可以注意到$\eta$为0的时候$t_n=t_f=t_m$，而$\eta$为1的时候就是原始的近远平面，按照作者的思路应该$\eta$逐渐从0增加到1,因此看$\eta$的定义，作者还增加了超参数$N_t$：到第几轮的时候应该停止退火,和$p_s$：初始的$\eta$，可以看出max和min肯定是写反了，从代码来看也能知道。

![image-20240113164015919](./assets/image-20240113164015919.png)

## FreeNeRF

**FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization**
*Jiawei Yang, Marco Pavone, Yue Wang*
CVPR 2023, 13 Mar 2023
[[arXiv](https://arxiv.org/abs/2303.07418)] [[Project](https://jiawei-yang.github.io/FreeNeRF/)] [[Github](https://github.com/Jiawei-Yang/FreeNeRF)] [[Notes](https://github.com/yangjiheng/nerf_and_beyond_docs/blob/main/paper_discussions/FreeNeRF.md)]

### motivation

现存方法的不足：

需要大规模预训练：pixelNeRF，MVSNeRF

引入了深度监督，导致了复杂的管线：Depth-supervised NeRF

patch级别的正则化，导致了较高的计算量：DietNeRF，RegNeRF

总的来说，作者引入了两个几乎不增加计算量的正则化方法，避免了以上策略的缺点，即无依赖（不需要预训练和深度来引入额外信息）和无开销（不增加计算量），使得稀疏视角下的重建质量得到了很大的提升。

### contribution

- 揭示了稀疏视角重建的失败与位置编码频率之间的关系
- 提出两种正则化

### method

#### Frequency Regularization（频率正则化）

![image-20240113202748545](./assets/image-20240113202748545.png)

作者首先发现低频位置编码反而在稀疏视角重建学得不错（尽管过度平滑），10%是指解锁前10%的位置编码，以L=10为例，10*10%=1, pos_enc[int(1):] = 0,即只保留第一个位置编码。剩下是讲故事环节：

原文只说明：高频的位置编码使得高频部分更快收敛，从而阻止了对低频信息的探索，导致NeRF合成的新视角图像中出现了预期之外的高频伪影。

接下来是个人理解：

拥有高频位置编码意味着能够学习那些只移动一点位置就能变化较大的场景细节，在稀疏视角重建的时候，由于对原始图像的过拟合，把一些本该是低频信息的（比如平滑的表面）学习成了随视角变化很大的高频信息，合成新视角时，移动视角就会带来很大的变化，从而出现各种各样的高频伪影。

于是就提出一个退火策略，随着轮次i增加，逐渐开放高频位置编码。

![image-20240113211947323](./assets/image-20240113211947323.png)

#### Occlusion Regularization（遮挡正则化）

![image-20240113210555465](./assets/image-20240113210555465.png)

观察伪影，还可以发现有一些伪影并不是高频的，变化剧烈的，而是像墙一样直接堵住了物体的一部分，这部分是怎么回事呢？

通过上图可以知道，其实是输入的稀疏视角中极少重合的那一部分被NeRF解释成了更靠近相机近平面上的密集体密度块（漂浮物），在渲染新视角的时候采样到这些高体密度块就会出现一些墙挡住后面的物体。热力图是深度图，实线矩形里面是输入图像，虚线矩形里面是新视角图像，虚线圈起来的部分是导致了新视角图像“Wall”的“极少重叠部分”。

> 关于为什么会到相机近平面，个人认为是因为更容易收敛到这里，因为在近平面上改变一小部分的体密度就能改动很大的一块图像，近大远小嘛，这样的话这里就是梯度下降最快的方向，也是一个过拟合的问题。

![image-20240113214518748](./assets/image-20240113214518748.png)

提出的正则化如上，假设采样64个点，K=1,2,3...64,（near to far），可以知道越靠近相机的点Loss越大（1/K），同时$\sigma_k$表示该采样点体密度，$m_k$是一个二进制掩码，表示该点是否需要被正则化，为0的点该项不生效。总的来说，离相机越近，体密度越高，惩罚越强。

## **FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting**

*Zehao Zhu, Zhiwen Fan, Yifan Jiang, Zhangyang Wang*
arXiv preprint, 1 Dec 2023
[[arXiv](https://arxiv.org/abs/2312.00451)] [[Project](https://zehaozhu.github.io/FSGS/)]

论文阅读笔记：https://zhuanlan.zhihu.com/p/674709488

### motivation

- NeRF的稀疏视角重建方法不能取得质量和速度的平衡
- 在稀疏视角下，SFM生成的点云质量不佳，由于初始化不充分，后续原始3DGS的split和clone方法也无法弥补缺陷，导致结果过度平滑（什么叫过度平滑？之后可以做实验看看，NeRF那边是说有高频伪影反而要减少高频）
- 因此，提出一种新的改动高斯球的方法，称为Proximity-guided Gaussian Unpooling，同时引入深度先验来保证这种方法生成的高斯核的合理性

### method

- 针对不够充分的高斯初始化，采用Proximity-guided Gaussian Unpooling策略增加高斯球，通过测量现有高斯分布之间的接近度并将新的高斯分布战略性地放置到最具代表性的位置来填补空白空间。
- 为了确保密集化的高斯分布几何形状合理，用了深度先验
- 用pseudo view generation防止过拟合于稀释视角（伪视图生成？）

> 文中提到Additionally, some Gaussians tend to grow towards extremely large volumes, leading to results that overfit the training views and generalize badly to novel viewpoints (See Fig. 3).
>
> 也就是说，类似FreeNeRF中提到的遮挡在近摄像头平面的黑团，3DGS在稀疏视角下也有将高斯核训练的过大来拟合单一视角的问题，或许可以通过正则化解决。

#### Proximity-guided Gaussian Unpooling

为每个高斯分配一个属性proximity score：接近度分数，定义为其到最近的K个高斯核的平均距离，默认K=3

![image-20240223164012926](./assets/image-20240223164012926.png)

当proximity score超过一个阈值$t_{prox}$，就在其自身（ori）与它的K个邻居（dst）之间创建一个新的高斯核，其scale和opacity由dst决定，rotation和SH初始化为0，通过以上过程可以在representative location（代表性位置？大概就是本来应该要很密集的区域）增加高斯密度，在优化的过程中逐渐填补观测缺乏带来的空白。

> 怎么没讲如何具体的进行depth guidance修正新生成的高斯，虽然图示已经比较清楚了，猜测是拿着DPT生成的深度图，获得到新生成的高斯所对应的位置的深度，修正高斯核的深度
>
> 看完代码，误会解除，原来并没有调整高斯深度的挨骂，而是靠深度正则化约束。

#### Geometry Guidance for Gaussian Optimization

通过上述致密化方案补充高斯核后，直接用光度损失（估计就是3DGS原始损失）进行优化，会由于稀疏视角而不能优化出连贯的几何，以及容易过拟合于稀疏视角而在新视角合成没有泛化性。作者提出用深度先验来帮助改善高斯的几何。

##### Injecting Geometry Coherence from Monocular Depth（构建估计深度和渲染深度的几何相关性）

![image-20240223170224318](./assets/image-20240223170224318.png)

就是一个深度损失，估计深度图来自预训练模型DPT，渲染深度图来自3DGS

##### Differentiable Depth Rasterization（可微深度光栅化）

当然，为了深度可以被反向传播优化，需要写可微深度估计，深度由下式计算

![image-20240223170513164](./assets/image-20240223170513164.png)

可以发现，和渲染颜色时差不多

![image-20240223170531373](./assets/image-20240223170531373.png)

所以cuda代码应该还是比较容易实现的

#### Synthesize Pseudo Views

没太看懂，大概是在两个已知视角之间合成一个新视角，但是没仔细说是怎么用，应该是用来数据增强，实验中提到在2000次迭代后才开始合成视角。

看了下代码

```python
    if iteration % args.sample_pseudo_interval == 0 and iteration > args.start_sample_pseudo and iteration < args.end_sample_pseudo:
        if not pseudo_stack:
            pseudo_stack = scene.getPseudoCameras().copy()
        # 线性插值得到的伪视角
        pseudo_cam = pseudo_stack.pop(randint(0, len(pseudo_stack) - 1))
		# 用3DGS渲染伪视角，得到pred深度
        render_pkg_pseudo = render(pseudo_cam, gaussians, pipe, background)
        rendered_depth_pseudo = render_pkg_pseudo["depth"][0]
        # midas: 深度估计模型，当做gt深度，把3DGS渲染的图片传给深度估计模型得到的
        midas_depth_pseudo = estimate_depth(render_pkg_pseudo["render"], mode='train')
		# reshape
        rendered_depth_pseudo = rendered_depth_pseudo.reshape(-1, 1)
        midas_depth_pseudo = midas_depth_pseudo.reshape(-1, 1)
        # 两个深度做loss
        depth_loss_pseudo = (1 - pearson_corrcoef(rendered_depth_pseudo, -midas_depth_pseudo)).mean()

        if torch.isnan(depth_loss_pseudo).sum() == 0:
            loss_scale = min((iteration - args.start_sample_pseudo) / 500., 1)
            loss += loss_scale * args.depth_pseudo_weight * depth_loss_pseudo
```



### Experiments

迭代次数降低为10_000次

透明度重置在2000,5000,7000进行

9500次后降低深度损失的权重

不进行过大高斯核剔除（size_threshold = None）

数据集中，mip-nerf360被特别提到，因为他们第一个在无界数据集上进行稀疏视角重建。

![image-20240223171958860](./assets/image-20240223171958860.png)

消融实验如上

## Splatter Image: Ultra-Fast Single-View 3D Reconstruction（未读完）

[Stanislaw Szymanowicz](https://szymanowiczs.github.io/), [Christian Rupprecht](https://chrirupp.github.io/), [Andrea Vedaldi](https://www.robots.ox.ac.uk/~vedaldi/),

Visual Geometry Group - University of Oxford

website：https://szymanowiczs.github.io/splatter-image

大致意思是3DGS将高斯核映射成图像，本文找到了一种方法将图像通过Unet映射成高斯核

## GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces

[Yingwenqi Jiang](https://github.com/Asparagus15)1, [Jiadong Tu](https://github.com/donjiaking)1, [Yuan Liu](https://liuyuan-pal.github.io/)2, [Xifeng Gao](https://gaoxifeng.github.io/)3, [Xiaoxiao Long](https://www.xxlong.site/)2,*, [Wenping Wang](https://www.cs.hku.hk/people/academic-staff/wenping)4, [Yuexin Ma](https://yuexinma.me/aboutme.html)1,*

*Corresponding author

1ShanghaiTech University, 2The University of Hong Kong, 3Tencent America, 4Texas A&M University

website：https://asparagus15.github.io/GaussianShader.github.io/

### motivation

现存的3DGS没有明确的建模外观属性，因此对镜面反射，显著视图变化的渲染质量不佳，特别是渲染那些反射为主要特征的材料的时候。

提到Ref-NeRF和ENVIDR渲染速度太慢，后者由于SDF的限制ENVIDR甚至不能建模复杂场景。

既然要提到反射，法线估计是不可避免的，如果直接通过搜索附近的高斯来估计法线，会带来很高的计算开销。这篇文章基于高斯球的最短轴方向提出了一种法线估计方法，同时还从深度图中导出法线图与估计的法线构成一个法线一致性正则项。

### contribution

- 提出了shading function逼近渲染方程，增强了反射和镜面的真实感
- 提出了一种新的法线估计方法
- 因为3DGS很快，实现了实时渲染

### method

![image-20240222192218636](./assets/image-20240222192218636.png)

大致流程如上，放弃了SH描述颜色，引入了一些Shading Attributes，一张可微的环境光照模拟间接照明

#### 描述颜色

作为SH的代替，该文章用如下方程描述颜色，是对渲染方程的一种近似

![image-20240222193421155](./assets/image-20240222193421155.png)

$\omega_0$：view dir

$c_d$:漫反射颜色

$s$:tint，材质本身的颜色

$n,ρ$：法线、粗糙度

$L_s$:镜面反射光

$c_r$:**残差颜色**，唯一一个陌生概念，作者说这是因为一些复杂的反射，例如间接光照的散射和反射不能用上述的直接光反射来解释，所以用这项来解释这些复杂的反射，当然因为也是反射同样和view dir有关

原文表示残差颜色由SH参数化

以上参数除了直接提供的$\omega_0$都可以训练

#### $L_s$的计算

![image-20240222195442807](./assets/image-20240222195442807.png)

这是计算$L_s$的式子，回顾一下渲染方程

![image-20240222195659103](./assets/image-20240222195659103.png)

很相近，所以原理也大致相同，都是积分有哪些$w_i$贡献了$w_o$这个方向上的L，渲染方程中根据不同材质有不同的BRDF函数$L_i$,这里也根据不同的粗糙度$ρ$和反射角$r$有不同的D函数，其中$L(w_i)$用可训练的6x64x64的cube map表示。

#### 法线估计

![image-20240223124320762](./assets/image-20240223124320762.png)

先用椭圆最短轴当法线，但是椭圆最短轴可能朝外也可能朝内，先用上述式子选择和视线方向$\omega_0$一致的方向，为了修正法线引入一个可训练的法线偏移$△n$,同时为了保证不偏移太远引入正则项

![image-20240223124606761](./assets/image-20240223124606761.png)

但是以上法线定义在每个高斯核上，没有和局部的其他高斯核联系，而法线又是反应局部梯度变化的量，这导致了估计法线和几何不一致，简单的方法是直接搜索局部高斯核共同估计法线，但是计算开销太高，所以作者提出用深度图得到法线图，然后法线图和之前的法线估计之间使用法线一致性损失

![image-20240223125538008](./assets/image-20240223125538008.png)

三种法线的图示，有偏移的$n$,最短轴$v$和深度图导出的法线$\hat{n}$

![image-20240223125709557](./assets/image-20240223125709557.png)

#### Sparse loss

![image-20240223130900633](./assets/image-20240223130900633.png)

帮助不透明度趋于0或1，同时使得高斯球的几何形状趋近于thin plate从而提高渲染质量

### Experiments

![image-20240223133226757](./assets/image-20240223133226757.png)

从消融实验来看，最重要的其实是可训练的环境贴图，其次就是法线一致性正则化。

从实验结果来看，各项指标相比原先的GS提升不大，甚至有的数据集中会下降，但是法线质量会大幅提高，在一些反射效果比较重要的区域会有比较好的视觉效果

## **GaussianPro: 3D Gaussian Splatting with Progressive Propagation**

单位：中科大, 港大, 南大, 阿大等

主页：[https://kcheng1021.github.io/gaussianpro.github.io/](https://link.zhihu.com/?target=https%3A//kcheng1021.github.io/gaussianpro.github.io/)

代码：[https://github.com/kcheng1021/GaussianPro](https://link.zhihu.com/?target=https%3A//github.com/kcheng1021/GaussianPro)

论文：[https://arxiv.org/abs/2402.1465](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.14650)

### motivation

根据介绍来看，该论文主要改进方向是大场景下的新视角生成。

- 在大场景下有很多无纹理区域，这些区域在SFM下不能生成点云，因此导致后续3GGS使用原有的致密化策略时，很难生成可靠的高斯点。
- 原有的高斯克隆策略忽略了已有的几何先验，要么克隆和之前完全一样的高斯，要么采用位置和方向随机的高斯初始化，这使得高斯在noisy geometries区域难以优化，以及在无纹理区域较少的高斯点。

### method

#### 生成2D法线图和深度图指导高斯生长

法线估计直接用的GaussianShader里的方法用椭圆短轴，似乎没有用到偏移法线，那生成出来的法线应该会有些不准确。

深度估计就是正常方法

最后都用α合成生成2D的法线图和深度图

#### 2D图中根据相邻像素更新每个像素的深度和法线，重投影回3D空间，生成新高斯（没看懂，用了很多上古方法）

通过一种比较复杂的方法，根据相邻像素推测出当前像素的深度和法线是否准确，大致思路如此，下面有这种方法的参考文献

BARNES C, SHECHTMAN E, FINKELSTEIN A, et al. PatchMatch[J/OL]. ACM Transactions on Graphics, 2009: 1-11.

上述方法会有不可避免的估计错误，还要通过多视图几何一致性（这里又引了一个2016年提出的方法）来过滤掉不正确的深度和法线，最后才得到最终要用来修正原来的法线图和深度图的新图（propaganda map），对于绝对差大于阈值的像素，生成高斯点。

> 太菜了，对于传统方法实在是提不起动力学习，除非是非常重要且通用的。
>
> 既然这种方法测出来的法线不是准确的，像GaussianShader那样用可训练法线偏移来计算无疑对我来说更简单。

#### 平面损失

![image-20240227160737927](./assets/image-20240227160737927.png)

原先的3DGS没有几何约束，只有光度约束，从而导致不能准确描述几何形状，所以这里加入了两个关于几何的损失。

![image-20240227161034029](./assets/image-20240227161034029.png)

第一个是约束rendered normal map（椭圆短轴）和propaganda map的法线一致性

第二个是从NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance抄过来的损失，保证椭圆足够扁，使得椭圆中心足够接近其表面

> 流体仿真+高斯那篇文章约束高斯足够圆，这里约束高斯足够扁，太神奇啦，深度学习

![image-20240227161138873](./assets/image-20240227161138873.png)

#### Experiments

![image-20240227163543172](./assets/image-20240227163543172.png)

从消融实验来看还是Propagation策略起到的作用比较大

![image-20240227163620851](./assets/image-20240227163620851.png)

有趣的是还做了稀疏视角下的实验，30%表示只用了30%的训练图像，可以看到对于稀疏视角重建也有比较稳定的提升。

## Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting

论文：https://arxiv.org/abs/2402.15870

没代码，随便看看吧，不过用MLP代替球谐函数的方法很有趣，有其他文章也写到

### motivation

原始的3DGS中，反射和镜面反射难以建模，这是因为低阶球谐函数 (SH) 捕获这些场景中所需的高频信息的能力有限。

针对这个问题，作者提出了以下方法

- 采用anisotropic spherical Gaussian (ASG)来模拟外观，可以有效针对高频信息进行建模，有效模拟各向异性和镜面的部分。
- 采用sparse anchor points控制子高斯的位置和表示的混合方法（？
- coarse-to-fine的训练策略，消除floater，在初始化阶段优化低分辨率图像，可以避免在训练初期增加不必要的高斯

### method

#### ASG

![image-20240228133734703](./assets/image-20240228133734703.png)

ASG函数定义如上，用24维feature f作为MLP输入训练其中三个参数，这个f是每个高斯核上携带的

![image-20240228135630455](./assets/image-20240228135630455.png)

颜色分解为漫反射场和高光场

![image-20240228135727176](./assets/image-20240228135727176.png)

$c_d$:first three order of SH

![image-20240228140649448](./assets/image-20240228140649448.png)

$c_s$：将N个ASG的输出拼接成一个latent feature，然后联合被位置编码的view direction和<n, -d>进入MLP计算

#### 法线估计

GaussianShader最短轴

![image-20240228141453019](./assets/image-20240228141453019.png)

反射方向计算的便捷方法

都是常用知识，不展开了

#### Anchor-Based Gaussian Splatting

加速，省内存用的，略过

#### Corase-to-fine

作者提出：3DGS中漂浮物的出现源于对特定像素及其邻居的过度关注，而不是考虑更广泛的全局信息，因此作者要从低分辨率到高分辨率逐步训练3DGS，训练过程中的分辨率如公式所述：

![image-20240228142610226](./assets/image-20240228142610226.png)

$r_s:起始分辨率，r_e:终止分辨率,τ:迭代阈值，默认20k$

该方法优化质量的同时，将训练时间降低了20%

> 原始的高斯也有预热策略，在开始时用1/4的resolution训练，在250次和500次迭代的时候进行上采样

#### 保证高斯核足够小的正则化

![image-20240228143641415](./assets/image-20240228143641415.png)

Prod表示高斯核三个轴scaling乘积

### Experiments

![image-20240228143840152](./assets/image-20240228143840152.png)

使用MLP+ASG可以有效建模高光

![image-20240228143925366](./assets/image-20240228143925366.png)

Corase-to-fine能去除一些漂浮物

消融实验就给几张图是吧，代码不给数据不给有点过分了，字节好感度↓↓

### Limitation

该方法虽然能有效建模高光和各向异性材质，但是对于反射仍然不能取得很好的质量，这是因为3DGS缺少明确的geometry和environment

此外，实验中还观察到，如何明确的给3DGS提供几何信息，在强约束下会让结果更符合期望，但是渲染质量会有所降低。将来打算探索在3DGS中建模反射的可能解决方案。

## GEA: Reconstructing Expressive 3D Gaussian Avatar from Monocular Video

Website:https://3d-aigc.github.io/GEA/.

### motivation

- 源于现成的姿势估计方法准确性和稳定性的不足，缺少对手部、脚部区域的控制能力

![image-20240301165331956](./assets/image-20240301165331956.png)

- 现有的模型存在Unbalanced Aggregation和Initialization Bias
  - Unbalanced Aggregation：纹理丰富区域会出现较多的高斯核，相应的缺少纹理的区域会缺少高斯核
  - Initialization Bias：与initial shape偏离较大的区域（披肩、发饰、配饰）接收到较少的高斯点分配

### contribution

- 提出一个two-satge pose refinement method，提高了身体和手部姿势估计的准确性
- 提出一个迭代re-initialization方案，包括meshing，resampling和re-Gaussians，保证高斯点在表面附近均匀分布
- 大量实验证明有效性

### method

#### Drivable body-hand avatar representation based on 3D Gaussian Splatting

对于每个高斯点，找到最近的四个joints，用如下公式计算它的pose transformation

![image-20240301185443435](./assets/image-20240301185443435.png)

$\theta$ : 给定pose

$\Gamma(\theta)$: 每个joint计算出的pose transformation matrix

$W_p(\mu)$ : 从高斯核μ最近的vertex of SMPL-X得到的blend weights

$P$ : 指最近的四个joint

最终高斯核μ的新位置

![image-20240301190225779](./assets/image-20240301190225779.png)

此外，针对非刚性形变的情况，比如服装变形，增加了可训练高斯位置上的偏移，输入参数为给定的姿势

![image-20240301190422711](./assets/image-20240301190422711.png)

对比LBS，可以发现同样是从pose$\theta$中得到仿射矩阵$T$,LBS中每个vertex对应的blend weights已知，这篇文章就找离高斯核最近的vertex来当做这个高斯核的权重，同时不累计所有的joint变化对该高斯的影响而是只取最近的4个

![image-20240301191141264](./assets/image-20240301191141264.png)

#### Twostage pose estimation and optimization technique

目的：为了更好的位姿估计

![image-20240301192747083](./assets/image-20240301192747083.png)

stage1： 使用PyMAF-X进行姿态估计，得到姿态$\theta$和相机参数$\Pi$

![image-20240301193206900](./assets/image-20240301193206900.png)

直接用PyMAF-X在侧面时容易导致的现象

stage2：用SAM预测轮廓，用ICON或者PiFuHD预测法线，修正stage1预测出的pose渲染的法线和轮廓

![image-20240301193449941](./assets/image-20240301193449941.png)

最终Loss如下

![image-20240301193617959](./assets/image-20240301193617959.png)

其中$L_{regular}$定义如下，法线和轮廓为L1损失

![image-20240301193647025](./assets/image-20240301193647025.png)

$\omega_i$ ： 距离根节点的远近程度，该正则项让stage2的优化不偏离stage1太远，同时手和脚受到的约束更低，在stage中refinement的幅度更大

#### Iterative re-initialization mechanism

目的：解决Unbalanced Aggregation和Initialization Bias，因此希望高斯点均匀分布在主体的真实表面附近。

- Unbalanced Aggregation：纹理丰富区域会出现较多的高斯核，相应的缺少纹理的区域会缺少高斯核
- Initialization Bias：与initial shape偏离较大的区域（披肩、发饰、配饰）接收到较少的高斯点分配

分为三步：

mesh：使用alpha shape重建表面网格。

resample: 对网格执行拉普拉斯平滑，来注入表面平滑先验。随后，对网格执行基于曲率的均匀采样，生成新的高斯点。

Re-Gaussian ： 为重新采样的点找到它们的K个最近高斯点，并继承它们的不透明度 η 和球谐函数 f 属性。旋转 R 和缩放 s 属性被随机重新初始化。

在训练过程中，以上re-initialization被重复2~3次

![image-20240301195443325](./assets/image-20240301195443325.png)

#### Loss

![image-20240301200142838](./assets/image-20240301200142838.png)

第一个光度损失

第二个是将渲染图像C和输入的图像I通过VGG后，最后softmax输出n维向量作差，作用是VGG对高频信息敏感，可以帮助恢复高频纹理细节。

> 有点疑问，首先是这个i和M不明所以
>
> 第二是经过VGG卷积得到的Loss是如何反向传播到对应的高斯核上的，光度损失好理解因为每个像素是由哪些高斯核alpha合成的是清楚的，所以每个像素颜色的改变可以正常反向传播，但是VGG卷积后得到的是N*1维向量，这是怎么从一个像素反向传播回原图片所在的所有像素的
>
> 思考了一下，正常的VGG是锁定像素颜色，反向传播训练卷积核参数，这玩意反过来训练像素颜色，能反向传播好像也挺合理
>
> 数学不好导致的

![image-20240301202625968](./assets/image-20240301202625968.png)

第三个是约束非刚性形变的残差尽量小，避免显著干扰avatar

![image-20240301202620705](./assets/image-20240301202620705.png)

### Experiments

![image-20240301210735157](./assets/image-20240301210735157.png)

相比于用SMPL的GART，用SMPL-X的GEA有明显更好的手部表现

![image-20240301210851158](./assets/image-20240301210851158.png)

出色的捕捉高频细节的能力

> 明天就用3DGS做一个VGG损失，你骗我我就举报

#### 消融实验

![image-20240301211521221](./assets/image-20240301211521221.png)

首先Drivable body-hand avatar representation based on 3D Gaussian Splatting这个方法肯定不能少，不然都不能渲染，或者说就是原始方法了，剩下的分别是缺少Pose Refine，即two-stage refinement，缺少手部骨架，应该用来对比SMPL和SMPL-X的，以及method中的循环初始化

## FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization

2024 CVPR 暂无主页

### motivation

3DGS在densification时往往存在过度重建，高频区域被几个大高斯核覆盖，导致模糊和伪影。

类似FreeNeRF，尝试引入频率退火正则化解决频率问题，最小化pred和gt之间的频域差异。

### method

#### Frequency Regularization

作者首先提出，原来的L1 Loss并不能很好的描述那些过度重建的区域，进而导致较大的高斯核无法分裂

我们先来复习一下高斯densification的代码

```python
# Densification
# 超过densify_until_iter不再进行densification和透明度重置
if iteration < opt.densify_until_iter:
    # Keep track of max radii in image-space for pruning
    # 会从多个视角、多次优化的过程中去看同一个高斯核，记录下看到的最大2D投影半径
    gaussians.max_radii2D[visibility_filter] = torch.max(gaussians.max_radii2D[visibility_filter], radii[visibility_filter])
    # 记录下xyz累积梯度，即位移倾向较高的高斯核，他们应该被split或clone
    gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter)
    # warm-up后才开始densification，每100次迭代进行一次
    if iteration > opt.densify_from_iter and iteration % opt.densification_interval == 0:
        # 3000次迭代之后才开始剔除过大的高斯核
        size_threshold = 20 if iteration > opt.opacity_reset_interval else None
        gaussians.densify_and_prune(opt.densify_grad_threshold, 0.005, scene.cameras_extent, size_threshold)
    # 每3000次迭代，重置透明度，或背景为白色且第500次迭代时
    if iteration % opt.opacity_reset_interval == 0 or (dataset.white_background and iteration == opt.densify_from_iter):
        gaussians.reset_opacity()
```

首先，要保证梯度大于densify_grad_threshold，才会根据scaling进行split或者clone，显然如果过度重建部分的梯度不够大，那么那些较大的高斯核也不会分裂。我们干观察下图的圆点，发现紫色的圆点和绿色的圆点几乎重合，也就是说well-reconstruction区域和over-reconstruction区域的梯度是相似的，进而误导了高斯的densification

<img src="./assets/image-20240312144349720.png" alt="image-20240312144349720" style="zoom: 67%;" />

那么什么样的loss能够更好的区别over-reconstruction区域（注意区分under-reconstruction区域，这篇文章应该不太注重那些过小的高斯核所以不关注），作者就提出引入频域。我们先看结果，上图中心为三角的两条线明显的被分离开来，从而能够区分这两种区域。

首先将图片由空间域（x，y）转换到频域（u，v）

![image-20240312144944583](./assets/image-20240312144944583.png)

从频域中提取振幅和相位

![image-20240312145001611](./assets/image-20240312145001611.png)

Re表示实数，Im表示虚数

计算input和render的振幅、相位差

![image-20240312145627728](./assets/image-20240312145627728.png)

> 这玩意怎么反向传播啊

#### Frequency Annealing

按Free-NeRF的思路来，应该先进行低频滤波，尽量还原图片的低频区域，再逐渐解锁高频区域

一样的思想，创建一个低频滤波器和高频滤波器

![image-20240312150410949](./assets/image-20240312150410949.png)

分别得到低频滤波和高频滤波后的相位差和振幅差

![image-20240312150431990](./assets/image-20240312150431990.png)

退火式的定义高频滤波

![image-20240312150445645](./assets/image-20240312150445645.png)

$D_0$是低频滤波器的range max，$D_t$是高频滤波器允许通过的频带，t表示cur迭代次数，$T_0$表示开始使用高频滤波器的迭代次数，$T$表示迭代终止次数。可以发现，随着训练进行，高频滤波器允许通过的频率越来越高，也就是起到了解锁高频的效果。

最后是loss，先引入低频滤波，在$T_0$后引入高频滤波

![image-20240312150803314](./assets/image-20240312150803314.png)

$w_l, w_h$是training weights

### Experiments

![image-20240312153147547](./assets/image-20240312153147547.png)

先摆个消融实验

![image-20240312153158702](./assets/image-20240312153158702.png)提升

## DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization

2024 CVPR 暂未开源

大概会在3月20日公开

代码：https://github.com/Fictionarry/DNGaussian/blob/main/submodules/diff-gaussian-rasterization/cuda_rasterizer/backward.cu

### motivation

现有的深度正则迫使高斯的shape适应于平滑的深度，而不是复杂的几何外观。考虑到场景几何形状的基础在于高斯图元的位置而不是它们的形状，我们**冻结了形状参数**，并提出了硬性和软性深度正则化，通过**鼓励图元之间的移动**来实现空间重塑。

现有的**尺度不变深度损失**往往选择将深度图对齐到固定尺度，这导致**忽视了小的损失**。为解决这个问题，我们将全局-局部深度归一化引入深度损失函数中，从而以尺度不变的方式鼓励学习小的局部深度变化。通过局部和全局尺度归一化，我们的方法引导损失函数重新关注小的局部误差，同时保持对绝对尺度的了解，以增强深度正则化的详细几何重塑过程。

> Darf,MonoSDF用的尺度不变损失
>
> SparseNeRF和NeuralLift-360用的深度排序损失

### contribution

- A Hard and Soft Depth Regularization：通过鼓励高斯函数的移动来约束3D辐射场的几何形状，从而实现了粗略的深度空间重塑，同时保持细致的色彩性能
- A Global-Local Depth Normalization：在局部尺度上归一化depth patch，关注小的局部深度变化，提高重建细节

### method

Instead, considering the instability of point clouds in sparse-view situations, we initialize our method with a random set of Gaussians

**考虑到稀疏点云的不确定性，采用随机初始化**

#### Depth Regularization for Gaussians

we freeze the scaling s and rotation q in the depth regularization.

在深度正则化中冻结了scaling和旋转四元数q，而保留了透明度α和位置μ，总的来说用两个约束，一个去优化位置（贴近于物体表面），另一个去优化透明度（保持丰富的色彩）

> 我去，降低工作量新思路，少写两反向传播呢

##### Hard Depth Regularization

第一个高斯，往往是物体的表面 or floter和noises，

![image-20240312190116888](./assets/image-20240312190116888.png)

通过将高斯核上的α在计算深度时替换成一个较大的$\Gamma$，来近似的算出第一个高斯的深度

从而得到第一个深度相似损失

![image-20240312190222918](./assets/image-20240312190222918.png)

由于只有μ能够被优化，透明度、shape都被freezing，所以这样可以让那些物体表面的高斯贴紧物体表面，而那些floater或许会因此被优化？

##### Soft Depth Regularization

![image-20240312191754804](./assets/image-20240312191754804.png)

剩下的就是正常的深度渲染，这时候冻结μ

##### Global-local Depth Normalization

现有的Loss（即使是L1）都容易忽略局部的深度细节，对small depth errors不敏感，进而导致noisy primitives和新视角合成的失败（左图）。而作者加入的局部损失可以让模型优化局部细节（右图），重建更精确的表面

![image-20240312193006018](./assets/image-20240312193006018.png)

为了让损失更关注局部，首先将深度图切分成局部的patch，然后在每个patch做归一化，让patch内深度变为均值为0，方差为1的数据

![image-20240312194054190](./assets/image-20240312194054190.png)

进一步，在关注局部的基础上保证对全局的关注，对全图做归一化，最小单位为patch

![image-20240312194139509](./assets/image-20240312194139509.png)

这项损失定义为

![image-20240312194510910](./assets/image-20240312194510910.png)

#### 颜色

![image-20240312194816264](./assets/image-20240312194816264.png)

在稀疏视角下SH容易过拟合，采用MLP，和grid有关，先放着

>Neural Color Renderer. 3D Gaussian Splatting represents the color via spherical harmonic, however, it is easy to overfit with only sparse input views. To relieve this problem, we take a grid encoder and an MLP as the Neural Color Renderer to predict color for each primitive (Figure 3). During inference, we store the median result and only calculate the last MLP layers to merge view direction for acceleration.

### Experiments

![image-20240312195440764](./assets/image-20240312195440764.png)

终于有用LLFF的，我哭死，但是指标怎么这么高

只迭代6000次，在1000次后用软深度正则化。

![image-20240312195537505](./assets/image-20240312195537505.png)

损失函数的超参数![image-20240312195550574](./assets/image-20240312195550574.png)

shape freezing的作用，消除floater

<img src="./assets/image-20240312200135469.png" alt="image-20240312200135469" style="zoom:50%;" />

shape freezing和center freezing的消融

![image-20240312200231171](./assets/image-20240312200231171.png)

所有创新点的消融

<img src="./assets/image-20240312200445631.png" alt="image-20240312200445631" style="zoom: 80%;" />

AP指直接对深度使用L2正则化，估计深度还是要归一化的不然尺度不一致

关于使用MLP合成颜色

![image-20240312200836809](./assets/image-20240312200836809.png)

> 不得不吐槽，一句话带过的创新点实际上带来了超高的质量提升，而这个创新点直接来自NGP

### Details

patchsize 

在LLFF数据集上，[5, 17]随机采样

在DTU数据集上，[17, 51]随机采样（物体占据了更大的空间）

相机位姿：合成数据集直接给定，真实数据集用colmap采集所有图像（包括训练集）来计算位姿

训练集和测试集：

LLFF，每8张作为测试集，剩余视图中均匀采样训练集

DTU：DTU 数据集 [17] 由一组固定相机捕获的 124 个以对象为中心的场景组成。我们遵循[27,42,52]直接在15个场景上评估模型，扫描id为8,21,30,31,34,38,40,41,45,55,63,82,103,110和114。在每次扫描中，在我们的3视图设置中，以下id为25、22和28的图像被用作输入视图。测试集由 ID 为 1, 2, 9, 10, 11, 12, 14, 15, 23, 24, 26, 27, 29, 30, 31 32, 33, 34, 35, 41, 42, 43, 45, 46 和 47 的图像组成进行评估。图像被下采样4×

Blender：我们遵循[16,52]中用于Blender数据集[11]的数据拆分。从训练图像中选择8个输入视图，ID为26、86、2、55、75、93、16、73和8。从测试图像中均匀采样25个测试视图进行评估。在实验过程中，所有图像都被下采样 2 倍到 400 × 400。

>The splatting technique of our Gaussian Splatting [18] backbone directly merges existing primitives to render the pixel-level color without interpolation. However, since not every pixel can be overlapped by the projected primitives, the empty space between two Gaussian primitives would cause hollows and cracks when the camera pose changes. For example, some hollows can be seen at Scan 40 in Figure 14. In this work, we try to solve this problem by paying more attention to high-frequency details and therefore encouraging the densifying of primitives to fill these empty areas. In the future, we believe this problem can be fundamentally solved by the improvement of the representation itself.
>
>两个高斯椭球之间的空白会在相机位姿改变的时候导致空洞和裂缝，这篇文章通过更加关注高频细节来解决这个问题，鼓励椭球密集化，这是一个值得思考的思路，在我合成的深度图里明显出现了各种各样的裂缝，这是由于高斯椭球半身的数量不够导致的，怎么在稀疏视角下做更好的densification是值得思考的。

## SparseGS: Real-Time 360° Sparse View Synthesis using Gaussian Splatting

website:https://formycat.github.io/SparseGS-Real-Time-360-Sparse-View-Synthesis-using-Gaussian-Splatting/

涉及了difussion，这里只读深度相关部分

<img src="./assets/image-20240313135612175.png" alt="image-20240313135612175" style="zoom:67%;" />

很眼熟的两种深度估计，总之是为了选择到真正的位于物体表面的深度，这里提出的方法是选择权重（是T不是阿尔法，即理论上后续的高斯会有劣势，除非透明度够大）最大的高斯

![image-20240313135713451](./assets/image-20240313135713451.png)

损失上，仍然采用Pearson损失，不过加入了patch，每个iteration随机的采样N个不重叠的patch，patch的大小是超参数

![image-20240313140521362](./assets/image-20240313140521362.png)

接下来，根据模式深度和alpha合成深度来选择裁剪掉一些floater，理由是观察到存在floater的深度通常是双峰的，dip_test是一个测试数据是否呈现单峰分布的统计方法。

> 感觉有点难懂，直接从伪代码解释吧

<img src="./assets/image-20240313142220711.png" alt="image-20240313142220711" style="zoom: 67%;" />

先计算机mode深度和alpha深度，然后计算$\frac{d^{mode}-d^{alpha}}{d^{alpha}}$,执行dip_test，确定在这个相机Pose下双峰严不严重，如果严重就在之后动态的根据D提高remove的阈值，来保证更多的去处floater，如果不严重就降低，来恢复场景的细节

![image-20240314143933702](./assets/image-20240314143933702.png)

## Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images

随便扫了一眼，总之就是加了个深度约束+平滑约束，没细看，直接看实验的trick

### Experiments

实验中设置SH degree= 1，直接去除了透明度重置

![image-20240315151318980](./assets/image-20240315151318980.png)

baseline给你压完了

## Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?

### motivation

fewshot的两个问题

- 训练数据优先，模型容易过拟合输入视图，导致几何分布在2D平面上而不是3Dvolume

> 3DGS自然也是如此

- artifacts比如ghost和floater的存在限制了新视图的保真度和3D一致性

解决以上问题主要分为两种思路

- 基于先验：multiview stereo  or image-based rendering（？）
- 基于正则化：频率、深度等

但是都没有关注MLP

同时研究了过拟合问题

- FreeNeRF：原始的MLP容易快速收敛到高频细节，通过这种方式快速记录输入视图，而不是推断底层几何图形，因此直接的解决方案是减少模型参数来降低模型容量
- 但是如DietNeRF所示，虽然减少模型参数能解决过拟合问题，但是结果中缺少细节，这表明网络应该保留模型容量。

基于以上观察提出了multi-input MLP，将原始MLP的输入（position and view direction）合并到每一层

![image-20240319130651258](./assets/image-20240319130651258.png)

> 好简单

基于三个key insights：

- 将输入合并到每一层可以实现输入和输出之间的较短路径，允许以端到端的方式使用更少的参数合成；
-  我们保持模型容量不变，因为它有利于合成高频细节
- 我们保持输入和输出不变，使其成为当前基于 NeRF 的管道即插即用的解决方案。

为了进一步减少伪影，受到几何通常比外观更平滑的假设的启发[23]，我们提出了一个新方法：不像 NeRF 那样使用共享模型来建模颜色和体密度，而是提议分别对它们进行建模，以启用具有不同频率的位置编码[21]。我们还提出了一种新的正则化项，以减少物体为中心的场景中的背景伪影，并提出了一种采样退火策略，以解决前向场景中的近场伪影。

### contribution

- mi-mlp
- 两个正则化提高质量

#### Related Work

正则化方法里总结的挺好，这些文章可以回头看看

>相反，基于正则化的方法遵循类似于 vanilla NeRF [21] 的逐场景优化方式，并引入额外的正则化项或训练源以获得更好的新视图合成。具体来说，首先引入语义一致性损失[12]、深度平滑损失[23]和射线熵损失[14]来约束不可见视图，以获得更好的几何恢复。为了增加可用的训练视图数量，一些工作 [1, 5, 15, 48] 提出使用深度扭曲来生成新的视图图像作为伪标签。最近，FreeNeRF [50] 通过位置编码的新频率退火策略遵循从粗到细的方式。MixNeRF [33] 将光线建模为拉普拉斯算子的混合，然后是 FlipNeRF [32]，它使用翻转的反射射线作为额外的训练源。SimpleNeRF [35] (siggraph asia)提出使用增强模型来避免过度拟合，这在前向场景中表现良好。尽管取得了显着的结果，但所有这些方法都仍然使用 vanilla NeRF 提出的网络结构。相比之下，本文从设计更好的网络结构的角度实现了少镜头视图合成。

### method

#### Per-layer Inputs Incorporation

总之就是位置编码输入到每一层的好处和数学证明，用更少的参数进行端到端合成，允许浅层梯度小于深层深度

#### Modeling Colors and Volume Density Separately

人们普遍认为，几何不需要外观那么详细，因为几何通常是局部平滑的，因此作者提出要减少体积密度输入的嵌入维数，这也意味着要将预测几何和外观的MLP分离，用两个单独的MLP来估计

![image-20240319141539169](./assets/image-20240319141539169.png)

关于密度分支，简单的使用每层输入位置编码的MLP

![image-20240319142218622](./assets/image-20240319142218622.png)

而作者经验性的发现颜色合成需要密度的辅助，因此合成颜色时借用了密度的输出

![image-20240319142445601](./assets/image-20240319142445601.png)

#### Background Regularization

以对象为中心的重建时，常常有背景伪影，即在输入视图不可见区域，产生了一些伪影

![image-20240319144149105](./assets/image-20240319144149105.png)

作者的解决方法是向可见区域外采样，并在这些区域添加正则化

![image-20240319144213670](./assets/image-20240319144213670.png)

希望这些不可见区域（训练集中的不可见，测试集中的可见）更加接近背景颜色

#### Sampling Annealing

为了避免近场伪影，进行退火式的采样，在训练开始时采样更少的点，而随着训练进行逐渐增加采样点

![image-20240319144934596](./assets/image-20240319144934596.png)

### Experiments

![image-20240319145146508](./assets/image-20240319145146508.png)

分别是muti-input mlp，分离几何和外观mlp，背景正则化，退火采样

分离直接加在nerf上不是很好，但是配合其他就能取得更好的效果，有点奇怪，不过总之最重要的是muti-input mlp

## GS-IR: 3D Gaussian Splatting for Inverse Rendering

2024 ECCV

逆渲染的文章，涉及了法线和深度估计，看两眼

### motivation

During the GS optimization, the adaptive control of the Gaussian density may lead to loose geometry, making it difficult to estimate accurate scene's normal. Consequently, it is necessary to introduce a well-designed strategy to regularize GS's normal estimation.

在高斯优化过程中，高斯的自适应密度控制导致了松散的几何，导致难以估计场景法线。因此，需要一个良好设计的策略来正则高斯的法线估计。

还有一个是关于间接照明的

### contribution

- 一种具有正则化的高效优化方案，将深度梯度集中中GS周围，为GS-IR生成可靠法线
- 开发了一种嵌入在GS-IR中的bake方法来处理建模间接照明中的遮挡

### method

为了避免权重和不为1的时候，深度的值出现在一条光线的第一个高斯之前或最后一个高斯之后，修改深度公式，在最后除以总权重，也就是在一条光线的高斯核上线性插值的深度

![image-20240325135553760](./assets/image-20240325135553760.png)

深度梯度法线和预测法线的一致性正则化（有点眼熟

![image-20240325140203937](./assets/image-20240325140203937.png)

当然无约束的训练法线是非常粗糙的，加上pixel-level的平滑度正则化

![image-20240325140307004](./assets/image-20240325140307004.png)

最后的loss

![image-20240325140627097](./assets/image-20240325140627097.png)

### experiments

![image-20240325141030411](./assets/image-20240325141030411.png)

从左到右，普通的体渲染累积深度，选择峰值深度（不透明度最高的高斯所在位置），线性插值深度

![image-20240325141300803](./assets/image-20240325141300803.png)

标有超天酱的表示用了法线一致性正则化

## CoherentGS

https://people.engr.tamu.edu/nimak/Papers/CoherentGS

404，给我整幽默的？

few-shot的文章

coherency(相干性、一致性、连贯性)

### motivation

However, it struggles to generate a good representation given a sparse set of training images. In such cases, the representation severely overfits to the training views and appears as a collection of semi-random anisotropic blobs from novel views, as shown in Fig. 1.

然而，它在给定稀疏的训练图像集时很难生成良好的表示。在这种情况下，该表示严重过度拟合于训练视图，并在新视角中呈现为半随机各向异性斑点的集合，如图1所示。

最先进的基于 NeRF 的方法会产生次优结果，因为它们的正则化不能为合理的 3D 重建提供足够的约束。此外，这些方法中的大多数依赖于神经网络学习的隐式表示的一致性，并不直接适用于具有显式非结构化表示的 3DGS。

关键思想：通过约束高斯在优化过程中的移动，增强明确的非结构化表示的连贯性。

由于高斯的非结构化特性，难以在3D空间中做出限制，因此提出为每个输入图像的每个像素分配一个单独的空间，在2D空间中强制执行单视图和多视图约束。

具体而言：

- 使用implicit decoder强制使每个图像的高斯具有相似的深度移动以保持一致性
- 对于不同视角，通过总变化损失确保使用所有高斯渲染的深度平滑
- 提出了基于流的损失，以确保两个图像中相应像素的高斯位置相似。
- 为了帮助优化，我们提出使用现有的单目深度预测模型初始化高斯的位置
- 我们基于深度的初始化将高斯适当地定位在世界空间中，而我们的正则化优化鼓励更新，特别是位置，保持连贯和平滑。

### contribution

- 我们提出了一种使用极其稀疏的输入集合进行3D重建的方法，利用3DGS技术。

- 我们提出了一种结构化高斯表示方法，并通过各种正则化手段引入了连贯性。

- 我们引入了基于深度的初始化方法来初始化3D高斯，这与正则化优化相辅相成。

### Related work

与我们的技术并行的是，Zhu等人（FSGS）[62]和Xiong等人（SparseGS）[54]提出利用3DGS进行稀疏视图合成。然而，与我们的方法不同，它们没有提出一种方法来强制在相邻高斯之间保持连贯性，从而产生偶发的floater。此外，这些方法会用延长的高斯填补所有输入图像中被遮挡的区域，导致模糊的结果。相反，我们的方法允许我们识别和修补被遮挡的区域，并产生高质量的虚构细节（见图1 - 右侧）。

![image-20240329142410289](./assets/image-20240329142410289.png)

> 被遮挡，也就是所有输入图像都没有的区域，这篇文章可以不进行合成，在后续进行inpainting

### method

我们的目标是重建静态场景的 3D 高斯表示。我们的关键思想是在优化过程中向 3D 高斯引入相干性。换句话说，当更新高斯的位置时，在优化过程中也应该类似地影响相邻高斯。通过这种一致性，我们可以使用稀疏正则化来进一步约束优化并避免对输入图像的过度拟合。

#### Coherent 3D Gaussian Optimization

首先，为每个像素分配一个高斯，同时限制每个高斯的运动只允许在这个像素所对应的光线上。

![image-20240329144436352](./assets/image-20240329144436352.png)

在初始化的深度上，用残差深度控制高斯位置(x)移动

![image-20240329144457994](./assets/image-20240329144457994.png)

g表示将像素p根据深度d投影到3d空间的位置x

假设单目深度准确，那么残差深度应该只会平滑的变化来调整不一致性，基于这一观察提出了以下的单视图和多视图约束：

##### Single-view Constraint

残差深度并不是直接优化的，而是通过implicit decoder在单张图像上解码的

![image-20240329145918473](./assets/image-20240329145918473.png)

其中f代表decoder，n是输入图像的index，每张图像独立的被输入decoder中，通过优化decoder的参数来更新残差深度而不是直接优化像素上残差深度，这使得像素间的残差深度得以建立联系（也就是保证coherent），确保了残差深度的平滑。

然而，平滑的深度变化使得decoder难以处理图像中不同object的sharp depth discontinuities，所以引入了C-channel的二进制分割掩码（实现中取C=5），即每张输入图像根据深度的不同切分成5个区域，因此decoder也输出C-chanel的残差深度，最后的残差深度由5个通道合并（直接求和）

在透明度上也是用这个约束

##### Multiview Constraint

单视图约束确保了每个图像高斯的平滑变形，但是没有保证来自所有高斯形成的3D表面是平滑的，于是在inverse depth上采用TV正则化

![image-20240329151600998](./assets/image-20240329151600998.png)

> 1+应该是防止除0的偏移，R表示alpha合成深度，左式全图平滑，右式是每个分割区域连通平滑

在实验中，$L_{TV}$首先较大来获得全局平滑，再逐渐增加$L_{MTV}$来改善细节

![image-20240329151811611](./assets/image-20240329151811611.png)

实验中$\lambda_s$​逐渐从0增加到1

#### Additional Regularization

受启发于一些深度估计算法，提出一种基于流的正则化，也就是约束不同图像对应同一位置的像素所对应的高斯位置一致，p、q由现有的flow method计算，M是一个前向后向一致性掩码（也是某个深度估计方法里的）

![image-20240329152637036](./assets/image-20240329152637036.png)

#### 最终Loss

![image-20240329153919455](./assets/image-20240329153919455.png)

可训练参数由上述loss训练，还有透明度和位置直接由Single-view Constraint优化

#### multisampling

类似MSAA的方式

虽然 3DGS 仅通过采样每个像素的中心来优化目标，但我们发现这种策略对于稀疏输入设置是有问题的。在这种情况下，高斯将被变形以匹配每个像素的中心的颜色，留下未覆盖的剩余区域。因此，从新视图来看，表面会出现半透明。为了解决这个问题，我们在每个像素内的多个样本上执行优化。多重采样确保高斯正确地覆盖每个像素，从而产生显著改善的图像，如补充视频和表3所示。

### 3D Gaussian Initialization

![image-20240329155057878](./assets/image-20240329155057878.png)

单目深度估计模型的深度是错位的（左），也就是不具有多视角一致性，因此在初始化前先训练深度图的scaling和offset（作者提出曾经尝试过直接训练深度，但是仅在光流准确的地方生效，于是改为训练scaling和offset）

![image-20240329155317441](./assets/image-20240329155317441.png)

$D^m$​表示单目深度估计深度，其他参数和4.2一致

scaling初始化比较特别，如图所示，其他都和原始高斯类似或是较为简单

![image-20240329170108293](./assets/image-20240329170108293.png)

### Experiments

![image-20240329171236708](./assets/image-20240329171236708.png)

![image-20240329171723992](./assets/image-20240329171723992.png)

LLFF上的消融实验

test

