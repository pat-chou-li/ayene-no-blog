---
title: 分割论文阅读整理

date: 2024.6.20

categories: 阅读笔记

tags:

 - 图形学
 - NeRF
 - 3DGS
---



## Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing

website : https://feature-splatting.github.io

### motivation

这篇工作提出了Feature Splatting，半自动的将静态3D场景动态化，用自然语言来操纵外观和注册材质属性来控制动态交互。通过使用**additional view-invariant features**和视觉模型CLIP、DINOv2、SAM来增强3DGS实现。进一步地扩展到基于物理的动态场景，其中物体的类别和物理参数通过语言模型queries赋予。

**让3DGS携带特征**和**基于物理的动态**都遇到了意想不到的技术挑战：

- feature map的提取：

  高斯在geometry和radiance上有效地共享相同的interpolation kernal（？什么意思），但是从reference camera获得的2D特征图是低分辨率且有噪声的。直接采用来自Decomposing NeRF for Editing via Feature Field Distillation（NeRF的特征蒸馏）方法会导致低质量的结果，以及大量的高频噪声。我们解决这个问题通过提出一个新方法来提取feature map和一个蒸馏它们的步骤。

- 基于物理的动态：

  为了适应volume-dependent physical effects, 提出in-fill现存的静态高斯的方法，以及在显著变形下如何将3DGS进行转换的方法，这些方法比以往表现的更好。

这个场景建模和合成pipeline，feature splatting，包含了丰富的语义以致于可以使用自然语言来编辑，包括分解静态场景（成每个组件），并联系每个组件通过material properties和基于物理的动态。

贡献如下：

- 提出feature splatting，增强静态场景，使其富有语义和余元驱动的物理真实movement
- 一种基于MPM的物理引擎，适用于基于高斯的表示，一种新颖的方法来融合来自多个基础视觉2D模型的特征以进行准确的分解
- 一个演示，来正面feature splatting是一个杰出的editing tool

### Related work

- Scene Editing with Distilled Feature Fields：许多工作提出NeRF的编辑方式，现存的工作都主要关注与如何操纵外观。例如

  - Distiled Feature Fields（DFF）通过zero-shot open-text分割进行外观编辑，其中使用知识蒸馏将特征从2D基础视觉模型中嵌入。渲染过程中，DFF通过将语言查询与蒸馏特征关联起来分割受影响的体积，从而分解场景。然后，可以对分割出的对象执行外观编辑，例如颜色更改或移除。
  - NeRFShop:提出了一个允许用户输入以对NeRF进行几何修改的交互式流程
  - Instruct-NeRF2NeRF : 提出不在渲染过程中进行修改，而是使用现成的2D编辑方法来修改用于训练NeRF的图像
  - ClimateNeRF : 与本文关系最密切，提出在渲染过程中注入物理模拟以模拟不同的天气效果。然而，由于隐式场景表示的固有局限性，ClimateNeRF只能支持在神经渲染中修改光线行进过程，从而限制其仅能模拟天气效果中的光线反射、折射和衍射。相比之下，我们的方法使用显式表示来支持以对象为中心的物理模拟，具有更广泛的潜在应用。

- Concurrent Work: 列举一些3D场景理解的文章。然后提到

  在物理仿真最相关的工作：PhysGaussian，表示同样使用了MPM，但不包括语义，并且PhysGaussian手动选择和分配高斯的物理属性。同时在处理高斯旋转时方式也有所不同。

  在分割最相关的工作：Feature3DGS，fuse (2D reference features ) using (priors from multiple foundation models).我们将特征蒸馏视为整个pipeline中的一个组件，用系统优化的技术将训练技术提高30%

### method

三个关键组件：

- 一个将丰富的semantic feature从vision-language models蒸馏到3DGS的方法
- 一个将场景通过open-text queries分解成key constituents的方法
- 作为物理真实动态场景合成的一部分，一种通过语言确定材料属性的方法

#### 3.1 Differentiable Feature Splatting

##### Feature Splatting

对每个GS添加一个额外的vector$f_i∈R^d$，与视角方向无关。

然后用volume rendering渲染特征图

![image-20240514173544908](./assets/image-20240514173544908.png)

##### Syatems Considerations

直接光栅化高维度特征会导致昂贵的训练时间，深入分析后发现主要瓶颈在内存访问模式，通过设计了cuda kernal解决

##### Improving Reference Feature Quality Using Part-Priors

Feature Splatting生成的特征取决于参考特征，但直接使用CLIP作为参考特征会导致质量降低，因为CLIP的特征比较粗糙，如下图

![image-20240514175837391](./assets/image-20240514175837391.png)

与基于NeRF的方法相比，在NeRF上这一问题并不显著，NeRF的隐式连续表示起到了正则化的效果。但3DGS没有这样的正则化，容易过拟合于粗糙reference feature map的噪声。

本文提出了一个改进GS Feature map质量的方法，通过使用DINOv2和SAM的object priors。

考虑一个输入图像，首先用SAM生成一个part-level masks集合${M}$, 对于一个给定的二值掩码M和粗糙的CLIP feature map $F_c$,本文使用一个Masked Average Pooling（MAP）来聚合一个single feature vector

![image-20240514191241457](./assets/image-20240514191241457.png)

其中i是$F_c$​中的一个像素坐标，然后将w分配给分割部分内的所有像素，如果一个像素属于多个部分，则该像素特征通过平均所有相关部分的特征来获得，这样就得到了上图的(d) SAM-enhanced feat

> 公式是将特征图$F_c$过一个分割掩码mask，比如说椅子就只剩下椅子那部分的feature map，然后将椅子对应像素的所有特征向量作平均池化，再重分配给椅子上对应的所有像素

为了进一步降低过拟合的可能性，本文提出了一个shallow MLP，有两个output，并以3DGS的rendered features$\hat{F}$ 作为中间特征（不是输入么？），两个output分别为预测的$\hat{F}_{CLIP}, \hat{F}_{DINOv2}$，用真实的CLIP以及DINOv2的输出做监督，pipeline如下![image-20240514200537791](./assets/image-20240514200537791.png)

首先是Feature splatting渲染出一个feature map，每个像素上有一个d维的Latent Feature，通过一层MLP输出预测的$\hat{F}_{CLIP}, \hat{F}_{DINOv2}$，用真实的CLIP以及DINOv2的输出做监督，其中$F_c$的监督权重更高，反向传播让3DGS能够通过render合成一个修正过的CLIP features map。

> 换种方式解释，目的是为了让每个gs上有一个语义属性，因此需要render出一个feature map和gt feature map做监督，直接用CLIP生成的feature map监督效果不好，两个优化，第一个是用mask修正clip gt，第二个是用DINOv2作为正则在监督时加入。

#### 3.2 Language-guided Scene Decomposition

那么要怎么使用每个高斯上的feature，让它完成场景理解任务呢。

我们识别3DGS，通过查询其CLIP feature更接近正查询而非负查询（？）

具体来说，给定一个词汇作为正样本（比如bulldozer），用一个通用的词汇作为负样本（比如object，thing），使用frozen CLIP text encoder来获得以上词汇的text embeddings，遵循CLIP的标准实现，计算rasterized CLIP feature of every Gaussian（应该是每个高斯的CLIP feature，那rasterized这个前缀是不是有些误导性）和text embeddings的余弦相似度，选择那些相似度大于0.6作为前景object。在附录中包含了使用负文本查询的分割结果。

通过上述方式就选中了需要编辑或者进行仿真的高斯。

一些简单的编辑操作就容易实现了：

![image-20240515140219149](./assets/image-20240515140219149.png)

#### 3.3 Language-Driven Physics Synthesis

feature splatting可以自动选择用于模拟的物理属性、估计碰撞表面、预测重力方向。核心方法在于使用Taichi扩展的MPM

##### Decoupling Objects for Simulation

对于常见的刚体创建了一组词汇（e.g. wood, ceramic, steel）,可以随用户的可选输入扩展。

给定一个selected multi-part object（比如一个插有花的花瓶），再执行一次CLIP相似度查询选择对象内与这些材料更接近的粒子。

在模拟过程中，这些被选定的粒子被认为是刚性的。

##### Language-grounded Collision Surface Estimation

预定义一组规范查询（包括常见的平面物体，比如floor和tabletop），将这组查询输入到上述的场景分解pipeline中，获取这些物体的高斯表示，并用RANSAC来估计这些平面的几何形状，在物理模拟时作为碰撞表面。

重力方向是被估计为“floor”的平面几何的法向量。

##### Taichi MPM for gaussians

可以直接将高斯中心μ视为点云使用MPM，但质量不佳，有两个问题：

- 缺乏内部支撑，物体在与碰撞表面接触的时候回塌陷
- 当物体发生变形时，出现不期望的伪影

本文的方法超越了简单的基于点的物理模拟，利用了特定于高斯的信息，如各向同性的不透明度和高斯协方差，在变形过程中进行体积保持和协方差修改，以解决上述两个挑战。

解决方案：

- Implicit Volume Preservation ： 在从少量真实世界图像中模拟物理现象时，体积保持是一个未解决的挑战。

  例如，没有体积保持，模拟一个排球撞击地面时会在碰撞时塌陷。因此，我们提出了一种使用高斯的不透明度和协方差的隐式体积保持技术。具体来说，我们首先使用协方差和不透明度信息按照PhysGaussian的方法在表面高斯的圆盘上采样点以增加表面点的密度。通过增加表面点的密度，我们随后填充从对象中心到表面的透明支撑粒子。填充粒子的透明性旨在确保在T = 0时的渲染质量一致，从而实现从静态场景到动态场景的平滑连续过渡。

- Estimating Rotation : 当物体发生变形的时候，需要校正协方差矩阵的旋转分量，否则可能会产生伪影。PhysGaussian也注意到了这个问题，尝试使用MPM的变形梯度F来更新高斯的协方差。然而变形梯度主要捕捉局部变化，当弹性物体发生大变形的时候，这种方法就会失效。因此我们提出用法线来估算弹性物体的旋转矩阵。

  3DGS难以在稀疏重建时估计法线，因此本文采用了一种基于神经网络的方法。

  具体来说，对于每个要模拟的高斯，我们找到其两个最近的邻居。这三个高斯的中心形成一个平面，我们使用该平面法线的旋转作为整个物体动态的代理。与从变形估算的旋转相比，我们的方法在物体发生大变形时产生的伪影更少。

#### Experiments

dataset : deep blending, Mip-NeRF360, custom dataset

## Segment Any 3D Gaussians

https://jumpat.github.io/SAGA

### motivation

过去的方法：

- 使用2D模型中提取2D特征，提升到3D空间中，用3D特征的相似性判断两个带你是否属于同一对象：速度较快，但分割粒度粗，因为缺乏解析嵌入特征的机制（e.g. 分割解码层）

> related word中提到：仅仅依靠欧几里得距离或余弦距离时无法充分利用嵌入在高维视觉特征中的信息，因此此类方法的分割质量是有限的。

- (segment anything nerf)直接将细粒度的2D分割结果投影到3Dmask grid上：产生精确的结果，但是由于多次执行基础模型和体渲染，导致过大时间开销

3DGS绕过对广泛且空的空间的处理，提供丰富的显式信息，成为分割任务的理想选择

SAGA避免了推理期间多次前向传递 2D 分割模型的时间消耗。蒸馏是通过基于 SAM [23] 自动提取的掩码来训练高斯的 3D 特征实现的。在推理过程中，根据输入提示生成一组查询，然后通过高效的特征匹配检索预期的高斯。支持点、涂鸦和掩码在内的各种提示类型。

### method

#### SAM

提供一个输入图片$I$和提示（可以是point，涂鸦或者mask）$P$，得到一个掩码$M$
$$
M = SAM(I,P)
$$

#### Overall pipeline

##### 训练阶段：

给定一个预训练完成的3DGS model $G$，输入图片$I$

对于一组图片，使用SAM得到一组2D特征图$F^{SAM}_I∈R^{C^{SAM}×H×W}$，和一组不同粒度的掩码$M^{SAM}_I$

在每个高斯上训练一个低维特征$f_g∈R^C$，通过SAM-guidance loss做监督。

> 没太看懂的correspondence loss的定义
>
> To further enhance the feature compactness, we derive point-wise correspondences from extracted masks and distills them into the features (i.e., the correspondence loss).

##### 推理阶段：

从给定的提示$P$中生成一组查询$Q$，用$Q$​通过特征匹配来检索3DGS，此后用3DGS提供的类似点云的丰富显式先验来做后处理

#### Training Features for Gaussians

##### SAM-guidance Loss

先将SAM的特征通过MLP $φ$降维到和高斯相同
$$
F'_I=φ(F^{SAM}_I)
$$
对于每一个mask，做一个maked平均池化，比如一个pikachu的mask，将皮卡丘上所有像素点的特征平均池化成一个特征向量(论文中称为Query)$T^M∈R^C$，现在$T^M$就代表pikachu

![image-20240528161206976](./assets/image-20240528161206976.png)

![image-20240528161215385](./assets/image-20240528161215385.png)

代码对应如下：

![image-20240528165428089](./assets/image-20240528165428089.png)

然后将render出的特征图和$T^M$做点积，相当于使用$T_M$在feature map上做查询，询问哪些像素点是pikachu？如果是，该像素上的feature点乘$T^M$就会得到较高的值（余弦相似度高），从而得到分割图$P_M$，再通过Sigmoid归一化成概率图

![image-20240528172903491](./assets/image-20240528172903491.png)

将查询到的分割图和真实的分割图做二值交叉熵进行训练

![image-20240528174904961](./assets/image-20240528174904961.png)

##### Correspondence Loss

TODO : 总之后面在看，先看推理

### Inference

##### SAM-based Prompt

可以直接使用SAM低维特征做查询。

先将prompts给SAM生成一个精确的2D分割掩码$M_v^{ref}$

在这个掩码上做平均池化得到一个查询$Q^{mask}$[注: 查询应该是一个C维特征向量]

用$Q^{mask}$分割2D渲染特征图$F^r_v$得到2D分割掩码$M_v^{temp}$

如果$M_v^{ref}$和$M_v^{temp}$的交集占据了$M_v^{ref}$超过90%的空间，$Q^{mask}$就作为正式的查询。

否则，使用K-means算法来从低维特征中提取另一组查询$Q^{kmans}_v$

使用这个策略是因为分割目标可能包含许多components，不能简单的使用masked平均池化来捕获

---

获得查询$Q^{SAM}_v=Q^{mask}_v\quad or\quad   Q^{kmeans}_v$后，用点乘计算每个高斯点的特征$f_g$和查询$Q$的相似度，如果查询分数大于自适应阈值（所有分数的均值和标准差之和）就认为是所要的点

## OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary Understanding

website：https://3d-aigc.github.io/OpenGaussian.

### motivation

首先举出现存特征GS的管线：
![image-20240624133901178](./assets/image-20240624133901178.png)

提出两个问题

- 对被遮挡的物体无法识别，失去了3DGS本身固有的能力
- 不是point-level的语义理解，对具身智能任务失效

因此提出了一个拥有3D point-level open-vocabulary understanding的模型

那么如果想要实现点级别的查询，一般可以通过以下流程，查询每个点的相似度：
![image-20240624135833239](./assets/image-20240624135833239.png)

> 当然，个人觉得并不一定要用语言，池化特征也挺好用的

但是其他模型效果不好，归结为以下原因：

- 因为512维特征嵌入困难（内存和速度限制），所以采用了降维或量化（这里提到一篇没看过的文章），这损害了特征的精度
- **2D-3D对应关系不准确**：alpha混合渲染技术基于不透明度权重累积3D点的值以渲染2D像素，这阻碍了2D与3D之间的一对一对应关系的建立。因此，在2D和3D解释之间出现了性能不匹配的情况。

为了解决上述挑战，提出了以下方法，在point level上学习具有区别性和一致性特征的方法，既能在对象之间也能在对象内部保持一致性。我买的方法将高维无损的CLIP特征和3D高斯点关联起来，实现了开放词汇的3D场景理解。

- 使用提出的掩码内平滑损失和掩码间对比损失来训练在3D点级别上既有区别性又有3D一致性的实例特征，利用来自SAM [23]的布尔掩码而无需跨帧关联；
- 引入两级粗到细的码本来离散化实例特征，从而生成离散的3D实例簇；
- 提出基于IoU和特征距离的实例级2D-3D关联方法，将多个视角的CLIP特征关联到每个3D实例。
