<!DOCTYPE html><html lang="en" data-beasties-container><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="/favicon.svg"><script>!function(){const e=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,t=localStorage.getItem("vueuse-color-scheme")||"auto";("dark"===t||e&&"light"!==t)&&document.documentElement.classList.toggle("dark",!0)}()</script><style type="text/css">:root{color-scheme:light dark;--va-c-bg:#fff}html{background-color:var(--va-c-bg)}</style><script>const locale=localStorage.getItem("valaxy-locale")||"zh-CN";document.documentElement.setAttribute("lang",locale)</script><script type="module" async crossorigin src="/ayene-no-blog/assets/app.CZR6THSy.js"></script><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/framework.DmXNyegR.js"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/chunks/@vueuse/motion.Bf6_DV_X.js"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/chunks/vue-router.CDV2LZ_i.js"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/chunks/dayjs.BdcnXKr1.js"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/chunks/vue-i18n.DfFCechf.js"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/chunks/pinia.DvPpdoxl.js"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/chunks/nprogress.Bru8d7fl.js"><link rel="stylesheet" crossorigin href="/ayene-no-blog/assets/app.Dgo4sgoW.css"><link rel="stylesheet" crossorigin href="/ayene-no-blog/assets/group-icons.Bm1YPxXM.css"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/post.sr24ljMs.js"><link rel="stylesheet" href="/ayene-no-blog/assets/group-icons.Bm1YPxXM.css"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/论文阅读笔记.9-pe8oPE.js"><title>论文阅读笔记，各种各样的 - ayane no blog</title><script id="check-mac-os" async>document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap"><meta property="og:locale:alternate" content="en"><meta name="description" content="我从来没有觉得学图形学开心过。"><meta property="og:description" content="我从来没有觉得学图形学开心过。"><meta property="og:locale" content="zh-CN"><meta property="og:site_name" content="ayane no blog"><meta property="og:title" content="论文阅读笔记，各种各样的"><meta property="og:image" content="/favicon.svg"><meta property="og:type" content="website"><meta property="og:url" content="https://pat-chou-li.github.io/ayene-no-blog/"><link rel="icon" href="/favicon.svg" type="image/svg+xml"><meta name="generator" content="Valaxy 0.26.10"><meta name="theme-color" content="#fff"><meta name="msapplication-TileColor" content="#fff"><meta name="twitter:card" content="summary_large_image"></head><body><div id="app" data-server-rendered="true"><!--[--><!--[--><!--[--><!----><!----><div class="yun-page-header-gradient" style="--gradient-from:161 196 253;--gradient-to:194 233 251"></div><!----><!----><!----><canvas class="fireworks"></canvas><!--[--><div class="yun-bg"></div><!--]--><div class="yun-page-loading" absolute left-0 right-0 bottom-0 top-0 flex justify="center" items-center z-10 bg="$va-c-bg" data-v-673bc094><div class="spinner" data-v-673bc094></div></div><a href="#" class="back-to-top yun-icon-btn bg-$va-c-bg-soft shadow-md"><div class="size-8" i-ri-arrow-up-s-line></div><svg class="progress-circle-container" viewBox="0 0 100 100"><circle stroke-dasharray="301.59289474462014 301.59289474462014" stroke-dashoffset="301.59289474462014" stroke="currentColor" stroke-width="2" stroke-linecap="round" class="progress-circle" cx="50" cy="50" r="48" fill="none"/></svg></a><!-- TODO --><!-- <YunDock /> --><!--]--><!--[--><!--]--><!----><!--]--><!--[--><div flex="~" class="w-full m-auto justify-center items-start gap-4 mt-12 md:mt-24"><!--[--><!----><main class="yun-main lt-md:w-full" flex="~ center"><!--[--><div class="content w-full md:w-3xl lg:w-2xl xl:w-2xl 2xl:w-4xl" flex="~ col grow" p="lt-md:0"><div class="yun-card flex-center rounded-2 relative" flex="col" min-h="100px" bg="$va-c-bg-light" m="0" style><!----><!----><!--[--><div class="mt-8 mb-4"><!--[--><header class="post-header"><h1 p="2" text="2xl center" font="serif black" style="" class="post-title flex-center"><!----><span inline-flex class="leading-none">论文阅读笔记，各种各样的</span></h1></header><!--]--></div><!--[--><!--[--><!--[--><!--[--><!----><!----><!----><div flex="~ center" text="sm" class="flex-col gap-2! post-meta gap-4"><div class="post-time flex items-center gap-4"><span class="posted-time inline-flex-center gap-1" title="发表于2023-11-01 00:00:00"><div class="inline-block" i-ri-calendar-line></div><time class="op-80">2023-11-01</time></span><!----></div><!--[--><!--]--><div class="inline-flex-center gap-4"><!----><!----></div></div><!--]--><div class="inline-flex mt-2" text="sm" py="1"><a href="/ayene-no-blog/categories?category=%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0" class="transition post-category inline-flex-center text-xs border-$va-c-divider" px-2 h="7" border rounded-full hover="bg-blue-500 text-white"><div m="x-1" inline-flex i-ri-folder-2-line></div><span>阅读笔记</span></a><span mx="2"></span><div class="post-tags inline-flex" items="center" gap="1" flex="wrap 1" justify="end"><!--[--><a href="/ayene-no-blog/tags/?tag=%E5%9B%BE%E5%BD%A2%E5%AD%A6" class="transition post-tag inline-flex-center text-xs border-$va-c-divider" px-2 h="7" rounded-full border hover="bg-blue-500 text-white"><span>图形学</span></a><a href="/ayene-no-blog/tags/?tag=NeRF" class="transition post-tag inline-flex-center text-xs border-$va-c-divider" px-2 h="7" rounded-full border hover="bg-blue-500 text-white"><span>NeRF</span></a><a href="/ayene-no-blog/tags/?tag=3DGS" class="transition post-tag inline-flex-center text-xs border-$va-c-divider" px-2 h="7" rounded-full border hover="bg-blue-500 text-white"><span>3DGS</span></a><!--]--></div></div><!--]--><!--]--><!--]--><div p="x-4 b-8" class="sm:px-6 lg:px-12 xl:px-16" w="full"><!--[--><!--]--><!--[--><!-- <Transition appear> --><article class="markdown-body"><!--[--><!----><blockquote class="yun-time-warning" op="80">本文最后更新于 2 年前，文中所描述的信息可能已发生改变。</blockquote><!--[--><!--]--><!--[--><h2 id="nerfren-2022-cvpr" tabindex="-1">nerfren（2022 CVPR） <a class="header-anchor" href="#nerfren-2022-cvpr" aria-label="Permalink to &quot;nerfren（2022 CVPR）&quot;">​</a></h2><ul><li>Category：光照反射</li><li>Project: <a href="https://bennyguo.github.io/nerfren/" target="_blank" rel="noreferrer">https://bennyguo.github.io/nerfren/</a></li><li>Code: <a href="https://github.com/bennyguo/nerfren" target="_blank" rel="noreferrer">https://github.com/bennyguo/nerfren</a></li><li>Paper: <a href="https://arxiv.org/pdf/2111.15234.pdf" target="_blank" rel="noreferrer">https://arxiv.org/pdf/2111.15234.pdf</a></li></ul><h3 id="motivation" tabindex="-1">Motivation <a class="header-anchor" href="#motivation" aria-label="Permalink to &quot;Motivation&quot;">​</a></h3><p>原来的nerf:</p><ul><li><p>缺乏对反射的建模，对于镜子反射的物体，在体渲染的模型下，相当于在镜子里存在一个虚拟的空间，从而导致错误的深度估计，<strong>同时会将反射的物体建模为半透明的，从而导致雾状几何</strong>。</p><blockquote><p>其实我觉得这个很符合直觉，体渲染并不是真正的光线追踪，不能做到反射的光线也只能用虚拟的空间来欺骗视觉了</p></blockquote><ul><li><p>先理解什么是雾状，将其和一般的半透明（如一个泡泡）区分开来，在一块区域中，每个采样点都有一些几何存在，但都是半透明的，看见的就是雾状，而一个泡泡看起来很清晰，是因为它内部是不存在发光的采样点的，而只在表面存在，这个是透明。</p></li><li><p>在ref-nerf中提到，NeRF倾向于在物体内部使用各向同性发光点来“伪造”镜面反射，而不是由表面点发射的视相关辐射，导致物体具有半透明性或雾状壳。</p></li></ul><img src="/ayene-no-blog/assets/image-20231123164551426-1700729325052-18-1700729333233-20-1701848684483-1.BfHHcXFb.png" alt="image-20231123164551426" style="zoom:50%"><ul><li>↑可视化的density，在正确的物理世界中，镜子应该只在表面有密度，而nerf中是认为镜子有一个半透明的表面，然后在镜子内部的不同深度出现了发光点，从而是雾状的。<ul><li>这样还会导致一个问题，这样训练出来的nerf，在镜子的交界附近也能看到那个镜子中的虚拟世界，也就是所谓伪影。</li></ul></li></ul><img src="/ayene-no-blog/assets/image-20231118152612523-1700729317192-16-1701848694294-3.BdYZO_-r.png" alt="image-20231118152612523" style="zoom:80%"><ul><li>所以问题就回到如何让nerf学习到基于表面的反射，如果用原来的nerf结构，最后的结果应该是：最后的镜子上有很高的density，即认为镜子是不透明物体，同时镜子平面上每个采样点的c来表示镜面反射，通过一些光学模型作为几何先验进行正则化，或者是修改模型等等，这篇文章就在解决这个问题。</li></ul></li></ul><h3 id="method" tabindex="-1">Method <a class="header-anchor" href="#method" aria-label="Permalink to &quot;Method&quot;">​</a></h3><ul><li>将神经辐射场分解为独立的transmitted部分和reflected部分，同时学习一个reflection fraction map <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span>，最后image由两者通过各自的辐射场渲染，最后相加得到，即</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mo>=</mo><msub><mi>I</mi><mi>t</mi></msub><mo>+</mo><mi>β</mi><mo>∗</mo><msub><mi>I</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">I = I_t+\beta*I_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">I</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2806em"><span style="top:-2.55em;margin-left:-.0785em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.05278em">β</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0785em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></span></p><ul><li><p>当然，it is highly under-constrained，如果不更改nerf就简单的将其分解为两个场，其实就是做一个无监督任务，那么会有无穷多种二分类的分解方法最后合成得到正确的视图。常见的有：分解为full场（渲染完整视图）和empty，分解为2个full场，以及两者之间。那么自然就要对训练做出约束了，基于以下三个假设：</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span>只应该和transmitted有关，因为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span>其实表示了物质的材料（我理解是反射率），而不应该和被它反射的物体有关系</li><li>transmitted的深度图应该是局部平滑的，因为现实中大部分反射体都是光滑平面（疑问：可是这个场也要用来重建其他物体，从case来看就是只盯着镜面重建了，但是case中镜面与其他物体的交界处好像重建的也还行）</li><li>reflected部分只需要简单的几何，因为大部分情况下我们只能从有限的观察方向看到反射图像。（意思是正常的三维重建我们追求360度全方位重建，但是重建出镜面中反射的物体我们只能看见一部分）</li></ul></li><li><p>自然，三个假设做出三种约束</p><ul><li><p>设计特定的网络结构，分解成两个场，这里的α（reflection fraction map）只和transmitted有关</p><ul><li>整个镜面反射r场都和视线无关是一个比较反直觉的设计，有些不能理解。在原来的折射场中保持了方向是为了保留高光。在我看来是完全抛弃了物理假设，也就是图形学中的PBR公式，BRDF项等一系列物理假设，把镜子中的图像认为是藏在镜子中的另一个世界，不同视角带来的差异是因为摄像机在不同位置带来的，而并不是由于反射的光来源于不同位置造成的。比如看一个鼠标，从正面，上面，左边的三视图看到的是不同的鼠标图像，这是视线无关的场，场中某个点就是鼠标的组成部分，不会因为视角变化而导致鼠标本身的样子发生了变化，但鼠标上的高光真的是会因为你视角不一样而发生改变的。</li></ul><img src="/ayene-no-blog/assets/image-20231118153245250-1700729315140-14-1701848705589-5.CEc70LkZ.png" alt="image-20231118153245250" style="zoom:80%"></li><li><p>应用深度平滑先验</p><ul><li>p是某像素，q是周围的8个像素，t*是估计深度（通过体渲染时采样点的深度加权求和得到），pq的深度差距越大以及颜色差距越大，都会导致Loss的增大，保证transmitted场的深度局部平滑</li></ul></li></ul><figure><img src="/ayene-no-blog/assets/image-20231118160816671-1701848707092-7.CYSAyi4B.png" alt="image-20231118160816671" loading="lazy" decoding="async"></figure><ul><li><p>双向深度一致性约束（bidirectional depth consistency constraint）</p><p>镜面反射场最好描述了一个什么物体？作者的推导逻辑是，因为只有在很少的视角能观测-&gt;简单的几何，一种光线只能击中一个表面的几何-&gt;应该是一个不透明表面，一个贴了纹理的壳</p><blockquote><p>其实个人感觉故事讲得不好，这个约束在于将表面约束的足够薄，不应该由只能从正面看镜子来导出，其实将镜面反射简化为一张2D纹理是图形学渲染中的常见手段，但是确实不是很物理也不好讲故事，我猜作者是受此启发的，但是故事没太讲好，或者是我没get到，或者说从ref-nerf受启发也很合理，要将发光体约束在表面就要使得物体足够薄]</p></blockquote><ul><li><p>定义了一个<strong>反向</strong>的深度，也就是从最<strong>远</strong>采样点的深度加权求和得到</p></li><li><p>定义了一个<strong>正向</strong>的深度，也就是从最<strong>近</strong>采样点的深度加权求和得到</p></li><li><p>这里比较容易让人产生疑惑，从近到远求和以及从远到近求和结果有什么区别？其实权重不一样，同一个采样点，从近到远求和的权重取决于它之前的点的不透明度，而另一个取决于它之后的点的不透明度，因此如果有两个表面，正向深度就会更偏向于前面的表面，因为后面的表面权重被前面的表面大幅降低了。<img src="/ayene-no-blog/assets/image-20231118164117531-1700729312546-11-1701848708687-9.DWaneOWD.png" alt="image-20231118164117531"></p></li><li><p>来看一下不同的几何下正向深度和反向深度长什么样，横轴是采样点的距离，蓝线的纵轴表示密度，所以a表示遇到了两个表面，b是雾，c则是理想的镜面反射体——一张贴了纹理的固体。那么作者提出的约束就很简单，让两个深度足够近。</p><figure><img src="/ayene-no-blog/assets/image-20231118174744343-1701848710428-11.DQQqwx0H.png" alt="image-20231118174744343" loading="lazy" decoding="async"></figure></li></ul></li></ul></li><li><p>另外，对于有挑战性的场景，比如镜子，可以手动的提供mask，来使得场景被更正确的分解。（其实感觉这个是不是有点过分了，破坏了end to end，加入了手工的方法来大幅提升精度，不过某些先语义分割再nerf的方法好像也殊途同归，我训练个专门识别反射体的segment的cnn/transformer作为预输入语义是否有一定效果？就是novelty几乎没有，还会有个臃肿的模型，不过发散的讲，更多种分解场的方法，以及不同对应的预输入？）</p><ul><li>具体是，利用提供的mask图作为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span>的约束，要在mask区域<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span>尽可能大，而其他区域尽可能小。</li></ul></li></ul><img src="/ayene-no-blog/assets/IMG_9445-1701848735342-13.7YUuwDpA.JPG" alt="IMG_9445"><figure><img src="/ayene-no-blog/assets/image-20231118150053296-1701848737166-15.BPItGjGU.png" alt="image-20231118150053296" loading="lazy" decoding="async"></figure><h3 id="limitation" tabindex="-1">Limitation <a class="header-anchor" href="#limitation" aria-label="Permalink to &quot;Limitation&quot;">​</a></h3><ul><li><p>这篇文章太注重平面反射的建模了，自然会产生一个问题：对于非平面的镜面反射不能有很好的效果，比如下图的弯曲镜面<img src="/ayene-no-blog/assets/image-20231118181253580-1701848749616-17.lp7miFml.png" alt="image-20231118181253580"></p></li><li><p>另一个是没有模拟出菲涅尔效应，这里作者只是提了一下，没有给出failure case，其实个人感觉反射系数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span>近似了一种菲涅尔系数，不过没有进行建模和约束，仅靠MLP学习，所以大概率学到的不是物理意义上的菲涅尔。</p></li><li><p>虽然文章没写，但是感觉过于精细的调参也是一个缺点，它要在早期屏蔽view direction以及它自己新增的几何约束，才能出比较好的效果，否则基本就是分解失败</p></li></ul><figure><img src="/ayene-no-blog/assets/image-20231120191622445-1701848750885-19.D3WvtIxi.png" alt="image-20231120191622445" loading="lazy" decoding="async"></figure><h2 id="ref-nerf-2021-cvpr" tabindex="-1">Ref-Nerf（2021 CVPR） <a class="header-anchor" href="#ref-nerf-2021-cvpr" aria-label="Permalink to &quot;Ref-Nerf（2021 CVPR）&quot;">​</a></h2><ul><li>Project: <a href="https://dorverbin.github.io/refnerf/" target="_blank" rel="noreferrer">https://dorverbin.github.io/refnerf/</a></li><li>Code: <a href="https://github.com/google-research/multinerf" target="_blank" rel="noreferrer">https://github.com/google-research/multinerf</a></li><li>Paper: <a href="https://arxiv.org/pdf/2112.03907.pdf" target="_blank" rel="noreferrer">https://arxiv.org/pdf/2112.03907.pdf</a></li></ul><h3 id="motivation-1" tabindex="-1">motivation <a class="header-anchor" href="#motivation-1" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><ul><li>以view direction为MLP的输入，不方便进行插值，因为radiance function关于view direction变化太快</li><li>nerf倾向于用在物体内部的各向同性的发光点来伪造镜面反射，导致物体呈现出半透明或者雾状。也因此导致法向量充满噪声，难以使用。</li></ul><h3 id="contribution" tabindex="-1">contribution <a class="header-anchor" href="#contribution" aria-label="Permalink to &quot;contribution&quot;">​</a></h3><ul><li><p>重新参数化nerf，将view direction替换为出射方向</p></li><li><p>提出一种IPE，使得即使分开建模漫反射和镜面反射时，radiance function在不同的纹理和材质下仍可以平滑插值（还没看mip-Nerf，先TODO，有球谐函数和高斯的知识，也可能和3DGS有共通之处）</p></li><li><p>一种正则化方法，使得体积密度能够集中于表面，从而优化法向量的精确度</p></li></ul><h3 id="method-1" tabindex="-1">method <a class="header-anchor" href="#method-1" aria-label="Permalink to &quot;method&quot;">​</a></h3><ul><li>重新参数化nerf，将view direction替换为出射方向<ul><li>用一个四个平行光的简单场景为例，黑色的是可见（但是没有光）部分，棋盘格是不可见部分。</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">\omega_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>表示出射方向，永远指向平行光所以不随着x变动，因此第二行的图中这些光都是平行线，如果我们要预测<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">\omega_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>不取在这四个平行光所在方向的参数，Directional MLP只需要进行平滑的插值。</li><li>下方的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>n</mi><mo>^</mo></mover><mo separator="true">,</mo><mi>ρ</mi><mo separator="true">,</mo><msub><mi>c</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">\hat{n}, ρ, c_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">n</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal">ρ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>表示ref-Nerf对这三个量进行了建模，将它从镜面反射中分离开来，最明显的变化是使得右边第二列，相比于第一列，让Directional MLP不需要对漫反射进行插值。</li></ul></li></ul><img src="/ayene-no-blog/assets/image-20231123144423362-1701848752833-21.B3s2b1rz.png" alt="image-20231123144423362" style="zoom:50%"><ul><li>网络的框架，ρ表示粗糙度，和IDE有关，法向量和视线方向用来算出反射方向<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">\omega_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>，以及位置<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span>通过空间MLP编码后得到的特征向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal">b</span></span></span></span>,还有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo separator="true">⋅</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">n·d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal">n</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal">d</span></span></span></span>(为了模拟菲涅尔项，以及其他可能的BRDF函数)来计算高光反射的颜色，最后和漫反射颜色一起通过色调映射输出srgb颜色。[色调映射：将颜色从线性空间转换到srgb并且将颜色范围限制到[0,1]]</li></ul><figure><img src="/ayene-no-blog/assets/image-20231123150232055-1701848754511-23.wh6dwf8O.png" alt loading="lazy" decoding="async"></figure><ul><li><p>提出一种IPE，使得即使分开建模漫反射和镜面反射时，radiance function在不同的纹理和材质下仍可以平滑插值（还没看mip-Nerf，先TODO，有球谐函数和高斯的知识，也可能和3DGS有共通之处）</p></li><li><p>一种正则化方法，使得体积密度能够集中于表面，从而优化法向量的精确度</p><ul><li>针对体密度梯度计算出的法向量充满噪声，不够平滑的问题，提出第一个正则化，就是在MLP输出另外一个法向量，要求这两个法向量足够接近。作者提到：MLP预测的法线更加平滑，这和vanilla NeRF提到的位置编码来自于同一理论，论文Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains中提到MLP的这一特性，以及克服MLP趋向于学习低频特征而不擅长学习高频特征的方法，包括原始NeRF中的位置编码。当然，这里是反过来利用MLP学习低频特征的特性，来预测一个尽可能平滑的法线。</li></ul><figure><img src="data:image/png;base64,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" alt="image-20231123160605760" loading="lazy" decoding="async"></figure><ul><li>第二个问题是针对NeRF总喜欢用一些物体表面后的发光点的问题，提出要让高可见度(也就是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.0269em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>高)和光线同向的法线受到惩罚，这样物体存在于物体背面又能够发光被我们看见的物体就会减少。通俗的讲，让NeRF倾向于让面对光线的那一部分表面来表达物体的颜色，而不要让物体表面变成半透明，而让里面的物体发光来解释所看到的颜色。</li></ul><figure><img src="/ayene-no-blog/assets/image-20231123161646209-1701848758427-27.BWI1at1Y.png" alt="image-20231123161646209" loading="lazy" decoding="async"></figure></li></ul><h2 id="nerf2mesh-2023-iccv" tabindex="-1">Nerf2Mesh（2023 ICCV） <a class="header-anchor" href="#nerf2mesh-2023-iccv" aria-label="Permalink to &quot;Nerf2Mesh（2023 ICCV）&quot;">​</a></h2><ul><li>Category：nerf2mesh, nerf-texture</li><li>Project: <a href="https://me.kiui.moe/nerf2mesh/" target="_blank" rel="noreferrer">https://me.kiui.moe/nerf2mesh/</a></li><li>Code: <a href="https://github.com/ashawkey/nerf2mesh" target="_blank" rel="noreferrer">https://github.com/ashawkey/nerf2mesh</a></li><li>Paper: <a href="https://arxiv.org/pdf/2303.02091.pdf" target="_blank" rel="noreferrer">https://arxiv.org/pdf/2303.02091.pdf</a></li></ul><p>好像不是很看得懂，尽力吧</p><p>这个翻译的不错：<a href="https://blog.csdn.net/m0_50910915/article/details/131823539" target="_blank" rel="noreferrer">https://blog.csdn.net/m0_50910915/article/details/131823539</a></p><p>这个是论文思路解释：<a href="https://blog.csdn.net/qq_40514113/article/details/129759065" target="_blank" rel="noreferrer">https://blog.csdn.net/qq_40514113/article/details/129759065</a></p><h3 id="motivation-2" tabindex="-1">motivation <a class="header-anchor" href="#motivation-2" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>关于深度学习重建mesh的方法中，</p><ul><li>mobile-nerf重建的mesh质量不佳，而且纹理是在特征空间中而不是RGB空间中，这使得纹理编辑成为难题</li><li>SDF相关的工作中，提取了过度平滑的几何，难以model thin structures。</li><li>通过Marching Cubes产生的网格有过多的冗余顶点和表面</li><li>NVdiffrec使用可微光栅化来优化可变形四边形网格，但只能用于对象级的重建，并且在复杂的多边形上失败</li></ul><h3 id="contribution-1" tabindex="-1">contribution <a class="header-anchor" href="#contribution-1" aria-label="Permalink to &quot;contribution&quot;">​</a></h3><ul><li>提出了Nerf2Mesh，从多视角RGB图像中提取网格，细化从Nerf中提取的粗网格，实现几何和外观的联合优化。</li><li>提出了一种迭代式的网格refine算法，能够自适应的调整表面密度，根据重投影的二维图像误差对复杂表面进行细分，对简单表面进行抽取（decimated）（???）</li><li>与最近方法相比，更好的网格质量，更小的网格尺寸，更好的渲染质量。</li></ul><h3 id="realted-work" tabindex="-1">Realted Work <a class="header-anchor" href="#realted-work" aria-label="Permalink to &quot;Realted Work&quot;">​</a></h3><p>Surface mesh for Scene Reconstruction方面不是很看得懂，直接看从NeRF中提取网格的工作吧。</p><p>Nerf使用体积密度场表示几何，并不形成确定的表面，因此在提取表面网格上也受到限制。一种流行的方法是学习SDF，但是SDF的表面太过平滑，无法学习thin construction，还提到了SAMURAI，mobielNerf，以及提到两篇工作发现<strong>指数密度激活函数</strong>可以帮助集中密度，形成更好的表面。</p><h3 id="method-2" tabindex="-1">Method <a class="header-anchor" href="#method-2" aria-label="Permalink to &quot;Method&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/01815b7515bd48249535fc5064994770.BC5NspTr.png" alt="img" loading="lazy" decoding="async"></figure><p>先训练一个grid-based的NeRF（InstantNGP），分为几何和外观联合优化</p><ul><li>几何上，先用Marching Cubes提取粗网格，然后通过文中提出的算法细化网格</li><li>外观上，通过颜色网格学习的，并分解为漫反射和镝面反射项。收敛后，我们可以导出精细网格，展开其UV坐标并烘焙纹理。</li></ul><h4 id="stage1" tabindex="-1">stage1 <a class="header-anchor" href="#stage1" aria-label="Permalink to &quot;stage1&quot;">​</a></h4><p>大致的流程图里已经很清楚了，不过文中提到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">f_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1076em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>可以被烘焙为纹理，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>L</mi><msub><mi>P</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">MLP_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>可以在fraa shader中实现，因此镜面反射可以被导出和渲染，很难理解这件事，特征是什么被提取为纹理的，着色器里怎么写MLP等等，感觉在这一领域欠缺了很多知识。</p><p>另外，本文的方法直接将光照烘焙为纹理，因为估计环境光具有挑战性，而且很可能会导致渲染质量降低。</p><p>当然，为了约束分解，还加入了L2正则化，原文提到这是为了促进漫反射和镜面反射的分解，于是对镜面反射应用L2正则化。这里复习下L1和L2，L1正则化趋向于让模型获得稀疏解，即在某些权重上为0，L2则让模型趋向于获得较为平滑的解。</p><blockquote><p>也就是说这边是让不同位置上的高光反射不过于强烈，可以让高光的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">c_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>平滑一些……为什么促进了分解？已经完全看不懂了</p><p>你用L1正则化我还能理解，但是文中也没有什么详细解释，只能你说是那就是了</p></blockquote><figure><img src="/ayene-no-blog/assets/image-20231207153921810.BIUWWJ6w.png" alt="image-20231207153921810" loading="lazy" decoding="async"></figure><p>为了使得表面更加锐利，采用交叉熵正则化。</p><p>兴许这个还是比较好理解的，交叉熵正则化鼓励权重的稀疏分布，也就是说让体渲染上每个点的体密度尽可能大，而不是形成雾状，这样就使得体密度集中在表面上了。</p><blockquote><p>求求作者解释一下吧，靠猜太累了，当然也可能是我读的论文太少了，这个作者认为不需要解释</p></blockquote><figure><img src="/ayene-no-blog/assets/image-20231207155808414.B2WuE4dE.png" alt="image-20231207155808414" loading="lazy" decoding="async"></figure><h4 id="stage2" tabindex="-1">stage2 <a class="header-anchor" href="#stage2" aria-label="Permalink to &quot;stage2&quot;">​</a></h4><ul><li><p>Appearance refinement：通过nvdiffrast进行可微渲染，仍然可以使用img逐像素的损失，来进行外观优化。（按文章脉络应该是在优化纹理，按图来说应该还是在优化<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>L</mi><msub><mi>P</mi><mn>1</mn></msub><mtext>，</mtext><mi>M</mi><mi>L</mi><msub><mi>P</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">MLP_1，MLP_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord cjk_fallback">，</span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的参数）</p></li><li><p>Iterative mesh refinement.</p><ul><li>顶点优化比较容易理解，对于每个顶点设置一个偏移量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">△</mi><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\triangle v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord">△</span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>，通过可微渲染反向传播image-space loss gradients（NVdiffrec提出）来优化这个偏移量。</li><li>面（face）优化就比较复杂，因为网格面不可微。本文提出的训练策略是将2D图像渲染误差重投影到对应的网格面上，累积网格面的误差，然后给出一个阈值，高于这个误差的做网格细化，低于的做网格抽取并重新网格化来降低网格密度。</li></ul><blockquote><p>复习一下obj格式吧，顶点用三维向量描述顶点位置，面则是由顶点索引组成，表示这个面由哪几个顶点链接得到，注意顶点索引顺序是有影响的，这会决定面的方向</p><p>网格抽取（decimate）似乎是一个图形学上几何的相关知识，读不懂还是图形基础不够</p></blockquote></li></ul><p>网格更新之后就重新初始化顶点便宜和面误差</p><ul><li>Unbounded scene.</li></ul><p>无界场景优化，没有读过相关文章，略过了。</p><h4 id="mesh-exportation" tabindex="-1">Mesh Exportation <a class="header-anchor" href="#mesh-exportation" aria-label="Permalink to &quot;Mesh Exportation&quot;">​</a></h4><p>将优化后的精细网格<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mrow><mi>f</mi><mi>i</mi><mi>n</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">M_{fine}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9694em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.109em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.10764em">f</span><span class="mord mathnormal mtight">in</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>用XAtlas解析UV坐标，然后烘焙漫反射颜色<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">c_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>和镜面反射特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">f_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1076em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>，分为生成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mi>d</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">I_d, I_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.0785em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0785em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></p><p>为了渲染镜面反射颜色，采用mobileNeRF中的方法，导出<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>L</mi><msub><mi>P</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">MLP_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的权重并合并到frag shader中</p><h3 id="limitation-1" tabindex="-1">Limitation <a class="header-anchor" href="#limitation-1" aria-label="Permalink to &quot;Limitation&quot;">​</a></h3><p>烘焙光照，无法relight，基于单通道光栅化，不能处理半透明</p><blockquote><p>读完了，但是和没读一样，不过还是有收获</p><ul><li>get一种约束体密度到表面的方法，可以对体渲染公式中的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.0037em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>做交叉熵损失</li><li>在网格重建方面还是缺少了很多的基础知识，比如MarchingCube，另外意识到NeRF可能不是网格重建的主要方法，SDF可能更加主流。</li><li>初步看到了可微渲染是什么东西，了解到一种可微渲染框架nvdiffrast</li></ul><p>以后如果做相关方向，可以再回来看看这篇文章。</p></blockquote><h2 id="pixelnerf" tabindex="-1"><strong>pixelNeRF</strong> <a class="header-anchor" href="#pixelnerf" aria-label="Permalink to &quot;**pixelNeRF**&quot;">​</a></h2><p><strong>pixelNeRF: Neural Radiance Fields from One or Few Images</strong></p><p><em>Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa</em> CVPR 2021, 3 Dec 2020</p><p>[<a href="https://arxiv.org/abs/2012.02190" target="_blank" rel="noreferrer">arXiv</a>] [<a href="https://alexyu.net/pixelnerf/" target="_blank" rel="noreferrer">Project</a>] [<a href="https://github.com/sxyu/pixel-nerf" target="_blank" rel="noreferrer">Github</a>]</p><p>这篇文章给出了PixelNeRF的简易实现：<a href="https://zhuanlan.zhihu.com/p/550890576" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/550890576</a></p><p>应该算是NeRF早期的Few-shot工作，引用量也比较高，就当做稀疏视角重建的启蒙作看吧</p><h3 id="realted-work-1" tabindex="-1">Realted Work <a class="header-anchor" href="#realted-work-1" aria-label="Permalink to &quot;Realted Work&quot;">​</a></h3><p>没有提NeRF相关的工作，都是之前的稀疏重建，提到了同期工作GRF。</p><h3 id="method-3" tabindex="-1">Method <a class="header-anchor" href="#method-3" aria-label="Permalink to &quot;Method&quot;">​</a></h3><p>motivation比较简单就合在method里，简单来说就是之前的NeRF只使用原来的图像进行训练，没有很好的利用原来图像的所有信息，所以当只有稀疏视角的时候新视角合成就有比较多的artifacts</p><p>这篇文章提出要用CNN提取输入的图像，形成一个feature volume，在推理的时候也使用这张特征图（额所以你为什么不叫feature map要叫volume），就能更充分的利用原来图片的信息了。</p><p>CNN Encoder由预训练的res-net34组成。</p><figure><img src="/ayene-no-blog/assets/image-20231215211057871.CB8fW7dd.png" alt="image-20231215211057871" loading="lazy" decoding="async"></figure><p>思想还是比较简单的，以单视角输入为例，将input view经过CNN Encoder得到一个Volume feature，即每个像素都有一个feature，即pixel-aligned，记作W，当需要进行新视角合成的时候，采样点从世界坐标映射到input view所在的view space，然后由最近的四个像素经双线性插值得到一个特征向量，将这个特征向量输入以下的网络</p><img src="/ayene-no-blog/assets/image-20231215211524634.UqCltvE6.png" alt="image-20231215211524634" style="zoom:50%"><p>值得注意的是当多视角输入的时候，得到的V是经过一个平均的后输入解下去的MLP的，这点应该还有优化空间，不同视角下对于空间中同一个点训练出来的特征应该是具有不同意义以及重要性的，比如要新合成的视角和正面比较近，那么背面的视图应该拥有较低的权重。</p><p>另外输入的时候直接使用view direction，没有经过位置编码，也不是在中途加入的，原文表示</p><ul><li>view direction可以作为不同视图的相关性和定位的信息</li><li>当新视角和已有视角更接近的时候，可以更依赖已有视角，否则应该更依赖学习出的prior</li></ul><blockquote><p>我怎么感觉还是没解释为什么不中途加入呢</p></blockquote><h2 id="regnerf" tabindex="-1"><strong>RegNeRF</strong> <a class="header-anchor" href="#regnerf" aria-label="Permalink to &quot;**RegNeRF**&quot;">​</a></h2><p>RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs</p><p><em>Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, Noha Radwan</em> CVPR 2022, 1 Dec 2021 [<a href="https://arxiv.org/abs/2112.00724" target="_blank" rel="noreferrer">arXiv</a>] [<a href="https://m-niemeyer.github.io/regnerf/index.html" target="_blank" rel="noreferrer">Project</a>] [<a href="https://github.com/google-research/google-research/tree/master/regnerf" target="_blank" rel="noreferrer">Code</a>] [<a href="https://github.com/yangjiheng/nerf_and_beyond_docs/blob/main/paper_discussions/RegNeRF.md" target="_blank" rel="noreferrer">Notes</a>]</p><p>总之先来两篇不错的阅读笔记：</p><p>顺便了解下李代数：<a href="https://zhuanlan.zhihu.com/p/532973564" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/532973564</a></p><p>fewshot工作总结：<a href="https://zhuanlan.zhihu.com/p/617570383" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/617570383</a></p><h3 id="motivation-3" tabindex="-1">motivation <a class="header-anchor" href="#motivation-3" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><ul><li>一些工作（MVSNeRF，IBRNet，GRF，PixelNerf）需要昂贵的预训练，通过“amortized inference”（摊余推断）与fine-tune来完成稀疏视角重建。尽管这些模型取得了令人期待的结果，但通过捕捉或渲染许多不同场景来获取必要的预训练数据可能成本过高。此外，这些技术在测试时可能无法很好地推广到新领域，并且可能由于稀疏输入数据的固有模糊性而产生模糊的伪影。</li></ul><blockquote><p>gpt: "摊余推断"（amortized inference）是指在模型的训练阶段进行的推断过程，其中模型被训练以在给定一些输入时，能够有效地生成输出，而不需要每次都进行全面的推断。这种方法的目标是通过训练模型来学习一种映射，使其在整个输入空间上都能产生良好的推断结果。</p><p>在文中提到的上下文中，摊余推断是指在测试时，通过使用已经训练好的模型，可以从仅有少量输入图像生成新颖的视图，而无需每次都重新进行完整的模型推断。这种方法有助于提高模型的效率，并使其更容易应用于实际场景，尤其是对于需要快速推断的应用。</p></blockquote><blockquote><p>还知道了一个常识：稀疏视角下有一个固有问题，就是会出现模糊和伪影，这里是在说这种做法还是没有解决模糊和伪影问题。</p></blockquote><ul><li>另一些工作是通过添加一些正则化，重新训练整个场景（例如引入深度监督的nerf，这里指Depth-supervised NeRF: Fewer Views and Faster Training for Free，以及引入clip的dietNeRF），但是现有方法要么过于依赖并非始终可用的外部信号（意思就是depth不是什么时候都能拿到的），要么只在低分辨率场景下（只提供high-level的信息，这里是针对clip）运行。</li></ul><h3 id="contribution-2" tabindex="-1">contribution <a class="header-anchor" href="#contribution-2" aria-label="Permalink to &quot;contribution&quot;">​</a></h3><ul><li>patch-based regularizer：用于新视角的深度图，可以减少伪影、增加几何质量</li><li>normalzing flow model：用于新视角的颜色，通过最大化渲染patch的对数似然（？），来避免颜色在不同viewpoint下的差异（消融实验来看是个没用的创新点，还很难看懂，这里直接跳过了）</li><li><strong>annealing strategy for sampling points：沿采样点的退火策略，首先在小范围内对场景内容进行采样，然后扩展到完整的场景边界，保证训练早期不出现分歧。</strong></li></ul><h3 id="method-4" tabindex="-1">method <a class="header-anchor" href="#method-4" aria-label="Permalink to &quot;method&quot;">​</a></h3><h4 id="深度平滑" tabindex="-1">深度平滑： <a class="header-anchor" href="#深度平滑" aria-label="Permalink to &quot;深度平滑：&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240106173912019.A32EnYiO.png" alt="image-20240106173912019" loading="lazy" decoding="async"></figure><p>从规定的可能的相机位姿矩阵中采样光线<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">r_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7167em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.0278em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.05724em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>，在patch范围内进行深度平滑，深度计算方法采用NeRF原代码中的方法。</p><blockquote><p>很常见的方法了，但是一百种NeRF改进有一百个实现，也有各种各样的理由，可以考虑作为一个小trick。</p></blockquote><h4 id="退火策略-sampling-space-annealing" tabindex="-1">退火策略（Sampling Space Annealing） <a class="header-anchor" href="#退火策略-sampling-space-annealing" aria-label="Permalink to &quot;退火策略（Sampling Space Annealing）&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240113161838002.iXNOlZ3E.png" alt="image-20240113161838002" loading="lazy" decoding="async"></figure><p>作者观察到，在稀疏输入的情况下，NeRF容易收敛到相机的近平面上，这虽然还原了输入视角，但是缺乏3D一致性，对新视角的合成就很不友好。</p><blockquote><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>:</mo><mtext>迭代次数，</mtext><msub><mi>t</mi><mi>n</mi></msub><mo>:</mo><mtext>近平面，</mtext><msub><mi>t</mi><mi>f</mi></msub><mo>:</mo><mtext>远平面，</mtext><msub><mi>t</mi><mi>m</mi></msub><mo>:</mo><mtext>中点</mtext></mrow><annotation encoding="application/x-tex">i:迭代次数，t_n:近平面，t_f:远平面，t_m:中点</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6595em"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord cjk_fallback">迭代次数，</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.9694em;vertical-align:-.2861em"></span><span class="mord cjk_fallback">近平面，</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.10764em">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord cjk_fallback">远平面，</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord cjk_fallback">中点</span></span></span></span></p></blockquote><p>可以注意到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.03588em">η</span></span></span></span>为0的时候<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>n</mi></msub><mo>=</mo><msub><mi>t</mi><mi>f</mi></msub><mo>=</mo><msub><mi>t</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">t_n=t_f=t_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7651em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.9012em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.10764em">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.7651em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>，而<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.03588em">η</span></span></span></span>为1的时候就是原始的近远平面，按照作者的思路应该<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.03588em">η</span></span></span></span>逐渐从0增加到1,因此看<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.03588em">η</span></span></span></span>的定义，作者还增加了超参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">N_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2806em"><span style="top:-2.55em;margin-left:-.109em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>：到第几轮的时候应该停止退火,和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">p_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>：初始的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.03588em">η</span></span></span></span>，可以看出max和min肯定是写反了，从代码来看也能知道。</p><figure><img src="/ayene-no-blog/assets/image-20240113164015919.BApG5pTT.png" alt="image-20240113164015919" loading="lazy" decoding="async"></figure><h2 id="freenerf" tabindex="-1">FreeNeRF <a class="header-anchor" href="#freenerf" aria-label="Permalink to &quot;FreeNeRF&quot;">​</a></h2><p><strong>FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization</strong><em>Jiawei Yang, Marco Pavone, Yue Wang</em> CVPR 2023, 13 Mar 2023 [<a href="https://arxiv.org/abs/2303.07418" target="_blank" rel="noreferrer">arXiv</a>] [<a href="https://jiawei-yang.github.io/FreeNeRF/" target="_blank" rel="noreferrer">Project</a>] [<a href="https://github.com/Jiawei-Yang/FreeNeRF" target="_blank" rel="noreferrer">Github</a>] [<a href="https://github.com/yangjiheng/nerf_and_beyond_docs/blob/main/paper_discussions/FreeNeRF.md" target="_blank" rel="noreferrer">Notes</a>]</p><h3 id="motivation-4" tabindex="-1">motivation <a class="header-anchor" href="#motivation-4" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>现存方法的不足：</p><p>需要大规模预训练：pixelNeRF，MVSNeRF</p><p>引入了深度监督，导致了复杂的管线：Depth-supervised NeRF</p><p>patch级别的正则化，导致了较高的计算量：DietNeRF，RegNeRF</p><p>总的来说，作者引入了两个几乎不增加计算量的正则化方法，避免了以上策略的缺点，即无依赖（不需要预训练和深度来引入额外信息）和无开销（不增加计算量），使得稀疏视角下的重建质量得到了很大的提升。</p><h3 id="contribution-3" tabindex="-1">contribution <a class="header-anchor" href="#contribution-3" aria-label="Permalink to &quot;contribution&quot;">​</a></h3><ul><li>揭示了稀疏视角重建的失败与位置编码频率之间的关系</li><li>提出两种正则化</li></ul><h3 id="method-5" tabindex="-1">method <a class="header-anchor" href="#method-5" aria-label="Permalink to &quot;method&quot;">​</a></h3><h4 id="frequency-regularization-频率正则化" tabindex="-1">Frequency Regularization（频率正则化） <a class="header-anchor" href="#frequency-regularization-频率正则化" aria-label="Permalink to &quot;Frequency Regularization（频率正则化）&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240113202748545.BNGH24E1.png" alt="image-20240113202748545" loading="lazy" decoding="async"></figure><p>作者首先发现低频位置编码反而在稀疏视角重建学得不错（尽管过度平滑），10%是指解锁前10%的位置编码，以L=10为例，10*10%=1, pos_enc[int(1):] = 0,即只保留第一个位置编码。剩下是讲故事环节：</p><p>原文只说明：高频的位置编码使得高频部分更快收敛，从而阻止了对低频信息的探索，导致NeRF合成的新视角图像中出现了预期之外的高频伪影。</p><p>接下来是个人理解：</p><p>拥有高频位置编码意味着能够学习那些只移动一点位置就能变化较大的场景细节，在稀疏视角重建的时候，由于对原始图像的过拟合，把一些本该是低频信息的（比如平滑的表面）学习成了随视角变化很大的高频信息，合成新视角时，移动视角就会带来很大的变化，从而出现各种各样的高频伪影。</p><p>于是就提出一个退火策略，随着轮次i增加，逐渐开放高频位置编码。</p><figure><img src="/ayene-no-blog/assets/image-20240113211947323.cc4BBb6J.png" alt="image-20240113211947323" loading="lazy" decoding="async"></figure><h4 id="occlusion-regularization-遮挡正则化" tabindex="-1">Occlusion Regularization（遮挡正则化） <a class="header-anchor" href="#occlusion-regularization-遮挡正则化" aria-label="Permalink to &quot;Occlusion Regularization（遮挡正则化）&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240113210555465.efPKU69d.png" alt="image-20240113210555465" loading="lazy" decoding="async"></figure><p>观察伪影，还可以发现有一些伪影并不是高频的，变化剧烈的，而是像墙一样直接堵住了物体的一部分，这部分是怎么回事呢？</p><p>通过上图可以知道，其实是输入的稀疏视角中极少重合的那一部分被NeRF解释成了更靠近相机近平面上的密集体密度块（漂浮物），在渲染新视角的时候采样到这些高体密度块就会出现一些墙挡住后面的物体。热力图是深度图，实线矩形里面是输入图像，虚线矩形里面是新视角图像，虚线圈起来的部分是导致了新视角图像“Wall”的“极少重叠部分”。</p><blockquote><p>关于为什么会到相机近平面，个人认为是因为更容易收敛到这里，因为在近平面上改变一小部分的体密度就能改动很大的一块图像，近大远小嘛，这样的话这里就是梯度下降最快的方向，也是一个过拟合的问题。</p></blockquote><figure><img src="/ayene-no-blog/assets/image-20240113214518748.DEvzZH4w.png" alt="image-20240113214518748" loading="lazy" decoding="async"></figure><p>提出的正则化如上，假设采样64个点，K=1,2,3…64,（near to far），可以知道越靠近相机的点Loss越大（1/K），同时<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>σ</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\sigma_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>表示该采样点体密度，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">m_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>是一个二进制掩码，表示该点是否需要被正则化，为0的点该项不生效。总的来说，离相机越近，体密度越高，惩罚越强。</p><h2 id="fsgs-real-time-few-shot-view-synthesis-using-gaussian-splatting" tabindex="-1"><strong>FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</strong> <a class="header-anchor" href="#fsgs-real-time-few-shot-view-synthesis-using-gaussian-splatting" aria-label="Permalink to &quot;**FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting**&quot;">​</a></h2><p><em>Zehao Zhu, Zhiwen Fan, Yifan Jiang, Zhangyang Wang</em> arXiv preprint, 1 Dec 2023 [<a href="https://arxiv.org/abs/2312.00451" target="_blank" rel="noreferrer">arXiv</a>] [<a href="https://zehaozhu.github.io/FSGS/" target="_blank" rel="noreferrer">Project</a>]</p><p>论文阅读笔记：<a href="https://zhuanlan.zhihu.com/p/674709488" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/674709488</a></p><h3 id="motivation-5" tabindex="-1">motivation <a class="header-anchor" href="#motivation-5" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><ul><li>NeRF的稀疏视角重建方法不能取得质量和速度的平衡</li><li>在稀疏视角下，SFM生成的点云质量不佳，由于初始化不充分，后续原始3DGS的split和clone方法也无法弥补缺陷，导致结果过度平滑（什么叫过度平滑？之后可以做实验看看，NeRF那边是说有高频伪影反而要减少高频）</li><li>因此，提出一种新的改动高斯球的方法，称为Proximity-guided Gaussian Unpooling，同时引入深度先验来保证这种方法生成的高斯核的合理性</li></ul><h3 id="method-6" tabindex="-1">method <a class="header-anchor" href="#method-6" aria-label="Permalink to &quot;method&quot;">​</a></h3><ul><li>针对不够充分的高斯初始化，采用Proximity-guided Gaussian Unpooling策略增加高斯球，通过测量现有高斯分布之间的接近度并将新的高斯分布战略性地放置到最具代表性的位置来填补空白空间。</li><li>为了确保密集化的高斯分布几何形状合理，用了深度先验</li><li>用pseudo view generation防止过拟合于稀释视角（伪视图生成？）</li></ul><blockquote><p>文中提到Additionally, some Gaussians tend to grow towards extremely large volumes, leading to results that overfit the training views and generalize badly to novel viewpoints (See Fig. 3).</p><p>也就是说，类似FreeNeRF中提到的遮挡在近摄像头平面的黑团，3DGS在稀疏视角下也有将高斯核训练的过大来拟合单一视角的问题，或许可以通过正则化解决。</p></blockquote><h4 id="proximity-guided-gaussian-unpooling" tabindex="-1">Proximity-guided Gaussian Unpooling <a class="header-anchor" href="#proximity-guided-gaussian-unpooling" aria-label="Permalink to &quot;Proximity-guided Gaussian Unpooling&quot;">​</a></h4><p>为每个高斯分配一个属性proximity score：接近度分数，定义为其到最近的K个高斯核的平均距离，默认K=3</p><figure><img src="/ayene-no-blog/assets/image-20240223164012926.BgclBEW7.png" alt="image-20240223164012926" loading="lazy" decoding="async"></figure><p>当proximity score超过一个阈值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">t_{prox}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9012em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">ro</span><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>，就在其自身（ori）与它的K个邻居（dst）之间创建一个新的高斯核，其scale和opacity由dst决定，rotation和SH初始化为0，通过以上过程可以在representative location（代表性位置？大概就是本来应该要很密集的区域）增加高斯密度，在优化的过程中逐渐填补观测缺乏带来的空白。</p><blockquote><p>怎么没讲如何具体的进行depth guidance修正新生成的高斯，虽然图示已经比较清楚了，猜测是拿着DPT生成的深度图，获得到新生成的高斯所对应的位置的深度，修正高斯核的深度</p><p>看完代码，误会解除，原来并没有调整高斯深度的挨骂，而是靠深度正则化约束。</p></blockquote><h4 id="geometry-guidance-for-gaussian-optimization" tabindex="-1">Geometry Guidance for Gaussian Optimization <a class="header-anchor" href="#geometry-guidance-for-gaussian-optimization" aria-label="Permalink to &quot;Geometry Guidance for Gaussian Optimization&quot;">​</a></h4><p>通过上述致密化方案补充高斯核后，直接用光度损失（估计就是3DGS原始损失）进行优化，会由于稀疏视角而不能优化出连贯的几何，以及容易过拟合于稀疏视角而在新视角合成没有泛化性。作者提出用深度先验来帮助改善高斯的几何。</p><h5 id="injecting-geometry-coherence-from-monocular-depth-构建估计深度和渲染深度的几何相关性" tabindex="-1">Injecting Geometry Coherence from Monocular Depth（构建估计深度和渲染深度的几何相关性） <a class="header-anchor" href="#injecting-geometry-coherence-from-monocular-depth-构建估计深度和渲染深度的几何相关性" aria-label="Permalink to &quot;Injecting Geometry Coherence from Monocular Depth（构建估计深度和渲染深度的几何相关性）&quot;">​</a></h5><figure><img src="/ayene-no-blog/assets/image-20240223170224318.CqkXIn3Z.png" alt="image-20240223170224318" loading="lazy" decoding="async"></figure><p>就是一个深度损失，估计深度图来自预训练模型DPT，渲染深度图来自3DGS</p><h5 id="differentiable-depth-rasterization-可微深度光栅化" tabindex="-1">Differentiable Depth Rasterization（可微深度光栅化） <a class="header-anchor" href="#differentiable-depth-rasterization-可微深度光栅化" aria-label="Permalink to &quot;Differentiable Depth Rasterization（可微深度光栅化）&quot;">​</a></h5><p>当然，为了深度可以被反向传播优化，需要写可微深度估计，深度由下式计算</p><figure><img src="data:image/png;base64,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" alt="image-20240223170513164" loading="lazy" decoding="async"></figure><p>可以发现，和渲染颜色时差不多</p><figure><img src="data:image/png;base64,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" alt="image-20240223170531373" loading="lazy" decoding="async"></figure><p>所以cuda代码应该还是比较容易实现的</p><h4 id="synthesize-pseudo-views" tabindex="-1">Synthesize Pseudo Views <a class="header-anchor" href="#synthesize-pseudo-views" aria-label="Permalink to &quot;Synthesize Pseudo Views&quot;">​</a></h4><p>没太看懂，大概是在两个已知视角之间合成一个新视角，但是没仔细说是怎么用，应该是用来数据增强，实验中提到在2000次迭代后才开始合成视角。</p><p>看了下代码</p><div class="language-python"><button title="Copy code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> iteration </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">%</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> args.sample_pseudo_interval </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">==</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> 0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> iteration </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> args.start_sample_pseudo </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> iteration </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> args.end_sample_pseudo:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">        if</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> pseudo_stack:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">            pseudo_stack </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> scene.getPseudoCameras().copy()</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">        # 线性插值得到的伪视角</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        pseudo_cam </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> pseudo_stack.pop(randint(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">len</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(pseudo_stack) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> 1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">))</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">		# 用3DGS渲染伪视角，得到pred深度</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        render_pkg_pseudo </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> render(pseudo_cam, gaussians, pipe, background)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        rendered_depth_pseudo </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> render_pkg_pseudo[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">"depth"</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">        # midas: 深度估计模型，当做gt深度，把3DGS渲染的图片传给深度估计模型得到的</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        midas_depth_pseudo </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> estimate_depth(render_pkg_pseudo[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">"render"</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">mode</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">'train'</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">		# reshape</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        rendered_depth_pseudo </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> rendered_depth_pseudo.reshape(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        midas_depth_pseudo </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> midas_depth_pseudo.reshape(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">        # 两个深度做loss</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        depth_loss_pseudo </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583"> -</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> pearson_corrcoef(rendered_depth_pseudo, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">midas_depth_pseudo)).mean()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">        if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> torch.isnan(depth_loss_pseudo).sum() </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">==</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> 0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">            loss_scale </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> min</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">((iteration </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> args.start_sample_pseudo) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">/</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> 500</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">            loss </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> loss_scale </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> args.depth_pseudo_weight </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> depth_loss_pseudo</span></span></code></pre><button class="code-block-unfold-btn"></button></div><h3 id="experiments" tabindex="-1">Experiments <a class="header-anchor" href="#experiments" aria-label="Permalink to &quot;Experiments&quot;">​</a></h3><p>迭代次数降低为10_000次</p><p>透明度重置在2000,5000,7000进行</p><p>9500次后降低深度损失的权重</p><p>不进行过大高斯核剔除（size_threshold = None）</p><p>数据集中，mip-nerf360被特别提到，因为他们第一个在无界数据集上进行稀疏视角重建。</p><figure><img src="/ayene-no-blog/assets/image-20240223171958860.B_x8mqXa.png" alt="image-20240223171958860" loading="lazy" decoding="async"></figure><p>消融实验如上</p><h2 id="splatter-image-ultra-fast-single-view-3d-reconstruction-未读完" tabindex="-1">Splatter Image: Ultra-Fast Single-View 3D Reconstruction（未读完） <a class="header-anchor" href="#splatter-image-ultra-fast-single-view-3d-reconstruction-未读完" aria-label="Permalink to &quot;Splatter Image: Ultra-Fast Single-View 3D Reconstruction（未读完）&quot;">​</a></h2><p><a href="https://szymanowiczs.github.io/" target="_blank" rel="noreferrer">Stanislaw Szymanowicz</a>, <a href="https://chrirupp.github.io/" target="_blank" rel="noreferrer">Christian Rupprecht</a>, <a href="https://www.robots.ox.ac.uk/~vedaldi/" target="_blank" rel="noreferrer">Andrea Vedaldi</a>,</p><p>Visual Geometry Group - University of Oxford</p><p>website：<a href="https://szymanowiczs.github.io/splatter-image" target="_blank" rel="noreferrer">https://szymanowiczs.github.io/splatter-image</a></p><p>大致意思是3DGS将高斯核映射成图像，本文找到了一种方法将图像通过Unet映射成高斯核</p><h2 id="gaussianshader-3d-gaussian-splatting-with-shading-functions-for-reflective-surfaces" tabindex="-1">GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces <a class="header-anchor" href="#gaussianshader-3d-gaussian-splatting-with-shading-functions-for-reflective-surfaces" aria-label="Permalink to &quot;GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces&quot;">​</a></h2><p><a href="https://github.com/Asparagus15" target="_blank" rel="noreferrer">Yingwenqi Jiang</a>1, <a href="https://github.com/donjiaking" target="_blank" rel="noreferrer">Jiadong Tu</a>1, <a href="https://liuyuan-pal.github.io/" target="_blank" rel="noreferrer">Yuan Liu</a>2, <a href="https://gaoxifeng.github.io/" target="_blank" rel="noreferrer">Xifeng Gao</a>3, <a href="https://www.xxlong.site/" target="_blank" rel="noreferrer">Xiaoxiao Long</a>2,<em>, <a href="https://www.cs.hku.hk/people/academic-staff/wenping" target="_blank" rel="noreferrer">Wenping Wang</a>4, <a href="https://yuexinma.me/aboutme.html" target="_blank" rel="noreferrer">Yuexin Ma</a>1,</em></p><p>*Corresponding author</p><p>1ShanghaiTech University, 2The University of Hong Kong, 3Tencent America, 4Texas A&amp;M University</p><p>website：<a href="https://asparagus15.github.io/GaussianShader.github.io/" target="_blank" rel="noreferrer">https://asparagus15.github.io/GaussianShader.github.io/</a></p><h3 id="motivation-6" tabindex="-1">motivation <a class="header-anchor" href="#motivation-6" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>现存的3DGS没有明确的建模外观属性，因此对镜面反射，显著视图变化的渲染质量不佳，特别是渲染那些反射为主要特征的材料的时候。</p><p>提到Ref-NeRF和ENVIDR渲染速度太慢，后者由于SDF的限制ENVIDR甚至不能建模复杂场景。</p><p>既然要提到反射，法线估计是不可避免的，如果直接通过搜索附近的高斯来估计法线，会带来很高的计算开销。这篇文章基于高斯球的最短轴方向提出了一种法线估计方法，同时还从深度图中导出法线图与估计的法线构成一个法线一致性正则项。</p><h3 id="contribution-4" tabindex="-1">contribution <a class="header-anchor" href="#contribution-4" aria-label="Permalink to &quot;contribution&quot;">​</a></h3><ul><li>提出了shading function逼近渲染方程，增强了反射和镜面的真实感</li><li>提出了一种新的法线估计方法</li><li>因为3DGS很快，实现了实时渲染</li></ul><h3 id="method-7" tabindex="-1">method <a class="header-anchor" href="#method-7" aria-label="Permalink to &quot;method&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20240222192218636.D6Bu_HtP.png" alt="image-20240222192218636" loading="lazy" decoding="async"></figure><p>大致流程如上，放弃了SH描述颜色，引入了一些Shading Attributes，一张可微的环境光照模拟间接照明</p><h4 id="描述颜色" tabindex="-1">描述颜色 <a class="header-anchor" href="#描述颜色" aria-label="Permalink to &quot;描述颜色&quot;">​</a></h4><p>作为SH的代替，该文章用如下方程描述颜色，是对渲染方程的一种近似</p><figure><img src="/ayene-no-blog/assets/image-20240222193421155.tNSsfHq7.png" alt="image-20240222193421155" loading="lazy" decoding="async"></figure><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\omega_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>：view dir</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">c_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>:漫反射颜色</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">s</span></span></span></span>:tint，材质本身的颜色</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo separator="true">,</mo><mi>ρ</mi></mrow><annotation encoding="application/x-tex">n,ρ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.1944em"></span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal">ρ</span></span></span></span>：法线、粗糙度</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">L_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>:镜面反射光</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">c_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>:<strong>残差颜色</strong>，唯一一个陌生概念，作者说这是因为一些复杂的反射，例如间接光照的散射和反射不能用上述的直接光反射来解释，所以用这项来解释这些复杂的反射，当然因为也是反射同样和view dir有关</p><p>原文表示残差颜色由SH参数化</p><p>以上参数除了直接提供的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\omega_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>都可以训练</p><h4 id="l-s的计算" tabindex="-1"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">L_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的计算 <a class="header-anchor" href="#l-s的计算" aria-label="Permalink to &quot;$L_s$的计算&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240222195442807.DjH4Yz0n.png" alt="image-20240222195442807" loading="lazy" decoding="async"></figure><p>这是计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">L_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的式子，回顾一下渲染方程</p><figure><img src="/ayene-no-blog/assets/image-20240222195659103.DZt3dXj1.png" alt="image-20240222195659103" loading="lazy" decoding="async"></figure><p>很相近，所以原理也大致相同，都是积分有哪些<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.0269em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>贡献了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">w_o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0269em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>这个方向上的L，渲染方程中根据不同材质有不同的BRDF函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">L_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>,这里也根据不同的粗糙度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding="application/x-tex">ρ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.1944em"></span><span class="mord mathnormal">ρ</span></span></span></span>和反射角<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span></span></span></span>有不同的D函数，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(w_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.0269em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>用可训练的6x64x64的cube map表示。</p><h4 id="法线估计" tabindex="-1">法线估计 <a class="header-anchor" href="#法线估计" aria-label="Permalink to &quot;法线估计&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240223124320762.Bfyh89pr.png" alt="image-20240223124320762" loading="lazy" decoding="async"></figure><p>先用椭圆最短轴当法线，但是椭圆最短轴可能朝外也可能朝内，先用上述式子选择和视线方向<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\omega_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>一致的方向，为了修正法线引入一个可训练的法线偏移<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>△</mtext><mi>n</mi></mrow><annotation encoding="application/x-tex">△n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord">△</span><span class="mord mathnormal">n</span></span></span></span>,同时为了保证不偏移太远引入正则项</p><figure><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJAAAAApCAIAAABV4TdSAAAGwElEQVR4nO2aQWwSex7Hv29jZi7MBS7MhelBOAiHlkPBxEdfbGlUjF02a+qGxReffVF5sWLclOcubZ7KdsVd06oRnnkljSXdJ7GRbiLavIKJ1KTwDuABPDAcyl6Gd6CX6WXmtAdqty2l7VhEqHxu85v/zP/358vv9//9//P/ovRbAS2ah999agdaSKMlWJPREqzJaAnWZLQEazJagjUZLcGajAOf2oHPCD7z0hdiSZoWMrE01T00dFqvkPySL1oL5zrBvXRMyb0uAwUAQsLbf/XF0bFfBo2ktNfsl5Qo8Lywk+WTulBKLaRn/xHMlK9Io/kYxJlIgpfay06CCTzP13vkH0AmYO/1Z7e3VMIvpTNcrcbGBk5a/JmqFoXxnNt93apevRREEYCMJKR2U12wUnrK+eeuryy9vZber74ZzzS2aOLyzpZNCGnf11e+dczUaGSiKAJidYtCbbEY6NUEyMfDM2AuWDokJsRqgnGRUfN3SY3L/+yny1oAyIeieamvbnC4F9OzIlB8FIxLzkt77Tpy11M4dXvMppOs11aCcZEfznj+a7950UhTCt0xew8AKOWSg7ehEdJTPtXQlSMA4r65XB175l7/azhuePz4L130hzxeIVgp5vW8wu/PWTXlaxFEu+nSPV+/enPLZoZ7Mc06Tluttn4lUHwQmK9TkHHzo8PZ42Pe420k+MSP4wnJ6XiTYEJq4kESB/st5eoTgKJr+L73bActPXgbFyE95VMNnKBBau0D9Qsybn7Uy/bdsqvA8zzHRiNv5XLJP+vGhTP/a2h2GUpbj65mXjYg3Itp1nHNRQKAosfWP/EmVHwQmD/mNVMb2vHZcPAdBwCQG4+e0Sv43EIkmi3woBi1ydytk7Ls5RP3z47MrWDuD8E126nbQ5Kd3yAYn1iIA0pru0bye5qHcng9fz+BkFr7wJGQ503cN5czn94wcEKpVucTgbvxAgxMp27qUZS22c93yrhXrj/dCPoXbj77wbxrzSjj4Pzi4N7dXy8Yn4jOAfIuXd2mKyHju/Jd8J24c0vIT/xzatggfStnM+vDq0zVICMVOnOfPXs3XsByeDLquO7UkwDQ1m7uQDL9KpK4bLbs3SNprBOMfxePAzhi1Natd1Ln+PG1o27dVYTXqhfVg+w9rGi4qV8TmQABAEm2CNRbsHVFRyEfB2AyaPdTfbGRcnFYuX2n6NmpXJQT1NY36s3/BculYiJg6jnUIJ7VnrXisJL6lot7YS0lsolwHsQfraZqMS4IIMWlZCTwNEAYzpf+4892ep8NdpRi/lCRWEnHOdX5W4NmGoCwFJn0p0CL+QwMVstBWqbS6hRbxu1SeHQkXNyNozLjOa+jYy9/plL0Keu47KqSP7YrFxuJVcGEVCxQhMG9RboAwL2+7xp5a3zoP8+Ajf66wnRoxv7uThSVKwsuz4pzwtaGU2qXZcRBM+HTTGryaw/GFi/qhbS394qPvvfELquWZdus1x727KbmAAhqT7+ikA0GlQOPq+8u7GImawTKgrGhO/8WgaSn/7AHIFRaU5/V0q7XqmiKBL8w/v2M2O+360gSMgqAsV1PM7AyualvkkRnOvwyDXBEu5piCxzEzCsRfQQA8lDXCczmlkVqm1mRpLa7WzNK0ekQISMCT1PbNOIJJVBs7CA7APAp790AceHhz51g8/H4QiLxJht9kI2WG8hl8oMmd/AnC1MxglIhlYfmmtWqBQDr8XK5J+iOEoilcjadRuSXoe06VO9CqhIhG5x4gyKC7Nxumq8GWSnmPHkjWTYlb5w8HHM/H7Vw04e/fbTaLnTpcOiI+/loPWv7A+BXYLn+3MVQANrUevNxAEKpwBaWRQCEnFEzW88/kFE0kMovQdtWNghcblmu0V984l4e8IxyeobuCY6ZP2iPs7aQWmc47pT6lKJ7fLG7wmhbXLTVxqsP4gAoWl+xEUUqGJ2C2elZUqs/htnJ8XD7LStDgYt4JgnnXzVczJv48snUlw2aU5ocSYdwBC71NgNgKZ8rHdQoSMp87naiOHLH3nsHhPzopbHrZxTgE9lsdKY3CgAypp1mut03+zT7d21XZyQJRtJ628SGhEB3Dd9/PcTzIrFWO4ggTO6Q0yQDAKxw8ad3QuzE2X31deYTUotjbusLvdLCravvjD9ffF+DUwStUMuUNeilBYDaH3MT2CdX/3aPVfVYDDQBnsuSpgvOj193CBzLQqVb99Wu0vLRXcjkodZucGGzpQZ8lHOJAs+LAEDUZ431WfFRTv7WaTH8WbJfDpJ+NrQEazJagjUZLcGajJZgTUZLsCbjf87FwJxD+gzbAAAAAElFTkSuQmCC" alt="image-20240223124606761" loading="lazy" decoding="async"></figure><p>但是以上法线定义在每个高斯核上，没有和局部的其他高斯核联系，而法线又是反应局部梯度变化的量，这导致了估计法线和几何不一致，简单的方法是直接搜索局部高斯核共同估计法线，但是计算开销太高，所以作者提出用深度图得到法线图，然后法线图和之前的法线估计之间使用法线一致性损失</p><figure><img src="data:image/png;base64,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" alt="image-20240223125538008" loading="lazy" decoding="async"></figure><p>三种法线的图示，有偏移的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">n</span></span></span></span>,最短轴<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal" style="margin-right:.03588em">v</span></span></span></span>和深度图导出的法线<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>n</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">n</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></p><figure><img src="/ayene-no-blog/assets/image-20240223125709557.DcHcEhx7.png" alt="image-20240223125709557" loading="lazy" decoding="async"></figure><h4 id="sparse-loss" tabindex="-1">Sparse loss <a class="header-anchor" href="#sparse-loss" aria-label="Permalink to &quot;Sparse loss&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240223130900633.BYSovQTF.png" alt="image-20240223130900633" loading="lazy" decoding="async"></figure><p>帮助不透明度趋于0或1，同时使得高斯球的几何形状趋近于thin plate从而提高渲染质量</p><h3 id="experiments-1" tabindex="-1">Experiments <a class="header-anchor" href="#experiments-1" aria-label="Permalink to &quot;Experiments&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20240223133226757.mLXRRPn8.png" alt="image-20240223133226757" loading="lazy" decoding="async"></figure><p>从消融实验来看，最重要的其实是可训练的环境贴图，其次就是法线一致性正则化。</p><p>从实验结果来看，各项指标相比原先的GS提升不大，甚至有的数据集中会下降，但是法线质量会大幅提高，在一些反射效果比较重要的区域会有比较好的视觉效果</p><h2 id="gaussianpro-3d-gaussian-splatting-with-progressive-propagation" tabindex="-1"><strong>GaussianPro: 3D Gaussian Splatting with Progressive Propagation</strong> <a class="header-anchor" href="#gaussianpro-3d-gaussian-splatting-with-progressive-propagation" aria-label="Permalink to &quot;**GaussianPro: 3D Gaussian Splatting with Progressive Propagation**&quot;">​</a></h2><p>单位：中科大, 港大, 南大, 阿大等</p><p>主页：<a href="https://link.zhihu.com/?target=https%3A//kcheng1021.github.io/gaussianpro.github.io/" target="_blank" rel="noreferrer">https://kcheng1021.github.io/gaussianpro.github.io/</a></p><p>代码：<a href="https://link.zhihu.com/?target=https%3A//github.com/kcheng1021/GaussianPro" target="_blank" rel="noreferrer">https://github.com/kcheng1021/GaussianPro</a></p><p>论文：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.14650" target="_blank" rel="noreferrer">https://arxiv.org/abs/2402.1465</a></p><h3 id="motivation-7" tabindex="-1">motivation <a class="header-anchor" href="#motivation-7" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>根据介绍来看，该论文主要改进方向是大场景下的新视角生成。</p><ul><li>在大场景下有很多无纹理区域，这些区域在SFM下不能生成点云，因此导致后续3GGS使用原有的致密化策略时，很难生成可靠的高斯点。</li><li>原有的高斯克隆策略忽略了已有的几何先验，要么克隆和之前完全一样的高斯，要么采用位置和方向随机的高斯初始化，这使得高斯在noisy geometries区域难以优化，以及在无纹理区域较少的高斯点。</li></ul><h3 id="method-8" tabindex="-1">method <a class="header-anchor" href="#method-8" aria-label="Permalink to &quot;method&quot;">​</a></h3><h4 id="生成2d法线图和深度图指导高斯生长" tabindex="-1">生成2D法线图和深度图指导高斯生长 <a class="header-anchor" href="#生成2d法线图和深度图指导高斯生长" aria-label="Permalink to &quot;生成2D法线图和深度图指导高斯生长&quot;">​</a></h4><p>法线估计直接用的GaussianShader里的方法用椭圆短轴，似乎没有用到偏移法线，那生成出来的法线应该会有些不准确。</p><p>深度估计就是正常方法</p><p>最后都用α合成生成2D的法线图和深度图</p><h4 id="_2d图中根据相邻像素更新每个像素的深度和法线-重投影回3d空间-生成新高斯-没看懂-用了很多上古方法" tabindex="-1">2D图中根据相邻像素更新每个像素的深度和法线，重投影回3D空间，生成新高斯（没看懂，用了很多上古方法） <a class="header-anchor" href="#_2d图中根据相邻像素更新每个像素的深度和法线-重投影回3d空间-生成新高斯-没看懂-用了很多上古方法" aria-label="Permalink to &quot;2D图中根据相邻像素更新每个像素的深度和法线，重投影回3D空间，生成新高斯（没看懂，用了很多上古方法）&quot;">​</a></h4><p>通过一种比较复杂的方法，根据相邻像素推测出当前像素的深度和法线是否准确，大致思路如此，下面有这种方法的参考文献</p><p>BARNES C, SHECHTMAN E, FINKELSTEIN A, et al. PatchMatch[J/OL]. ACM Transactions on Graphics, 2009: 1-11.</p><p>上述方法会有不可避免的估计错误，还要通过多视图几何一致性（这里又引了一个2016年提出的方法）来过滤掉不正确的深度和法线，最后才得到最终要用来修正原来的法线图和深度图的新图（propaganda map），对于绝对差大于阈值的像素，生成高斯点。</p><blockquote><p>太菜了，对于传统方法实在是提不起动力学习，除非是非常重要且通用的。</p><p>既然这种方法测出来的法线不是准确的，像GaussianShader那样用可训练法线偏移来计算无疑对我来说更简单。</p></blockquote><h4 id="平面损失" tabindex="-1">平面损失 <a class="header-anchor" href="#平面损失" aria-label="Permalink to &quot;平面损失&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240227160737927.C_u0eSjR.png" alt="image-20240227160737927" loading="lazy" decoding="async"></figure><p>原先的3DGS没有几何约束，只有光度约束，从而导致不能准确描述几何形状，所以这里加入了两个关于几何的损失。</p><figure><img src="/ayene-no-blog/assets/image-20240227161034029.BBm91C4X.png" alt="image-20240227161034029" loading="lazy" decoding="async"></figure><p>第一个是约束rendered normal map（椭圆短轴）和propaganda map的法线一致性</p><p>第二个是从NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance抄过来的损失，保证椭圆足够扁，使得椭圆中心足够接近其表面</p><blockquote><p>流体仿真+高斯那篇文章约束高斯足够圆，这里约束高斯足够扁，太神奇啦，深度学习</p></blockquote><figure><img src="data:image/png;base64,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" alt="image-20240227161138873" loading="lazy" decoding="async"></figure><h4 id="experiments-2" tabindex="-1">Experiments <a class="header-anchor" href="#experiments-2" aria-label="Permalink to &quot;Experiments&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240227163543172.D3iEe88M.png" alt="image-20240227163543172" loading="lazy" decoding="async"></figure><p>从消融实验来看还是Propagation策略起到的作用比较大</p><figure><img src="/ayene-no-blog/assets/image-20240227163620851.CDMIJqLv.png" alt="image-20240227163620851" loading="lazy" decoding="async"></figure><p>有趣的是还做了稀疏视角下的实验，30%表示只用了30%的训练图像，可以看到对于稀疏视角重建也有比较稳定的提升。</p><h2 id="spec-gaussian-anisotropic-view-dependent-appearance-for-3d-gaussian-splatting" tabindex="-1">Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting <a class="header-anchor" href="#spec-gaussian-anisotropic-view-dependent-appearance-for-3d-gaussian-splatting" aria-label="Permalink to &quot;Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting&quot;">​</a></h2><p>论文：<a href="https://arxiv.org/abs/2402.15870" target="_blank" rel="noreferrer">https://arxiv.org/abs/2402.15870</a></p><p>没代码，随便看看吧，不过用MLP代替球谐函数的方法很有趣，有其他文章也写到</p><h3 id="motivation-8" tabindex="-1">motivation <a class="header-anchor" href="#motivation-8" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>原始的3DGS中，反射和镜面反射难以建模，这是因为低阶球谐函数 (SH) 捕获这些场景中所需的高频信息的能力有限。</p><p>针对这个问题，作者提出了以下方法</p><ul><li>采用anisotropic spherical Gaussian (ASG)来模拟外观，可以有效针对高频信息进行建模，有效模拟各向异性和镜面的部分。</li><li>采用sparse anchor points控制子高斯的位置和表示的混合方法（？</li><li>coarse-to-fine的训练策略，消除floater，在初始化阶段优化低分辨率图像，可以避免在训练初期增加不必要的高斯</li></ul><h3 id="method-9" tabindex="-1">method <a class="header-anchor" href="#method-9" aria-label="Permalink to &quot;method&quot;">​</a></h3><h4 id="asg" tabindex="-1">ASG <a class="header-anchor" href="#asg" aria-label="Permalink to &quot;ASG&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240228133734703.BJVB2Fwx.png" alt="image-20240228133734703" loading="lazy" decoding="async"></figure><p>ASG函数定义如上，用24维feature f作为MLP输入训练其中三个参数，这个f是每个高斯核上携带的</p><figure><img src="data:image/png;base64,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" alt="image-20240228135630455" loading="lazy" decoding="async"></figure><p>颜色分解为漫反射场和高光场</p><figure><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIQAAAAmCAIAAACK8qQVAAAE20lEQVRoge2aYUgbZxjHn67jDkoCJcfAY8Nz4IUtCcxkw9wH1w25ycTSmoEzQ4yMmg/L2IysI6C4ruqkkY7EFhxixyBFUDqWKXOVGmGtgyV+iP2QyHY3JJHBZYwLjBPG5dM+tFmXeEnuYqK3eb+Pby73/z/3f9/3eXN6iv89DRrq4KnjNqDxBC0MFaGFoSK0MFSEFoaK0MJQEVoYKkILQ0Wc9DBEZmMxyh+3i8ec9DD22Y2ZaOa4XTzmpIehKp4+xHdFjnkYj+8J+kYbZTdiNfNUVpLdju/EBZ3N1kIdjWQpI3WovcqVISSWPW8MhbLNtLPHSTeywcshpjaGSkuyi563B0JZgr7ocbYR7NxgiK2zZAkjdau9mpUhJhbc7jvWwJKPQgHExJdXJyM7dop3HZwh3KbPNfpgX8ZNkZah+RtOYynJZND9Xtg6c89nRQHExNfDk2tpu513kQWSIpdIZnIACG6y4GgVpVVEQe3KUR4Gv+l7fy597rN56lG1KGbr7IcempZyg7/qX39wSIsA/LpvdCn95vi8NS9p6u7/QE+3FUuiBgL50e1eoOa/teCHlpUwoqR25SgNQ4zf+jyWg25Hqz4/hFMXPVRNzJSSvDMZy0J357knklanxyp1LQp8Og3tXlLqw9Rd7yd3s0WD/B7kbrriSOGozjE+5Wg6YKTOtSsNYy/6fRbgAmWuyyYgCRtdywHQVLMcSXZ7BexXSMlLmzqDoc6iMX515DzbF/KaZdy77rUrDIPfY3IApJnQV74WAICP+ccWknJ6BjT0+ke6JPYWPs1kAVpsZSRFLh7ZYgRAjQ3cfSBdprocs+TULnDRyBYHiAgGirY3yXxKeRSGgTUaEYgZkGIVbjvMNTpsBx4CZvf6TTlZt0b00tYxwmiAmO6gZDScIR1WjNv0uRbwwIzXgqbCl6czhl6yHu1CRu0iO+v+ighMOXDg1qcGZ5FvfNb8IhIFIQeIXl92USk92pL0JRNsZ7gCN5sT/h3SLD0dUb1MStq00H0E7LCFkusTc3HShAEbGh5l+j/yWlAAwAwNAO2UZMOoAZVqZ7dup0EQeEEEBLN7e03/lMSFRzs6ujoGllNlBRT/zjC6rlyjY9MTGwzPpxKbi/7r/vs6T7DPUr8mYuwJXGuPT19fZXg+lVxfvDHm38I8n3osqBi9+0W62ZF//Oz2CtjN0g2jJkbK1062Dll/nRlwdLze7QrtE/86W+sMDToASMeSZV+Dnaryv0NEQcgBVFx4tURCkl8dOT/ZEPjhQwoFADbkuBRxLYXoHJMjZJ77HzXwn2Q18DJGAEBIMfuYEdeLApdcGRuew8dXJ14r2NISwZF0/1RXaWPVvptCK+wtdUBCEtEZADAdCgAgxjfCGbAReCoyJ/89LII1WgndoY0AF776jnshLgCgetzWRhOtNmNRc+FYxkyUnSKnfR8PK7OiJtCm5wl2dol79uzud7d/w41//sIZntn9+QXnW8QZmXd47pWuF88e3gly+q/dP/QEaTjD70RuLe9e8AxaCsNg1m7Cy+++VC6NarcpNSEKAuQnqiiI6JGu12In0mcmMRkcfkgHKnTW/0MY/wVEEdCKc0T7e8bRUDkJ0MJQFVoYKkILQ0VoYagILQwVoYWhIrQwVIQWhorQwlARWhgq4m+Pg/iRrMwwBgAAAABJRU5ErkJggg==" alt="image-20240228135727176" loading="lazy" decoding="async"></figure><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">c_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>:first three order of SH</p><figure><img src="/ayene-no-blog/assets/image-20240228140649448.Cz64ANz5.png" alt="image-20240228140649448" loading="lazy" decoding="async"></figure><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">c_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>：将N个ASG的输出拼接成一个latent feature，然后联合被位置编码的view direction和&lt;n, -d&gt;进入MLP计算</p><h4 id="法线估计-1" tabindex="-1">法线估计 <a class="header-anchor" href="#法线估计-1" aria-label="Permalink to &quot;法线估计&quot;">​</a></h4><p>GaussianShader最短轴</p><figure><img src="data:image/png;base64,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" alt="image-20240228141453019" loading="lazy" decoding="async"></figure><p>反射方向计算的便捷方法</p><p>都是常用知识，不展开了</p><h4 id="anchor-based-gaussian-splatting" tabindex="-1">Anchor-Based Gaussian Splatting <a class="header-anchor" href="#anchor-based-gaussian-splatting" aria-label="Permalink to &quot;Anchor-Based Gaussian Splatting&quot;">​</a></h4><p>加速，省内存用的，略过</p><h4 id="corase-to-fine" tabindex="-1">Corase-to-fine <a class="header-anchor" href="#corase-to-fine" aria-label="Permalink to &quot;Corase-to-fine&quot;">​</a></h4><p>作者提出：3DGS中漂浮物的出现源于对特定像素及其邻居的过度关注，而不是考虑更广泛的全局信息，因此作者要从低分辨率到高分辨率逐步训练3DGS，训练过程中的分辨率如公式所述：</p><figure><img src="data:image/png;base64,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" alt="image-20240228142610226" loading="lazy" decoding="async"></figure><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>s</mi></msub><mo>:</mo><mtext>起始分辨率，</mtext><msub><mi>r</mi><mi>e</mi></msub><mo>:</mo><mtext>终止分辨率</mtext><mo separator="true">,</mo><mi>τ</mi><mo>:</mo><mtext>迭代阈值，默认</mtext><mn>20</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">r_s:起始分辨率，r_e:终止分辨率,τ:迭代阈值，默认20k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0278em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord cjk_fallback">起始分辨率，</span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0278em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord cjk_fallback">终止分辨率</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.1132em">τ</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord cjk_fallback">迭代阈值，默认</span><span class="mord">20</span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span></p><p>该方法优化质量的同时，将训练时间降低了20%</p><blockquote><p>原始的高斯也有预热策略，在开始时用1/4的resolution训练，在250次和500次迭代的时候进行上采样</p></blockquote><h4 id="保证高斯核足够小的正则化" tabindex="-1">保证高斯核足够小的正则化 <a class="header-anchor" href="#保证高斯核足够小的正则化" aria-label="Permalink to &quot;保证高斯核足够小的正则化&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240228143641415.Dut5DSaQ.png" alt="image-20240228143641415" loading="lazy" decoding="async"></figure><p>Prod表示高斯核三个轴scaling乘积</p><h3 id="experiments-3" tabindex="-1">Experiments <a class="header-anchor" href="#experiments-3" aria-label="Permalink to &quot;Experiments&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20240228143840152.C6j93SaL.png" alt="image-20240228143840152" loading="lazy" decoding="async"></figure><p>使用MLP+ASG可以有效建模高光</p><figure><img src="/ayene-no-blog/assets/image-20240228143925366.Dn0PhhvB.png" alt="image-20240228143925366" loading="lazy" decoding="async"></figure><p>Corase-to-fine能去除一些漂浮物</p><p>消融实验就给几张图是吧，代码不给数据不给有点过分了，字节好感度↓↓</p><h3 id="limitation-2" tabindex="-1">Limitation <a class="header-anchor" href="#limitation-2" aria-label="Permalink to &quot;Limitation&quot;">​</a></h3><p>该方法虽然能有效建模高光和各向异性材质，但是对于反射仍然不能取得很好的质量，这是因为3DGS缺少明确的geometry和environment</p><p>此外，实验中还观察到，如何明确的给3DGS提供几何信息，在强约束下会让结果更符合期望，但是渲染质量会有所降低。将来打算探索在3DGS中建模反射的可能解决方案。</p><h2 id="gea-reconstructing-expressive-3d-gaussian-avatar-from-monocular-video" tabindex="-1">GEA: Reconstructing Expressive 3D Gaussian Avatar from Monocular Video <a class="header-anchor" href="#gea-reconstructing-expressive-3d-gaussian-avatar-from-monocular-video" aria-label="Permalink to &quot;GEA: Reconstructing Expressive 3D Gaussian Avatar from Monocular Video&quot;">​</a></h2><p>Website:<a href="https://3d-aigc.github.io/GEA/" target="_blank" rel="noreferrer">https://3d-aigc.github.io/GEA/</a>.</p><h3 id="motivation-9" tabindex="-1">motivation <a class="header-anchor" href="#motivation-9" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><ul><li>源于现成的姿势估计方法准确性和稳定性的不足，缺少对手部、脚部区域的控制能力</li></ul><figure><img src="/ayene-no-blog/assets/image-20240301165331956.C-zcPMYA.png" alt="image-20240301165331956" loading="lazy" decoding="async"></figure><ul><li>现有的模型存在Unbalanced Aggregation和Initialization Bias<ul><li>Unbalanced Aggregation：纹理丰富区域会出现较多的高斯核，相应的缺少纹理的区域会缺少高斯核</li><li>Initialization Bias：与initial shape偏离较大的区域（披肩、发饰、配饰）接收到较少的高斯点分配</li></ul></li></ul><h3 id="contribution-5" tabindex="-1">contribution <a class="header-anchor" href="#contribution-5" aria-label="Permalink to &quot;contribution&quot;">​</a></h3><ul><li>提出一个two-satge pose refinement method，提高了身体和手部姿势估计的准确性</li><li>提出一个迭代re-initialization方案，包括meshing，resampling和re-Gaussians，保证高斯点在表面附近均匀分布</li><li>大量实验证明有效性</li></ul><h3 id="method-10" tabindex="-1">method <a class="header-anchor" href="#method-10" aria-label="Permalink to &quot;method&quot;">​</a></h3><h4 id="drivable-body-hand-avatar-representation-based-on-3d-gaussian-splatting" tabindex="-1">Drivable body-hand avatar representation based on 3D Gaussian Splatting <a class="header-anchor" href="#drivable-body-hand-avatar-representation-based-on-3d-gaussian-splatting" aria-label="Permalink to &quot;Drivable body-hand avatar representation based on 3D Gaussian Splatting&quot;">​</a></h4><p>对于每个高斯点，找到最近的四个joints，用如下公式计算它的pose transformation</p><figure><img src="/ayene-no-blog/assets/image-20240301185443435.CgzbW2c7.png" alt="image-20240301185443435" loading="lazy" decoding="async"></figure><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal" style="margin-right:.02778em">θ</span></span></span></span> : 给定pose</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Γ</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Gamma(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">Γ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02778em">θ</span><span class="mclose">)</span></span></span></span>: 每个joint计算出的pose transformation matrix</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>p</mi></msub><mo stretchy="false">(</mo><mi>μ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">W_p(\mu)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">μ</span><span class="mclose">)</span></span></span></span> : 从高斯核μ最近的vertex of SMPL-X得到的blend weights</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span></span></span></span>​ : 指最近的四个joint</p><p>最终高斯核μ的新位置</p><figure><img src="/ayene-no-blog/assets/image-20240301190225779.CITf-73m.png" alt="image-20240301190225779" loading="lazy" decoding="async"></figure><p>此外，针对非刚性形变的情况，比如服装变形，增加了可训练高斯位置上的偏移，输入参数为给定的姿势</p><figure><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAALQAAAAiCAIAAAB5kGQ7AAAJhklEQVR4nO2bT0zbWB7Hv7NaOZfkklziC+ZAcmhyIDmQjMSQEYQRkFXbrIoyW0RHI0DtBNEGdbeZbv/MakrbhS7atKiwaBhVA6IiajWhqPzREJD4I03gkHBIOMQcEi7OHuyLudinPYQ/JnFoQqDQVT6n+Pn5+dnv+37/rHzG/jeJEiXk+MNZT6DE+aUkjhI5KYmjRE5K4iiRk5I4SuSkJI4SOTkRcQhMNBJlhJMYqsQ54iTEEZ9yd9zq6J5LnMBYJfJH4PPcj8Ix9+0JiCMRW0lBbXBVlhc/Vok8EWKD7f3LnPw5nuelumEjrzyDMb7wm/zxmJM7gIkENqC90m6nih7qfCLwvLh/QKhUirx7Qnp8xOVCVr8P3Iqe6BgW7zxzkJkn+Og7r4+ud9dh9FX82mOvWQVAY73hZf/e4Wv5xWPIPXkZihYHszFNg2ptNKmKHekcs5MMDHQMrQKAa+h3j0G2ExO49+e+dQCE5fZLby1FgIDIBAc6+hZFSaNCbn126KkHncMxgDBccZlJgIlPL65xBGVvue+5ZNRI+wrRwf5J++0JY+ZAQnS8o5NunfiHg4QgrvR1j9f/dsOsAADS0eVqvzcUHvKYC5BHsW6FDa/EiNo2l64gSZ4D2NDEQjwvX6xQqUijo86SPvLPLskbaHp+dD39S2erMpIqlQIKlUbvbHYCAOxfN6QbZe9AmhucFgCgHM1ud7PbfdM3Pfnr/QtMsL/jT38NSKO5xKLPX/GdU5f1QCvezmGl53ranCiUGohzIXr/NNnkrgr0TcXzeeI9ihQHH15e1dovWTUf7nrOSIWeL9A7BV2iJggAU76AzKdKIbzwM1WdFpCOVGd3IAqfIunoumMBsN7nj+zJWAj7h+mmOmumnRZCI/1ruOpuknoaLs6w+wcKc12bMO7PIW1ZihMHv7m8XOVq/b92KQdUt7VVAUgNzYYzT/HL/rkaZ+NJh+QaKj1iLMWkG4TN+UnOWW/KNECJxZFJTttWt+80hB0WAA6FMjqzDTMzG/mr4whxCPGJJ15fhD3cmJh/Fwjvja+qaH3Z5fi4WQqz9ML7YDbDI7Dh2Yn55ClXWgh9/aUaAHg9On34rcTnBuMNTquMwSgOYSf9SGoivf2EyMokLpoyXYoQ9g/HoLaZD04k4xsASI1S2o8yVWM5Esv7NeUWR/Rtx/O5Zf8KLRmLXxr45mF/36O5Pdel0Zupj2o22IXe798uB2djjKQx8a6z8+nzh29Cx0jXCkJd1eqqALA2sipx3kI4ME61Xyok1MsPfmN6BoDa7qxK++1kfANaisx448Lm/AwH1FoPtMEmw1tABak+NCcVZdBhMUwjT3JmKwl6TQRgMlCS8enIlAjAWlFI2ipEB291jm3KJGtZqJuejT6wHBHACHRsDQAuUBJzxcTWkgB0htPXqcLoajb4n8ZS/WNLtY9sKgDg1/0z1U43CciXHQpC5HcACDzPJVdHep4GRW3Nrcce2+6D7XBboNSZ74fenBEB9Xag58n07ihceA1ApT5jnVRKdToQMeYVJOYSBxtb2gCgtZZJIhw6ugQAFnNZIZtEYXT/Z8ldwAVHQIcWAcAutax8PLQKgLBWfAz/Rta2Xx7unuSC/nW3rY4EEsF3See3WRHiMWH8A9eCAKCkLhhdj3+xVunJ/ZfNMglApcyIbZlkTARM7tu9+3WP6LhteR12S2ZZQ6MuBxL5bFMAOcUhbEfXgMNuDMx2KAWg0qY7q+SEocMcAIu1QrIW25EgADSZy3JcJYR8fxvMjCFFBimi+6Y/o9nQ8tJrOXKhFVZni3ZyIBUZ8EfrPLqI30e0ThRWXDoCqv3uqDOrtiVFo8yYHkuvAmojdXBVPLwgQn3ZWVmkYnOIg96cAYBqk8Qu8fFIBABh0Z1VnZzdXKIBVJil6oxvLgFAtTFnqUVh9bywZjbGfJ+P6/79xHEMnetr2y0DPWuc3792uWZhpuaS+8jVPGEEUQAkj5oOWit1B3OgQ4EtaLucRQdB8uLYCzhMeon26MgUADRd0AF8IsJqTOX5KTMRePIwkMqnp9L6ba87Z2K8F3BUGfUHjQwdSQHQfWHQQGBoGmVG8rQLchr7tat9a6/F4NNvlrXOl8VsUHqiN2XzfpGfugilCmC4nUPiUCgVAHTknsqF6Hogpbb/s0GfPQDLJVBAyUVWHLsBB/TaHAEHG/SNi958xVHuvP3Snp+jI1RHDLkbcFh0UnMWXT4IOKL+e9M1Y8bT38cKc10b9XooyYmmLldWGTtvnw5mejxItjjz7a7SkGpEORaQ2jtCXSE5YoMjwynLD4M2uRcpiiKgJ6VXCzwv5vqKI5fK7gYcAMPt5YZ82Nc/lAKAclIDdmNe1WwvYA0Uqjw5as/vBhxIcvtFBma6v2cZAEyUGkIsGG0u3pbmuHkqgi3mIH/WOdsa0knmwWsQxHTWH0tIzCSfrkZlKUZgo4F/dfdsG3eje1HkAIBOHpXyUIZqpBjucKFCb23UgmF5AOCXXvXFGn701skvDkNHcFEaKjCBe1995fiyfZaR6y5nOXYDDjUR6v/aE3HoEQ9xZs+z35oWvZ39obEXDwQ4vTc/dlV0N+BQE9xAZ3vSYVay4S3C2fX+10af92kwMN47z5F37srY0iIR2HBw0T82LAJ93z8R2loc9ZQKUNU0XjbpnLtZCp8Irc8E3kwCAJJD/Y8ULa32SgW96h8ZDgIAZrpdIV3Z7p5lt2mOAwDiiken4BMrwbF3vnT5wT/g1ba02qpknaPKaLJgIEzftBolrfqLj1y3RqZjpGalz0/2TrTk+poRj67D0CwNFZRqrRKATitbv/ss+09NicDNv/RtwPLDe1+dkufFw9+OBZ7H0Tv8dBBCL77sfgvi+k9LLcYsW1j4rGK+z8d1748VkJ4l/LzXMWj4OXAtq0qatVJZMIFrrlDrdG99vvs6263sVTjMZZpdf3DofoqzUAb2A46aCio9iWJnpaRMZZpjfA07Y1Q1rdcRWI9mlcCzVyqT+Opo8qqrpgCLnyUO2QrH2SNb4SgGyjl446QqVx8ThbHBTb0ZCbIf7noIfmnsDXW/uaCQLEsc9FYQANFQc660wW6FaABVNvMn5gZOAU39o7sq38C0bAyZAyH0qoe/7q0v7O1lxxx8OLAqWmutp14tKAghMb8YI6sdxk9ws58GzILHm2r/qSUrj5bv7PWJ3/3YWF7gksoEpCU+DdhkVNTmU/Fj4zT0x/nkURJHiZyU/vFWIiclcZTIyf8AT2HQAIuixv4AAAAASUVORK5CYII=" alt="image-20240301190422711" loading="lazy" decoding="async"></figure><p>对比LBS，可以发现同样是从pose<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal" style="margin-right:.02778em">θ</span></span></span></span>中得到仿射矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.13889em">T</span></span></span></span>,LBS中每个vertex对应的blend weights已知，这篇文章就找离高斯核最近的vertex来当做这个高斯核的权重，同时不累计所有的joint变化对该高斯的影响而是只取最近的4个</p><figure><img src="/ayene-no-blog/assets/image-20240301191141264.D7bbpeoY.png" alt="image-20240301191141264" loading="lazy" decoding="async"></figure><h4 id="twostage-pose-estimation-and-optimization-technique" tabindex="-1">Twostage pose estimation and optimization technique <a class="header-anchor" href="#twostage-pose-estimation-and-optimization-technique" aria-label="Permalink to &quot;Twostage pose estimation and optimization technique&quot;">​</a></h4><p>目的：为了更好的位姿估计</p><figure><img src="/ayene-no-blog/assets/image-20240301192747083.BjmPvcl9.png" alt="image-20240301192747083" loading="lazy" decoding="async"></figure><p>stage1： 使用PyMAF-X进行姿态估计，得到姿态<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal" style="margin-right:.02778em">θ</span></span></span></span>和相机参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord">Π</span></span></span></span></p><figure><img src="/ayene-no-blog/assets/image-20240301193206900.LlTCAcYW.png" alt="image-20240301193206900" loading="lazy" decoding="async"></figure><p>直接用PyMAF-X在侧面时容易导致的现象</p><p>stage2：用SAM预测轮廓，用ICON或者PiFuHD预测法线，修正stage1预测出的pose渲染的法线和轮廓</p><figure><img src="/ayene-no-blog/assets/image-20240301193449941.DflpTNHr.png" alt="image-20240301193449941" loading="lazy" decoding="async"></figure><p>最终Loss如下</p><figure><img src="/ayene-no-blog/assets/image-20240301193617959.BocW2kZv.png" alt="image-20240301193617959" loading="lazy" decoding="async"></figure><p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>r</mi><mi>e</mi><mi>g</mi><mi>u</mi><mi>l</mi><mi>a</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{regular}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9694em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight">gu</span><span class="mord mathnormal mtight" style="margin-right:.01968em">l</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>定义如下，法线和轮廓为L1损失</p><figure><img src="/ayene-no-blog/assets/image-20240301193647025.CSDw4Uew.png" alt="image-20240301193647025" loading="lazy" decoding="async"></figure><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\omega_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> ： 距离根节点的远近程度，该正则项让stage2的优化不偏离stage1太远，同时手和脚受到的约束更低，在stage中refinement的幅度更大</p><h4 id="iterative-re-initialization-mechanism" tabindex="-1">Iterative re-initialization mechanism <a class="header-anchor" href="#iterative-re-initialization-mechanism" aria-label="Permalink to &quot;Iterative re-initialization mechanism&quot;">​</a></h4><p>目的：解决Unbalanced Aggregation和Initialization Bias，因此希望高斯点均匀分布在主体的真实表面附近。</p><ul><li>Unbalanced Aggregation：纹理丰富区域会出现较多的高斯核，相应的缺少纹理的区域会缺少高斯核</li><li>Initialization Bias：与initial shape偏离较大的区域（披肩、发饰、配饰）接收到较少的高斯点分配</li></ul><p>分为三步：</p><p>mesh：使用alpha shape重建表面网格。</p><p>resample: 对网格执行拉普拉斯平滑，来注入表面平滑先验。随后，对网格执行基于曲率的均匀采样，生成新的高斯点。</p><p>Re-Gaussian ： 为重新采样的点找到它们的K个最近高斯点，并继承它们的不透明度 η 和球谐函数 f 属性。旋转 R 和缩放 s 属性被随机重新初始化。</p><p>在训练过程中，以上re-initialization被重复2~3次</p><figure><img src="/ayene-no-blog/assets/image-20240301195443325.BdVpR-mK.png" alt="image-20240301195443325" loading="lazy" decoding="async"></figure><h4 id="loss" tabindex="-1">Loss <a class="header-anchor" href="#loss" aria-label="Permalink to &quot;Loss&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240301200142838.vqANPN_-.png" alt="image-20240301200142838" loading="lazy" decoding="async"></figure><p>第一个光度损失</p><p>第二个是将渲染图像C和输入的图像I通过VGG后，最后softmax输出n维向量作差，作用是VGG对高频信息敏感，可以帮助恢复高频纹理细节。</p><blockquote><p>有点疑问，首先是这个i和M不明所以</p><p>第二是经过VGG卷积得到的Loss是如何反向传播到对应的高斯核上的，光度损失好理解因为每个像素是由哪些高斯核alpha合成的是清楚的，所以每个像素颜色的改变可以正常反向传播，但是VGG卷积后得到的是N*1维向量，这是怎么从一个像素反向传播回原图片所在的所有像素的</p><p>思考了一下，正常的VGG是锁定像素颜色，反向传播训练卷积核参数，这玩意反过来训练像素颜色，能反向传播好像也挺合理</p><p>数学不好导致的</p></blockquote><figure><img src="/ayene-no-blog/assets/image-20240301202625968.NF6OeDfI.png" alt="image-20240301202625968" loading="lazy" decoding="async"></figure><p>第三个是约束非刚性形变的残差尽量小，避免显著干扰avatar</p><figure><img src="data:image/png;base64,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" alt="image-20240301202620705" loading="lazy" decoding="async"></figure><h3 id="experiments-4" tabindex="-1">Experiments <a class="header-anchor" href="#experiments-4" aria-label="Permalink to &quot;Experiments&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20240301210735157.BwnLpN5F.png" alt="image-20240301210735157" loading="lazy" decoding="async"></figure><p>相比于用SMPL的GART，用SMPL-X的GEA有明显更好的手部表现</p><figure><img src="/ayene-no-blog/assets/image-20240301210851158.b3zO0_3Y.png" alt="image-20240301210851158" loading="lazy" decoding="async"></figure><p>出色的捕捉高频细节的能力</p><blockquote><p>明天就用3DGS做一个VGG损失，你骗我我就举报</p></blockquote><h4 id="消融实验" tabindex="-1">消融实验 <a class="header-anchor" href="#消融实验" aria-label="Permalink to &quot;消融实验&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240301211521221.Be_mUd5a.png" alt="image-20240301211521221" loading="lazy" decoding="async"></figure><p>首先Drivable body-hand avatar representation based on 3D Gaussian Splatting这个方法肯定不能少，不然都不能渲染，或者说就是原始方法了，剩下的分别是缺少Pose Refine，即two-stage refinement，缺少手部骨架，应该用来对比SMPL和SMPL-X的，以及method中的循环初始化</p><h2 id="fregs-3d-gaussian-splatting-with-progressive-frequency-regularization" tabindex="-1">FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization <a class="header-anchor" href="#fregs-3d-gaussian-splatting-with-progressive-frequency-regularization" aria-label="Permalink to &quot;FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization&quot;">​</a></h2><p>2024 CVPR 暂无主页</p><h3 id="motivation-10" tabindex="-1">motivation <a class="header-anchor" href="#motivation-10" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>3DGS在densification时往往存在过度重建，高频区域被几个大高斯核覆盖，导致模糊和伪影。</p><p>类似FreeNeRF，尝试引入频率退火正则化解决频率问题，最小化pred和gt之间的频域差异。</p><h3 id="method-11" tabindex="-1">method <a class="header-anchor" href="#method-11" aria-label="Permalink to &quot;method&quot;">​</a></h3><h4 id="frequency-regularization" tabindex="-1">Frequency Regularization <a class="header-anchor" href="#frequency-regularization" aria-label="Permalink to &quot;Frequency Regularization&quot;">​</a></h4><p>作者首先提出，原来的L1 Loss并不能很好的描述那些过度重建的区域，进而导致较大的高斯核无法分裂</p><p>我们先来复习一下高斯densification的代码</p><div class="language-python"><button title="Copy code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D"># Densification</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D"># 超过densify_until_iter不再进行densification和透明度重置</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> iteration </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> opt.densify_until_iter:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    # Keep track of max radii in image-space for pruning</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    # 会从多个视角、多次优化的过程中去看同一个高斯核，记录下看到的最大2D投影半径</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    gaussians.max_radii2D[visibility_filter] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> torch.max(gaussians.max_radii2D[visibility_filter], radii[visibility_filter])</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    # 记录下xyz累积梯度，即位移倾向较高的高斯核，他们应该被split或clone</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    # warm-up后才开始densification，每100次迭代进行一次</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> iteration </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> opt.densify_from_iter </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> iteration </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">%</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> opt.densification_interval </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">==</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> 0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">        # 3000次迭代之后才开始剔除过大的高斯核</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        size_threshold </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> 20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583"> if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> iteration </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> opt.opacity_reset_interval </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">else</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> None</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        gaussians.densify_and_prune(opt.densify_grad_threshold, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">0.005</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, scene.cameras_extent, size_threshold)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    # 每3000次迭代，重置透明度，或背景为白色且第500次迭代时</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> iteration </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">%</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> opt.opacity_reset_interval </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">==</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> 0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> (dataset.white_background </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> iteration </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> opt.densify_from_iter):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        gaussians.reset_opacity()</span></span></code></pre><button class="code-block-unfold-btn"></button></div><p>首先，要保证梯度大于densify_grad_threshold，才会根据scaling进行split或者clone，显然如果过度重建部分的梯度不够大，那么那些较大的高斯核也不会分裂。我们干观察下图的圆点，发现紫色的圆点和绿色的圆点几乎重合，也就是说well-reconstruction区域和over-reconstruction区域的梯度是相似的，进而误导了高斯的densification</p><img src="/ayene-no-blog/assets/image-20240312144349720.BWvZPGfB.png" alt="image-20240312144349720" style="zoom:67%"><p>那么什么样的loss能够更好的区别over-reconstruction区域（注意区分under-reconstruction区域，这篇文章应该不太注重那些过小的高斯核所以不关注），作者就提出引入频域。我们先看结果，上图中心为三角的两条线明显的被分离开来，从而能够区分这两种区域。</p><p>首先将图片由空间域（x，y）转换到频域（u，v）</p><figure><img src="/ayene-no-blog/assets/image-20240312144944583.Dq-WplaS.png" alt="image-20240312144944583" loading="lazy" decoding="async"></figure><p>从频域中提取振幅和相位</p><figure><img src="/ayene-no-blog/assets/image-20240312145001611.wmZ-bkbP.png" alt="image-20240312145001611" loading="lazy" decoding="async"></figure><p>Re表示实数，Im表示虚数</p><p>计算input和render的振幅、相位差</p><figure><img src="/ayene-no-blog/assets/image-20240312145627728.D3uRijIa.png" alt="image-20240312145627728" loading="lazy" decoding="async"></figure><blockquote><p>这玩意怎么反向传播啊</p></blockquote><h4 id="frequency-annealing" tabindex="-1">Frequency Annealing <a class="header-anchor" href="#frequency-annealing" aria-label="Permalink to &quot;Frequency Annealing&quot;">​</a></h4><p>按Free-NeRF的思路来，应该先进行低频滤波，尽量还原图片的低频区域，再逐渐解锁高频区域</p><p>一样的思想，创建一个低频滤波器和高频滤波器</p><figure><img src="/ayene-no-blog/assets/image-20240312150410949.DvZdwfxW.png" alt="image-20240312150410949" loading="lazy" decoding="async"></figure><p>分别得到低频滤波和高频滤波后的相位差和振幅差</p><figure><img src="/ayene-no-blog/assets/image-20240312150431990.BkbtGz_A.png" alt="image-20240312150431990" loading="lazy" decoding="async"></figure><p>退火式的定义高频滤波</p><figure><img src="/ayene-no-blog/assets/image-20240312150445645.CP97h96u.png" alt="image-20240312150445645" loading="lazy" decoding="async"></figure><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">D_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.0278em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>是低频滤波器的range max，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">D_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2806em"><span style="top:-2.55em;margin-left:-.0278em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>是高频滤波器允许通过的频带，t表示cur迭代次数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">T_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>表示开始使用高频滤波器的迭代次数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.13889em">T</span></span></span></span>表示迭代终止次数。可以发现，随着训练进行，高频滤波器允许通过的频率越来越高，也就是起到了解锁高频的效果。</p><p>最后是loss，先引入低频滤波，在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">T_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>后引入高频滤波</p><figure><img src="/ayene-no-blog/assets/image-20240312150803314.sXU1I4FZ.png" alt="image-20240312150803314" loading="lazy" decoding="async"></figure><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>l</mi></msub><mo separator="true">,</mo><msub><mi>w</mi><mi>h</mi></msub></mrow><annotation encoding="application/x-tex">w_l, w_h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.0269em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.01968em">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.0269em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>是training weights</p><h3 id="experiments-5" tabindex="-1">Experiments <a class="header-anchor" href="#experiments-5" aria-label="Permalink to &quot;Experiments&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20240312153147547.CRxjExTI.png" alt="image-20240312153147547" loading="lazy" decoding="async"></figure><p>先摆个消融实验</p><p><img src="/ayene-no-blog/assets/image-20240312153158702.Cx82cK0d.png" alt="image-20240312153158702">提升</p><h2 id="dngaussian-optimizing-sparse-view-3d-gaussian-radiance-fields-with-global-local-depth-normalization" tabindex="-1">DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization <a class="header-anchor" href="#dngaussian-optimizing-sparse-view-3d-gaussian-radiance-fields-with-global-local-depth-normalization" aria-label="Permalink to &quot;DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization&quot;">​</a></h2><p>2024 CVPR 暂未开源</p><p>大概会在3月20日公开</p><p>代码：<a href="https://github.com/Fictionarry/DNGaussian/blob/main/submodules/diff-gaussian-rasterization/cuda_rasterizer/backward.cu" target="_blank" rel="noreferrer">https://github.com/Fictionarry/DNGaussian/blob/main/submodules/diff-gaussian-rasterization/cuda_rasterizer/backward.cu</a></p><h3 id="motivation-11" tabindex="-1">motivation <a class="header-anchor" href="#motivation-11" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>现有的深度正则迫使高斯的shape适应于平滑的深度，而不是复杂的几何外观。考虑到场景几何形状的基础在于高斯图元的位置而不是它们的形状，我们<strong>冻结了形状参数</strong>，并提出了硬性和软性深度正则化，通过<strong>鼓励图元之间的移动</strong>来实现空间重塑。</p><p>现有的<strong>尺度不变深度损失</strong>往往选择将深度图对齐到固定尺度，这导致<strong>忽视了小的损失</strong>。为解决这个问题，我们将全局-局部深度归一化引入深度损失函数中，从而以尺度不变的方式鼓励学习小的局部深度变化。通过局部和全局尺度归一化，我们的方法引导损失函数重新关注小的局部误差，同时保持对绝对尺度的了解，以增强深度正则化的详细几何重塑过程。</p><blockquote><p>Darf,MonoSDF用的尺度不变损失</p><p>SparseNeRF和NeuralLift-360用的深度排序损失</p></blockquote><h3 id="contribution-6" tabindex="-1">contribution <a class="header-anchor" href="#contribution-6" aria-label="Permalink to &quot;contribution&quot;">​</a></h3><ul><li>A Hard and Soft Depth Regularization：通过鼓励高斯函数的移动来约束3D辐射场的几何形状，从而实现了粗略的深度空间重塑，同时保持细致的色彩性能</li><li>A Global-Local Depth Normalization：在局部尺度上归一化depth patch，关注小的局部深度变化，提高重建细节</li></ul><h3 id="method-12" tabindex="-1">method <a class="header-anchor" href="#method-12" aria-label="Permalink to &quot;method&quot;">​</a></h3><p>Instead, considering the instability of point clouds in sparse-view situations, we initialize our method with a random set of Gaussians</p><p><strong>考虑到稀疏点云的不确定性，采用随机初始化</strong></p><h4 id="depth-regularization-for-gaussians" tabindex="-1">Depth Regularization for Gaussians <a class="header-anchor" href="#depth-regularization-for-gaussians" aria-label="Permalink to &quot;Depth Regularization for Gaussians&quot;">​</a></h4><p>we freeze the scaling s and rotation q in the depth regularization.</p><p>在深度正则化中冻结了scaling和旋转四元数q，而保留了透明度α和位置μ，总的来说用两个约束，一个去优化位置（贴近于物体表面），另一个去优化透明度（保持丰富的色彩）</p><blockquote><p>我去，降低工作量新思路，少写两反向传播呢</p></blockquote><h5 id="hard-depth-regularization" tabindex="-1">Hard Depth Regularization <a class="header-anchor" href="#hard-depth-regularization" aria-label="Permalink to &quot;Hard Depth Regularization&quot;">​</a></h5><p>第一个高斯，往往是物体的表面 or floter和noises，</p><figure><img src="/ayene-no-blog/assets/image-20240312190116888.9MokDfio.png" alt="image-20240312190116888" loading="lazy" decoding="async"></figure><p>通过将高斯核上的α在计算深度时替换成一个较大的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Γ</mi></mrow><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord">Γ</span></span></span></span>，来近似的算出第一个高斯的深度</p><p>从而得到第一个深度相似损失</p><figure><img src="/ayene-no-blog/assets/image-20240312190222918.CpWCJQ3Z.png" alt="image-20240312190222918" loading="lazy" decoding="async"></figure><p>由于只有μ能够被优化，透明度、shape都被freezing，所以这样可以让那些物体表面的高斯贴紧物体表面，而那些floater或许会因此被优化？</p><h5 id="soft-depth-regularization" tabindex="-1">Soft Depth Regularization <a class="header-anchor" href="#soft-depth-regularization" aria-label="Permalink to &quot;Soft Depth Regularization&quot;">​</a></h5><figure><img src="/ayene-no-blog/assets/image-20240312191754804.7sn8CRSJ.png" alt="image-20240312191754804" loading="lazy" decoding="async"></figure><p>剩下的就是正常的深度渲染，这时候冻结μ</p><h5 id="global-local-depth-normalization" tabindex="-1">Global-local Depth Normalization <a class="header-anchor" href="#global-local-depth-normalization" aria-label="Permalink to &quot;Global-local Depth Normalization&quot;">​</a></h5><p>现有的Loss（即使是L1）都容易忽略局部的深度细节，对small depth errors不敏感，进而导致noisy primitives和新视角合成的失败（左图）。而作者加入的局部损失可以让模型优化局部细节（右图），重建更精确的表面</p><figure><img src="/ayene-no-blog/assets/image-20240312193006018.BMj4jKot.png" alt="image-20240312193006018" loading="lazy" decoding="async"></figure><p>为了让损失更关注局部，首先将深度图切分成局部的patch，然后在每个patch做归一化，让patch内深度变为均值为0，方差为1的数据</p><figure><img src="/ayene-no-blog/assets/image-20240312194054190.CVvu3NW5.png" alt="image-20240312194054190" loading="lazy" decoding="async"></figure><p>进一步，在关注局部的基础上保证对全局的关注，对全图做归一化，最小单位为patch</p><figure><img src="/ayene-no-blog/assets/image-20240312194139509.DoQFdD8j.png" alt="image-20240312194139509" loading="lazy" decoding="async"></figure><p>这项损失定义为</p><figure><img src="/ayene-no-blog/assets/image-20240312194510910.DmNgkfG1.png" alt="image-20240312194510910" loading="lazy" decoding="async"></figure><h4 id="颜色" tabindex="-1">颜色 <a class="header-anchor" href="#颜色" aria-label="Permalink to &quot;颜色&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240312194816264.DbRvPEYj.png" alt="image-20240312194816264" loading="lazy" decoding="async"></figure><p>在稀疏视角下SH容易过拟合，采用MLP，和grid有关，先放着</p><blockquote><p>Neural Color Renderer. 3D Gaussian Splatting represents the color via spherical harmonic, however, it is easy to overfit with only sparse input views. To relieve this problem, we take a grid encoder and an MLP as the Neural Color Renderer to predict color for each primitive (Figure 3). During inference, we store the median result and only calculate the last MLP layers to merge view direction for acceleration.</p></blockquote><h3 id="experiments-6" tabindex="-1">Experiments <a class="header-anchor" href="#experiments-6" aria-label="Permalink to &quot;Experiments&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20240312195440764.BqgeXCpg.png" alt="image-20240312195440764" loading="lazy" decoding="async"></figure><p>终于有用LLFF的，我哭死，但是指标怎么这么高</p><p>只迭代6000次，在1000次后用软深度正则化。</p><figure><img src="/ayene-no-blog/assets/image-20240312195537505.DdLVN8l6.png" alt="image-20240312195537505" loading="lazy" decoding="async"></figure><p>损失函数的超参数<img src="data:image/png;base64,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" alt="image-20240312195550574"></p><p>shape freezing的作用，消除floater</p><img src="/ayene-no-blog/assets/image-20240312200135469.ERqMu4o_.png" alt="image-20240312200135469" style="zoom:50%"><p>shape freezing和center freezing的消融</p><figure><img src="/ayene-no-blog/assets/image-20240312200231171.Cslr8Kfd.png" alt="image-20240312200231171" loading="lazy" decoding="async"></figure><p>所有创新点的消融</p><img src="/ayene-no-blog/assets/image-20240312200445631.BfZ2PIe-.png" alt="image-20240312200445631" style="zoom:80%"><p>AP指直接对深度使用L2正则化，估计深度还是要归一化的不然尺度不一致</p><p>关于使用MLP合成颜色</p><figure><img src="/ayene-no-blog/assets/image-20240312200836809.C6walBOy.png" alt="image-20240312200836809" loading="lazy" decoding="async"></figure><blockquote><p>不得不吐槽，一句话带过的创新点实际上带来了超高的质量提升，而这个创新点直接来自NGP</p></blockquote><h3 id="details" tabindex="-1">Details <a class="header-anchor" href="#details" aria-label="Permalink to &quot;Details&quot;">​</a></h3><p>patchsize</p><p>在LLFF数据集上，[5, 17]随机采样</p><p>在DTU数据集上，[17, 51]随机采样（物体占据了更大的空间）</p><p>相机位姿：合成数据集直接给定，真实数据集用colmap采集所有图像（包括训练集）来计算位姿</p><p>训练集和测试集：</p><p>LLFF，每8张作为测试集，剩余视图中均匀采样训练集</p><p>DTU：DTU 数据集 [17] 由一组固定相机捕获的 124 个以对象为中心的场景组成。我们遵循[27,42,52]直接在15个场景上评估模型，扫描id为8,21,30,31,34,38,40,41,45,55,63,82,103,110和114。在每次扫描中，在我们的3视图设置中，以下id为25、22和28的图像被用作输入视图。测试集由 ID 为 1, 2, 9, 10, 11, 12, 14, 15, 23, 24, 26, 27, 29, 30, 31 32, 33, 34, 35, 41, 42, 43, 45, 46 和 47 的图像组成进行评估。图像被下采样4×</p><p>Blender：我们遵循[16,52]中用于Blender数据集[11]的数据拆分。从训练图像中选择8个输入视图，ID为26、86、2、55、75、93、16、73和8。从测试图像中均匀采样25个测试视图进行评估。在实验过程中，所有图像都被下采样 2 倍到 400 × 400。</p><blockquote><p>The splatting technique of our Gaussian Splatting [18] backbone directly merges existing primitives to render the pixel-level color without interpolation. However, since not every pixel can be overlapped by the projected primitives, the empty space between two Gaussian primitives would cause hollows and cracks when the camera pose changes. For example, some hollows can be seen at Scan 40 in Figure 14. In this work, we try to solve this problem by paying more attention to high-frequency details and therefore encouraging the densifying of primitives to fill these empty areas. In the future, we believe this problem can be fundamentally solved by the improvement of the representation itself.</p><p>两个高斯椭球之间的空白会在相机位姿改变的时候导致空洞和裂缝，这篇文章通过更加关注高频细节来解决这个问题，鼓励椭球密集化，这是一个值得思考的思路，在我合成的深度图里明显出现了各种各样的裂缝，这是由于高斯椭球半身的数量不够导致的，怎么在稀疏视角下做更好的densification是值得思考的。</p></blockquote><h2 id="sparsegs-real-time-360°-sparse-view-synthesis-using-gaussian-splatting" tabindex="-1">SparseGS: Real-Time 360° Sparse View Synthesis using Gaussian Splatting <a class="header-anchor" href="#sparsegs-real-time-360°-sparse-view-synthesis-using-gaussian-splatting" aria-label="Permalink to &quot;SparseGS: Real-Time 360° Sparse View Synthesis using Gaussian Splatting&quot;">​</a></h2><p>website:<a href="https://formycat.github.io/SparseGS-Real-Time-360-Sparse-View-Synthesis-using-Gaussian-Splatting/" target="_blank" rel="noreferrer">https://formycat.github.io/SparseGS-Real-Time-360-Sparse-View-Synthesis-using-Gaussian-Splatting/</a></p><p>涉及了difussion，这里只读深度相关部分</p><img src="/ayene-no-blog/assets/image-20240313135612175.q76tmEBL.png" alt="image-20240313135612175" style="zoom:67%"><p>很眼熟的两种深度估计，总之是为了选择到真正的位于物体表面的深度，这里提出的方法是选择权重（是T不是阿尔法，即理论上后续的高斯会有劣势，除非透明度够大）最大的高斯</p><figure><img src="data:image/png;base64,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" alt="image-20240313135713451" loading="lazy" decoding="async"></figure><p>损失上，仍然采用Pearson损失，不过加入了patch，每个iteration随机的采样N个不重叠的patch，patch的大小是超参数</p><figure><img src="/ayene-no-blog/assets/image-20240313140521362.DT7CgR0l.png" alt="image-20240313140521362" loading="lazy" decoding="async"></figure><p>接下来，根据模式深度和alpha合成深度来选择裁剪掉一些floater，理由是观察到存在floater的深度通常是双峰的，dip_test是一个测试数据是否呈现单峰分布的统计方法。</p><blockquote><p>感觉有点难懂，直接从伪代码解释吧</p></blockquote><img src="/ayene-no-blog/assets/image-20240313142220711.C01ElTIM.png" alt="image-20240313142220711" style="zoom:67%"><p>先计算机mode深度和alpha深度，然后计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi></mrow></msup><mo>−</mo><msup><mi>d</mi><mrow><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi></mrow></msup></mrow><msup><mi>d</mi><mrow><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi></mrow></msup></mfrac></mrow><annotation encoding="application/x-tex">\frac{d^{mode}-d^{alpha}}{d^{alpha}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4003em;vertical-align:-.3574em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0429em"><span style="top:-2.6426em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.782em"><span style="top:-2.786em;margin-right:.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">lp</span><span class="mord mathnormal mtight">ha</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.927em"><span style="top:-2.931em;margin-right:.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.927em"><span style="top:-2.931em;margin-right:.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">lp</span><span class="mord mathnormal mtight">ha</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.3574em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>,执行dip_test，确定在这个相机Pose下双峰严不严重，如果严重就在之后动态的根据D提高remove的阈值，来保证更多的去处floater，如果不严重就降低，来恢复场景的细节</p><figure><img src="/ayene-no-blog/assets/image-20240314143933702.DVlZHQAH.png" alt="image-20240314143933702" loading="lazy" decoding="async"></figure><h2 id="depth-regularized-optimization-for-3d-gaussian-splatting-in-few-shot-images" tabindex="-1">Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images <a class="header-anchor" href="#depth-regularized-optimization-for-3d-gaussian-splatting-in-few-shot-images" aria-label="Permalink to &quot;Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images&quot;">​</a></h2><p>随便扫了一眼，总之就是加了个深度约束+平滑约束，没细看，直接看实验的trick</p><h3 id="experiments-7" tabindex="-1">Experiments <a class="header-anchor" href="#experiments-7" aria-label="Permalink to &quot;Experiments&quot;">​</a></h3><p>实验中设置SH degree= 1，直接去除了透明度重置</p><figure><img src="/ayene-no-blog/assets/image-20240315151318980.Dp1tOdXn.png" alt="image-20240315151318980" loading="lazy" decoding="async"></figure><p>baseline给你压完了</p><h2 id="is-vanilla-mlp-in-neural-radiance-field-enough-for-few-shot-view-synthesis" tabindex="-1">Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis? <a class="header-anchor" href="#is-vanilla-mlp-in-neural-radiance-field-enough-for-few-shot-view-synthesis" aria-label="Permalink to &quot;Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?&quot;">​</a></h2><h3 id="motivation-12" tabindex="-1">motivation <a class="header-anchor" href="#motivation-12" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>fewshot的两个问题</p><ul><li>训练数据优先，模型容易过拟合输入视图，导致几何分布在2D平面上而不是3Dvolume</li></ul><blockquote><p>3DGS自然也是如此</p></blockquote><ul><li>artifacts比如ghost和floater的存在限制了新视图的保真度和3D一致性</li></ul><p>解决以上问题主要分为两种思路</p><ul><li>基于先验：multiview stereo or image-based rendering（？）</li><li>基于正则化：频率、深度等</li></ul><p>但是都没有关注MLP</p><p>同时研究了过拟合问题</p><ul><li>FreeNeRF：原始的MLP容易快速收敛到高频细节，通过这种方式快速记录输入视图，而不是推断底层几何图形，因此直接的解决方案是减少模型参数来降低模型容量</li><li>但是如DietNeRF所示，虽然减少模型参数能解决过拟合问题，但是结果中缺少细节，这表明网络应该保留模型容量。</li></ul><p>基于以上观察提出了multi-input MLP，将原始MLP的输入（position and view direction）合并到每一层</p><figure><img src="/ayene-no-blog/assets/image-20240319130651258.DYFN2TzT.png" alt="image-20240319130651258" loading="lazy" decoding="async"></figure><blockquote><p>好简单</p></blockquote><p>基于三个key insights：</p><ul><li>将输入合并到每一层可以实现输入和输出之间的较短路径，允许以端到端的方式使用更少的参数合成；</li><li>我们保持模型容量不变，因为它有利于合成高频细节</li><li>我们保持输入和输出不变，使其成为当前基于 NeRF 的管道即插即用的解决方案。</li></ul><p>为了进一步减少伪影，受到几何通常比外观更平滑的假设的启发[23]，我们提出了一个新方法：不像 NeRF 那样使用共享模型来建模颜色和体密度，而是提议分别对它们进行建模，以启用具有不同频率的位置编码[21]。我们还提出了一种新的正则化项，以减少物体为中心的场景中的背景伪影，并提出了一种采样退火策略，以解决前向场景中的近场伪影。</p><h3 id="contribution-7" tabindex="-1">contribution <a class="header-anchor" href="#contribution-7" aria-label="Permalink to &quot;contribution&quot;">​</a></h3><ul><li>mi-mlp</li><li>两个正则化提高质量</li></ul><h4 id="related-work" tabindex="-1">Related Work <a class="header-anchor" href="#related-work" aria-label="Permalink to &quot;Related Work&quot;">​</a></h4><p>正则化方法里总结的挺好，这些文章可以回头看看</p><blockquote><p>相反，基于正则化的方法遵循类似于 vanilla NeRF [21] 的逐场景优化方式，并引入额外的正则化项或训练源以获得更好的新视图合成。具体来说，首先引入语义一致性损失[12]、深度平滑损失[23]和射线熵损失[14]来约束不可见视图，以获得更好的几何恢复。为了增加可用的训练视图数量，一些工作 [1, 5, 15, 48] 提出使用深度扭曲来生成新的视图图像作为伪标签。最近，FreeNeRF [50] 通过位置编码的新频率退火策略遵循从粗到细的方式。MixNeRF [33] 将光线建模为拉普拉斯算子的混合，然后是 FlipNeRF [32]，它使用翻转的反射射线作为额外的训练源。SimpleNeRF [35] (siggraph asia)提出使用增强模型来避免过度拟合，这在前向场景中表现良好。尽管取得了显着的结果，但所有这些方法都仍然使用 vanilla NeRF 提出的网络结构。相比之下，本文从设计更好的网络结构的角度实现了少镜头视图合成。</p></blockquote><h3 id="method-13" tabindex="-1">method <a class="header-anchor" href="#method-13" aria-label="Permalink to &quot;method&quot;">​</a></h3><h4 id="per-layer-inputs-incorporation" tabindex="-1">Per-layer Inputs Incorporation <a class="header-anchor" href="#per-layer-inputs-incorporation" aria-label="Permalink to &quot;Per-layer Inputs Incorporation&quot;">​</a></h4><p>总之就是位置编码输入到每一层的好处和数学证明，用更少的参数进行端到端合成，允许浅层梯度小于深层深度</p><h4 id="modeling-colors-and-volume-density-separately" tabindex="-1">Modeling Colors and Volume Density Separately <a class="header-anchor" href="#modeling-colors-and-volume-density-separately" aria-label="Permalink to &quot;Modeling Colors and Volume Density Separately&quot;">​</a></h4><p>人们普遍认为，几何不需要外观那么详细，因为几何通常是局部平滑的，因此作者提出要减少体积密度输入的嵌入维数，这也意味着要将预测几何和外观的MLP分离，用两个单独的MLP来估计</p><figure><img src="/ayene-no-blog/assets/image-20240319141539169.u4slCYtI.png" alt="image-20240319141539169" loading="lazy" decoding="async"></figure><p>关于密度分支，简单的使用每层输入位置编码的MLP</p><figure><img src="/ayene-no-blog/assets/image-20240319142218622.B6bho8LB.png" alt="image-20240319142218622" loading="lazy" decoding="async"></figure><p>而作者经验性的发现颜色合成需要密度的辅助，因此合成颜色时借用了密度的输出</p><figure><img src="/ayene-no-blog/assets/image-20240319142445601.DKPPQr8m.png" alt="image-20240319142445601" loading="lazy" decoding="async"></figure><h4 id="background-regularization" tabindex="-1">Background Regularization <a class="header-anchor" href="#background-regularization" aria-label="Permalink to &quot;Background Regularization&quot;">​</a></h4><p>以对象为中心的重建时，常常有背景伪影，即在输入视图不可见区域，产生了一些伪影</p><figure><img src="/ayene-no-blog/assets/image-20240319144149105.jmlaoJCn.png" alt="image-20240319144149105" loading="lazy" decoding="async"></figure><p>作者的解决方法是向可见区域外采样，并在这些区域添加正则化</p><figure><img src="/ayene-no-blog/assets/image-20240319144213670.Cm1G_NC4.png" alt="image-20240319144213670" loading="lazy" decoding="async"></figure><p>希望这些不可见区域（训练集中的不可见，测试集中的可见）更加接近背景颜色</p><h4 id="sampling-annealing" tabindex="-1">Sampling Annealing <a class="header-anchor" href="#sampling-annealing" aria-label="Permalink to &quot;Sampling Annealing&quot;">​</a></h4><p>为了避免近场伪影，进行退火式的采样，在训练开始时采样更少的点，而随着训练进行逐渐增加采样点</p><figure><img src="/ayene-no-blog/assets/image-20240319144934596.B-HaRYel.png" alt="image-20240319144934596" loading="lazy" decoding="async"></figure><h3 id="experiments-8" tabindex="-1">Experiments <a class="header-anchor" href="#experiments-8" aria-label="Permalink to &quot;Experiments&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20240319145146508.Dq7UgQBS.png" alt="image-20240319145146508" loading="lazy" decoding="async"></figure><p>分别是muti-input mlp，分离几何和外观mlp，背景正则化，退火采样</p><p>分离直接加在nerf上不是很好，但是配合其他就能取得更好的效果，有点奇怪，不过总之最重要的是muti-input mlp</p><h2 id="gs-ir-3d-gaussian-splatting-for-inverse-rendering" tabindex="-1">GS-IR: 3D Gaussian Splatting for Inverse Rendering <a class="header-anchor" href="#gs-ir-3d-gaussian-splatting-for-inverse-rendering" aria-label="Permalink to &quot;GS-IR: 3D Gaussian Splatting for Inverse Rendering&quot;">​</a></h2><p>2024 ECCV</p><p>逆渲染的文章，涉及了法线和深度估计，看两眼</p><h3 id="motivation-13" tabindex="-1">motivation <a class="header-anchor" href="#motivation-13" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>During the GS optimization, the adaptive control of the Gaussian density may lead to loose geometry, making it difficult to estimate accurate scene’s normal. Consequently, it is necessary to introduce a well-designed strategy to regularize GS’s normal estimation.</p><p>在高斯优化过程中，高斯的自适应密度控制导致了松散的几何，导致难以估计场景法线。因此，需要一个良好设计的策略来正则高斯的法线估计。</p><p>还有一个是关于间接照明的</p><h3 id="contribution-8" tabindex="-1">contribution <a class="header-anchor" href="#contribution-8" aria-label="Permalink to &quot;contribution&quot;">​</a></h3><ul><li>一种具有正则化的高效优化方案，将深度梯度集中中GS周围，为GS-IR生成可靠法线</li><li>开发了一种嵌入在GS-IR中的bake方法来处理建模间接照明中的遮挡</li></ul><h3 id="method-14" tabindex="-1">method <a class="header-anchor" href="#method-14" aria-label="Permalink to &quot;method&quot;">​</a></h3><p>为了避免权重和不为1的时候，深度的值出现在一条光线的第一个高斯之前或最后一个高斯之后，修改深度公式，在最后除以总权重，也就是在一条光线的高斯核上线性插值的深度</p><figure><img src="/ayene-no-blog/assets/image-20240325135553760.OzxqI9Cp.png" alt="image-20240325135553760" loading="lazy" decoding="async"></figure><p>深度梯度法线和预测法线的一致性正则化（有点眼熟</p><figure><img src="data:image/png;base64,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" alt="image-20240325140203937" loading="lazy" decoding="async"></figure><p>当然无约束的训练法线是非常粗糙的，加上pixel-level的平滑度正则化</p><figure><img src="/ayene-no-blog/assets/image-20240325140307004.Cn9wtM8j.png" alt="image-20240325140307004" loading="lazy" decoding="async"></figure><p>最后的loss</p><figure><img src="data:image/png;base64,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" alt="image-20240325140627097" loading="lazy" decoding="async"></figure><h3 id="experiments-9" tabindex="-1">experiments <a class="header-anchor" href="#experiments-9" aria-label="Permalink to &quot;experiments&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20240325141030411.DBUPBpk7.png" alt="image-20240325141030411" loading="lazy" decoding="async"></figure><p>从左到右，普通的体渲染累积深度，选择峰值深度（不透明度最高的高斯所在位置），线性插值深度</p><figure><img src="/ayene-no-blog/assets/image-20240325141300803.DGGp7va0.png" alt="image-20240325141300803" loading="lazy" decoding="async"></figure><p>标有超天酱的表示用了法线一致性正则化</p><h2 id="coherentgs" tabindex="-1">CoherentGS <a class="header-anchor" href="#coherentgs" aria-label="Permalink to &quot;CoherentGS&quot;">​</a></h2><p><a href="https://people.engr.tamu.edu/nimak/Papers/CoherentGS" target="_blank" rel="noreferrer">https://people.engr.tamu.edu/nimak/Papers/CoherentGS</a></p><p>few-shot的文章</p><p>coherency(相干性、一致性、连贯性)</p><h3 id="motivation-14" tabindex="-1">motivation <a class="header-anchor" href="#motivation-14" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>However, it struggles to generate a good representation given a sparse set of training images. In such cases, the representation severely overfits to the training views and appears as a collection of semi-random anisotropic blobs from novel views, as shown in Fig. 1.</p><p>然而，它在给定稀疏的训练图像集时很难生成良好的表示。在这种情况下，该表示严重过度拟合于训练视图，并在新视角中呈现为半随机各向异性斑点的集合，如图1所示。</p><p>最先进的基于 NeRF 的方法会产生次优结果，因为它们的正则化不能为合理的 3D 重建提供足够的约束。此外，这些方法中的大多数依赖于神经网络学习的隐式表示的一致性，并不直接适用于具有显式非结构化表示的 3DGS。</p><p>关键思想：通过约束高斯在优化过程中的移动，增强明确的非结构化表示的连贯性。</p><p>由于高斯的非结构化特性，难以在3D空间中做出限制，因此提出为每个输入图像的每个像素分配一个单独的空间，在2D空间中强制执行单视图和多视图约束。</p><p>具体而言：</p><ul><li>使用implicit decoder强制使每个图像的高斯具有相似的深度移动以保持一致性</li><li>对于不同视角，通过总变化损失确保使用所有高斯渲染的深度平滑</li><li>提出了基于流的损失，以确保两个图像中相应像素的高斯位置相似。</li><li>为了帮助优化，我们提出使用现有的单目深度预测模型初始化高斯的位置</li><li>我们基于深度的初始化将高斯适当地定位在世界空间中，而我们的正则化优化鼓励更新，特别是位置，保持连贯和平滑。</li></ul><h3 id="contribution-9" tabindex="-1">contribution <a class="header-anchor" href="#contribution-9" aria-label="Permalink to &quot;contribution&quot;">​</a></h3><ul><li><p>我们提出了一种使用极其稀疏的输入集合进行3D重建的方法，利用3DGS技术。</p></li><li><p>我们提出了一种结构化高斯表示方法，并通过各种正则化手段引入了连贯性。</p></li><li><p>我们引入了基于深度的初始化方法来初始化3D高斯，这与正则化优化相辅相成。</p></li></ul><h3 id="related-work-1" tabindex="-1">Related work <a class="header-anchor" href="#related-work-1" aria-label="Permalink to &quot;Related work&quot;">​</a></h3><p>与我们的技术并行的是，Zhu等人（FSGS）[62]和Xiong等人（SparseGS）[54]提出利用3DGS进行稀疏视图合成。然而，与我们的方法不同，它们没有提出一种方法来强制在相邻高斯之间保持连贯性，从而产生偶发的floater。此外，这些方法会用延长的高斯填补所有输入图像中被遮挡的区域，导致模糊的结果。相反，我们的方法允许我们识别和修补被遮挡的区域，并产生高质量的虚构细节（见图1 - 右侧）。</p><figure><img src="/ayene-no-blog/assets/image-20240329142410289.Z3pAPeQf.png" alt="image-20240329142410289" loading="lazy" decoding="async"></figure><blockquote><p>被遮挡，也就是所有输入图像都没有的区域，这篇文章可以不进行合成，在后续进行inpainting</p></blockquote><h3 id="method-15" tabindex="-1">method <a class="header-anchor" href="#method-15" aria-label="Permalink to &quot;method&quot;">​</a></h3><p>我们的目标是重建静态场景的 3D 高斯表示。我们的关键思想是在优化过程中向 3D 高斯引入相干性。换句话说，当更新高斯的位置时，在优化过程中也应该类似地影响相邻高斯。通过这种一致性，我们可以使用稀疏正则化来进一步约束优化并避免对输入图像的过度拟合。</p><h4 id="coherent-3d-gaussian-optimization" tabindex="-1">Coherent 3D Gaussian Optimization <a class="header-anchor" href="#coherent-3d-gaussian-optimization" aria-label="Permalink to &quot;Coherent 3D Gaussian Optimization&quot;">​</a></h4><p>首先，为每个像素分配一个高斯，同时限制每个高斯的运动只允许在这个像素所对应的光线上。</p><figure><img src="/ayene-no-blog/assets/image-20240329144436352.Di7bvmCH.png" alt="image-20240329144436352" loading="lazy" decoding="async"></figure><p>在初始化的深度上，用残差深度控制高斯位置(x)移动</p><figure><img src="/ayene-no-blog/assets/image-20240329144457994.QAG-An1S.png" alt="image-20240329144457994" loading="lazy" decoding="async"></figure><p>g表示将像素p根据深度d投影到3d空间的位置x</p><p>假设单目深度准确，那么残差深度应该只会平滑的变化来调整不一致性，基于这一观察提出了以下的单视图和多视图约束：</p><h5 id="single-view-constraint" tabindex="-1">Single-view Constraint <a class="header-anchor" href="#single-view-constraint" aria-label="Permalink to &quot;Single-view Constraint&quot;">​</a></h5><p>残差深度并不是直接优化的，而是通过implicit decoder在单张图像上解码的</p><figure><img src="data:image/png;base64,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" alt="image-20240329145918473" loading="lazy" decoding="async"></figure><p>其中f代表decoder，n是输入图像的index，每张图像独立的被输入decoder中，通过优化decoder的参数来更新残差深度而不是直接优化像素上残差深度，这使得像素间的残差深度得以建立联系（也就是保证coherent），确保了残差深度的平滑。</p><p>然而，平滑的深度变化使得decoder难以处理图像中不同object的sharp depth discontinuities，所以引入了C-channel的二进制分割掩码（实现中取C=5），即每张输入图像根据深度的不同切分成5个区域，因此decoder也输出C-chanel的残差深度，最后的残差深度由5个通道合并（直接求和）</p><p>在透明度上也是用这个约束</p><h5 id="multiview-constraint" tabindex="-1">Multiview Constraint <a class="header-anchor" href="#multiview-constraint" aria-label="Permalink to &quot;Multiview Constraint&quot;">​</a></h5><p>单视图约束确保了每个图像高斯的平滑变形，但是没有保证来自所有高斯形成的3D表面是平滑的，于是在inverse depth上采用TV正则化</p><figure><img src="/ayene-no-blog/assets/image-20240329151600998.D7klzTGA.png" alt="image-20240329151600998" loading="lazy" decoding="async"></figure><blockquote><p>1+应该是防止除0的偏移，R表示alpha合成深度，左式全图平滑，右式是每个分割区域连通平滑</p></blockquote><p>在实验中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>T</mi><mi>V</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{TV}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span><span class="mord mathnormal mtight" style="margin-right:.22222em">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>首先较大来获得全局平滑，再逐渐增加<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>M</mi><mi>T</mi><mi>V</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{MTV}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">MT</span><span class="mord mathnormal mtight" style="margin-right:.22222em">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>来改善细节</p><figure><img src="/ayene-no-blog/assets/image-20240329151811611.BDoRM5Fr.png" alt="image-20240329151811611" loading="lazy" decoding="async"></figure><p>实验中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>​逐渐从0增加到1</p><h4 id="additional-regularization" tabindex="-1">Additional Regularization <a class="header-anchor" href="#additional-regularization" aria-label="Permalink to &quot;Additional Regularization&quot;">​</a></h4><p>受启发于一些深度估计算法，提出一种基于流的正则化，也就是约束不同图像对应同一位置的像素所对应的高斯位置一致，p、q由现有的flow method计算，M是一个前向后向一致性掩码（也是某个深度估计方法里的）</p><figure><img src="/ayene-no-blog/assets/image-20240329152637036.Ddw53_44.png" alt="image-20240329152637036" loading="lazy" decoding="async"></figure><h4 id="最终loss" tabindex="-1">最终Loss <a class="header-anchor" href="#最终loss" aria-label="Permalink to &quot;最终Loss&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20240329153919455.ROVC5pnt.png" alt="image-20240329153919455" loading="lazy" decoding="async"></figure><p>可训练参数由上述loss训练，还有透明度和位置直接由Single-view Constraint优化</p><h4 id="multisampling" tabindex="-1">multisampling <a class="header-anchor" href="#multisampling" aria-label="Permalink to &quot;multisampling&quot;">​</a></h4><p>类似MSAA的方式</p><p>虽然 3DGS 仅通过采样每个像素的中心来优化目标，但我们发现这种策略对于稀疏输入设置是有问题的。在这种情况下，高斯将被变形以匹配每个像素的中心的颜色，留下未覆盖的剩余区域。因此，从新视图来看，表面会出现半透明。为了解决这个问题，我们在每个像素内的多个样本上执行优化。多重采样确保高斯正确地覆盖每个像素，从而产生显著改善的图像，如补充视频和表3所示。</p><h3 id="_3d-gaussian-initialization" tabindex="-1">3D Gaussian Initialization <a class="header-anchor" href="#_3d-gaussian-initialization" aria-label="Permalink to &quot;3D Gaussian Initialization&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20240329155057878.Ctt-zjho.png" alt="image-20240329155057878" loading="lazy" decoding="async"></figure><p>单目深度估计模型的深度是错位的（左），也就是不具有多视角一致性，因此在初始化前先训练深度图的scaling和offset（作者提出曾经尝试过直接训练深度，但是仅在光流准确的地方生效，于是改为训练scaling和offset）</p><figure><img src="/ayene-no-blog/assets/image-20240329155317441.BgQanhu3.png" alt="image-20240329155317441" loading="lazy" decoding="async"></figure><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>D</mi><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">D^m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6644em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span></span></span></span></span></span></span>​表示单目深度估计深度，其他参数和4.2一致</p><p>scaling初始化比较特别，如图所示，其他都和原始高斯类似或是较为简单</p><figure><img src="/ayene-no-blog/assets/image-20240329170108293.ao_yCZb1.png" alt="image-20240329170108293" loading="lazy" decoding="async"></figure><h3 id="experiments-10" tabindex="-1">Experiments <a class="header-anchor" href="#experiments-10" aria-label="Permalink to &quot;Experiments&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20240329171236708.RK8UGA0f.png" alt="image-20240329171236708" loading="lazy" decoding="async"></figure><figure><img src="/ayene-no-blog/assets/image-20240329171723992.6jTvTkPc.png" alt="image-20240329171723992" loading="lazy" decoding="async"></figure><p>LLFF上的消融实验</p><h2 id="gomavatar-efficient-animatable-human-modeling-from-monocular-video-using-gaussians-on-mesh" tabindex="-1">GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh <a class="header-anchor" href="#gomavatar-efficient-animatable-human-modeling-from-monocular-video-using-gaussians-on-mesh" aria-label="Permalink to &quot;GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh&quot;">​</a></h2><p>cvpr 2024</p><p><a href="https://github.com/wenj/GoMAvatar" target="_blank" rel="noreferrer">https://github.com/wenj/GoMAvatar</a></p><h2 id="scarf-capturing-and-animation-of-body-and-clothing-from-monocular-video" tabindex="-1">SCARF : Capturing and Animation of Body and Clothing from Monocular Video <a class="header-anchor" href="#scarf-capturing-and-animation-of-body-and-clothing-from-monocular-video" aria-label="Permalink to &quot;SCARF : Capturing and Animation of Body and Clothing from Monocular Video&quot;">​</a></h2><p>2023 SIGGRAPH</p><h3 id="motivation-15" tabindex="-1">motivation <a class="header-anchor" href="#motivation-15" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>先前的工作</p><ul><li>通过估计统计3D网格模型的参数来从图像创建数字人：SCAPE（2005）、SMPL/SMPL-X(2015/2019)、Adam（2018）、GHUM（2020）、STAR（2020）</li><li>隐式表面模型：imGHUM（2021）、LEAP（2021）</li></ul><p>以上模型从穿着较少的身体扫描得到的数据训练，因此无法捕捉服装shape和appearance的变化，需要更灵活的表示</p><hr><p>能够recover clothed bodies的方法</p><ul><li>在clothed human的数据集上训练：ICON（2020）、PI-FU（2022）</li><li>从多目视频中直接训练：Netural Body等</li></ul><p>为了处理不同服装类型的复杂拓扑，这些方法用隐式表示对身体和服装一起建模。这导致了一些问题</p><ul><li>手和脸的重建很差，没有关节</li><li>身体和服装的整体模型不允许虚拟试穿，这需要身体和衣服分开表示</li><li>NeRF可以很好模拟头部，尚不清楚如何有效将这种part-based model和clothed body representation结合</li></ul><hr><p>一些方法将身体和衣服layered representation，这时候衣服在身体的外层</p><ul><li>SMPLicit、BCNet、[Xiang]等</li></ul><blockquote><p>[Xiang] : Modeling clothing as a separate layer for an animatable human avatar</p></blockquote><p>这些方法需要3D衣服扫描的巨大数据集来训练，而且仍然缺乏不同衣服种类的泛化性，且</p><ul><li>仍然只恢复了clothed body的几何，而没有appearance information（SMPLicit、BCNet）</li></ul><blockquote><p>在虚拟人的定义中，什么是几何什么是外观？</p></blockquote><ul><li>[Xiang]需要多目视频和精确的3D cloth mesh来构筑特定的avatar</li></ul><p>以上方法均不适用于loose clothing，例如短裙和长裙</p><hr><p>我们的目标：</p><ul><li>精确的人体表示，包括脸和手</li><li>当然，还有衣服，同时保证衣服能够在不同的数字人上切换</li></ul><p>基于以下观察，人体和衣服其实有不同的建模需求</p><ul><li>人体有相似的形状，可以被统计网格模型建模的很好</li><li>衣服的形状和外观往往更多变，需要灵活的表示来处理透明材料、复杂拓扑</li></ul><p>提出了<strong>SCARF</strong>，一个mesh和nerf的混合表示模型，来从单目视频中捕捉clothed human</p><p>具体来说，用SMPL-X来表示人体，并用NeRF来表示衣服的复杂拓扑，对于构建这种混合表示模型，有四个挑战：</p><ul><li>精确捕捉单目视频的人体motion，并将motion和cloth联系起来。NeRF在canonical space中建模，并用SMPL-X中的蒙皮变换将观察空间中的店变形到规范空间，这就要求对视频每一帧身体形状和姿势的精确估计。我们使用PIXIE（2021）估计身体姿势和形状参数，但这不够精确，因此我们在优化过程中refine pose和shape。</li><li>SMPL-X的蒙皮变换并不能很好的解释布料变形，特别是对于松散的衣服，因此我们学习了一个非刚性变形场来纠正身体和服装的偏差</li><li>这种混合表示模型，需要对volume rendering进行定制。要考虑服装和身体之间的遮挡关系。为了将mesh集成到体渲染中，要从相机的光学中心采样光线，直到和身体网格相交。</li><li>为了解耦身体和衣服，必须防止NeRF捕获衣服以外耳朵信息，因此需要一个分割掩码。</li></ul><p>总的来说，SCARF结合了混合表示的优势，拥有以下特点</p><ul><li>基于SMPL-X,提供了shape和pose的控制，以及更加丰富和手脸细节</li><li>NeRF带来了丰富的布料细节，同时可以让布料能在不同虚拟人之间转移，来完成虚拟试衣的功能。</li></ul><blockquote><p>Related Work暂略</p></blockquote><h3 id="method-16" tabindex="-1">method <a class="header-anchor" href="#method-16" aria-label="Permalink to &quot;method&quot;">​</a></h3><h4 id="_3-1-hybrid-representation" tabindex="-1">3.1 Hybrid Representation <a class="header-anchor" href="#_3-1-hybrid-representation" aria-label="Permalink to &quot;3.1 Hybrid Representation&quot;">​</a></h4><h5 id="body-representation" tabindex="-1">Body representation <a class="header-anchor" href="#body-representation" aria-label="Permalink to &quot;Body representation&quot;">​</a></h5><figure><img src="/ayene-no-blog/assets/image-20240429150644552.DitKCIpo.png" alt="image-20240429150644552" loading="lazy" decoding="async"></figure><p>原始的SMPL-X，增加了一个顶点偏移<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo>∈</mo><msup><mi>R</mi><mrow><msub><mi>n</mi><mi>v</mi></msub><mo>×</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">O∈R^{n_v×3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7224em;vertical-align:-.0391em"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.00773em">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1645em"><span style="top:-2.357em;margin-left:0;margin-right:.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>​来增加模型的灵活性</p><p>为了捕捉更多的几何细节，使用上采样的SMPL-X版本，拥有38,703个顶点和77336个面片。</p><p>上采样的方法：</p><ul><li>细分模型的四边形版本得到顶点和面片（四边形版本是怎么获得的？）</li><li>使用四边形版本的重心坐标对blend shape bases和skinning weights进行上采样</li></ul><p>对模型进行上采样不会改变模型表示的多样性，还需要对它进行训练，因此使用两个隐式模型</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>d</mi></msub><mo>:</mo><mi>t</mi><mo>→</mo><mi>o</mi></mrow><annotation encoding="application/x-tex">F_d:t\rightarrow o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6151em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">o</span></span></span></span>：模版顶点t的偏移量o</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>t</mi></msub><mo>:</mo><mi>t</mi><mo>→</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">F_t:t\rightarrow c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2806em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6151em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">c</span></span></span></span>：模版顶点t的颜色c</li></ul><h5 id="clothing-representation" tabindex="-1">Clothing representation <a class="header-anchor" href="#clothing-representation" aria-label="Permalink to &quot;Clothing representation&quot;">​</a></h5><p>训练一个标准空间下的NeRF：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>c</mi></msub><mo>:</mo><msup><mi>x</mi><mi>c</mi></msup><mo>→</mo><mo stretchy="false">(</mo><mi>c</mi><mo separator="true">,</mo><mi>σ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F_c:x^c\rightarrow (c,\sigma)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6644em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6644em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="mclose">)</span></span></span></span>来表示衣服</p><p>TODO ： 有点看不懂，大概是在学习一个偏移<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mi>c</mi></msup></mrow><annotation encoding="application/x-tex">d^c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6644em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span></span></span></span>，来表示一些衣服的非刚性变形，从而将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>c</mi></msup><mo>+</mo><msup><mi>d</mi><mi>c</mi></msup></mrow><annotation encoding="application/x-tex">x^c+d^c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7477em;vertical-align:-.0833em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6644em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6644em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span></span></span></span>输入NeRF</p><h4 id="_3-2canonicalization" tabindex="-1">3.2Canonicalization <a class="header-anchor" href="#_3-2canonicalization" aria-label="Permalink to &quot;3.2Canonicalization&quot;">​</a></h4><p>这节是关于如何将某个标准空间中的点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span>转换到观察空间<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>c</mi></msup></mrow><annotation encoding="application/x-tex">x^c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6644em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6644em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span></span></span></span>​</p><figure><img src="/ayene-no-blog/assets/image-20240429184443724.CdFRafAI.png" alt="image-20240429184443724" loading="lazy" decoding="async"></figure><p>在给定的body mesh M下，</p><ul><li>先将pose空间的点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span>左乘<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><msub><mi>M</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>β</mi><mo separator="true">,</mo><mi>θ</mi><mo separator="true">,</mo><mi>φ</mi><mo separator="true">,</mo><mi>O</mi><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">{M_i(\beta, \theta, φ, O)}^{-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.204em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.109em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.05278em">β</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.02778em">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal">φ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mclose">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.954em"><span style="top:-3.2029em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>进行pose空间到标准空间的逆变化，</li><li>再左乘<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><msup><mi>θ</mi><mi>c</mi></msup><mo separator="true">,</mo><mn>0</mn><mo separator="true">,</mo><mi>O</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{M_i(0, \theta^c, 0, O)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.109em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.6644em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mclose">)</span></span></span></span></span>将姿势变换到T-pose</li></ul><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">N(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>表示离点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span>最近的人体点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的集合，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi></mrow><annotation encoding="application/x-tex">\omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal" style="margin-right:.03588em">ω</span></span></span></span>表示人体点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的权重。权重由<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span>离<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的距离和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的混合权重决定。</p><h4 id="_3-4-objectives" tabindex="-1">3.4 Objectives <a class="header-anchor" href="#_3-4-objectives" aria-label="Permalink to &quot;3.4 Objectives&quot;">​</a></h4><p>Cloth segmentation loss</p><figure><img src="data:image/png;base64,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" alt="image-20240429203159051" loading="lazy" decoding="async"></figure><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">s_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>是真实的衣服mask，是一张01二值图，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">s_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>由下式定义，表示该像素代表的光线上的权重和，也可以理解为c取1的渲染图</p><figure><img src="/ayene-no-blog/assets/image-20240429203348854.BNNOYQrY.png" alt="image-20240429203348854" loading="lazy" decoding="async"></figure><p>保证<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">s_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">s_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>足够接近，可以让nerf只学习衣服存在的地方（即让衣服存在的对应像素对应光线上有足够的体密度，而其他地方的体密度较低）</p><h2 id="animatable-gaussians-learning-pose-dependent-gaussian-maps-for-high-fidelity-human-avatar-modeling" tabindex="-1">Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling <a class="header-anchor" href="#animatable-gaussians-learning-pose-dependent-gaussian-maps-for-high-fidelity-human-avatar-modeling" aria-label="Permalink to &quot;Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling&quot;">​</a></h2><h3 id="motivation-16" tabindex="-1">motivation <a class="header-anchor" href="#motivation-16" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>前人的工作：</p><hr><p>显示表达：</p><ul><li>Driving-Signal Aware Full-Body Avatars</li><li>The Power of Points for Modeling Humans in Clothing</li><li>Dressing Avatars</li></ul><p>往往需要dense reconstructed meshes来重建人类的几何，因此往往难以应用在sparse-view上</p><hr><p>隐式表达（以基于NeRF的工作为主）：</p><p>受到MLP的low-frequency spectral bias的限制</p><p>一些工作试图通过</p><ul><li><p>texture feature：Neural Actor</p></li><li><p>structured local NeRFs ：Structured Local Radiance Fields for Human Avatar Modeling</p></li></ul><p>来克服MLP的缺点，但仍然不能产生令人满意的结果</p><hr><p>显示的基于点的表达有潜力在2D图上参数化（The Power of Points for Modeling Humans in Clothing），从而能够使用2D网络来建模高保真的数字人</p><p>受到先前基于点的数字人工作的启发，首先从输入视频重建一个参数化template，并通过扩散蒙皮权重来继承SMPL参数，这个特定于角色的模版模拟了穿着衣服的基本形状，甚至包括长裙，这使得我们能够根据模版动作来animate 3D GS模型，同时避免了标准高斯模型中的密度控制，从而确保在接下来的2D参数化中保持3D GS的时间一致性结构。（慢慢看method吧，有点复杂）</p><hr><p>为了与2D网络兼容，要将3D template展开为2D map</p><p>front和back两个view几乎覆盖了整个canonical human，因此本文通过将canonical template正交投影到这两个view来实现参数化</p><p>在每个view中，将模版掩码内的每个像素定义为一个3DGS，由其位置、协方差、不透明度和颜色表示，从而得到两个GS maps。类似的，给定pose，得到两个pose map。这样的template-guided参数化通过强大的基于StyleGAN的[32-34]条件生成器StyleUNet [83]，能够根据姿势条件预测依赖姿势的高斯map。</p><p>本文的贡献：</p><ul><li>一种新的数字人表示法，它在数字人建模中引入了显示的 3DGS 技术，利用功能强大的 2D CNN 创建具有高保真姿态动态特性的逼真数字人。</li><li>模板指导参数化，可为一般服装（如连衣裙）学习特定特诊模板，并将 3D 高斯参数化为正反高斯映射，以便与二维网络兼容。</li><li>一种简单而有效的姿态投影策略，在驱动信号上使用 PCA，对新姿态具有更好的泛化能力。</li></ul><h3 id="method-17" tabindex="-1">method <a class="header-anchor" href="#method-17" aria-label="Permalink to &quot;method&quot;">​</a></h3><h4 id="流程" tabindex="-1">流程 <a class="header-anchor" href="#流程" aria-label="Permalink to &quot;流程&quot;">​</a></h4><ul><li><p>学习一个parametric template：</p><ul><li><p>从video中选择像A-pose的一帧</p></li><li><p>从该帧（pose空间下）优化一个标准SDF场和颜色场（用MLP表示）</p><p>具体来说：</p><p>对于一个pose空间下的点，通过root finding来search到其标准空间下的对应点</p><figure><img src="/ayene-no-blog/assets/image-20240505163748202.HkkL-LG7.png" alt="image-20240505163748202" loading="lazy" decoding="async"></figure><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Θ</mi></mrow><annotation encoding="application/x-tex">\Theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord">Θ</span></span></span></span> : pose条件</p><blockquote><p>LBS是从标准空间变换到pose空间的，如果是正变换直接计算就好，这里是逆变换所以需要优化算法来解方程</p></blockquote><p>通过上述方法得到标准空间下的点后，通过MLP查询SDF和颜色场，并使用SDF-based体渲染来得到图片，和gt对比来优化SDF场和颜色场</p></li><li><p>使用Marching Cubes从标准SDF场中提取mesh</p></li><li><p>将SMPL的蒙皮权重扩散到template上（沿SMPL表面法线扩散到整个3D空间，即与计算一个蒙皮权重volume <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.13889em">W</span></span></span></span>​）</p></li></ul></li><li><p>Template-guided Parameterization.</p><figure><img src="/ayene-no-blog/assets/image-20240505201330207.BqgVRpcD.png" alt="image-20240505201330207" loading="lazy" decoding="async"></figure><ul><li>给定一个训练姿势，将template通过LBS变换到姿势空间，用标准空间中的顶点和其对应的pose空间中顶点的颜色，用正交投影渲染出两个posed position maps <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>f</mi></msub><mo stretchy="false">(</mo><mi mathvariant="normal">Θ</mi><mo stretchy="false">)</mo><mtext>和</mtext><msub><mi>P</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi mathvariant="normal">Θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P_f(\Theta)和P_b(\Theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.10764em">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">Θ</span><span class="mclose">)</span><span class="mord cjk_fallback">和</span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">Θ</span><span class="mclose">)</span></span></span></span></li></ul></li><li><p>Pose-dependent Gaussian Maps</p><ul><li><p>位置图作为姿势条件，通过StyleUNet转换为front &amp; back高斯图<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>f</mi></msub><mo stretchy="false">(</mo><mi mathvariant="normal">Θ</mi><mo stretchy="false">)</mo><mtext>和</mtext><msub><mi>G</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi mathvariant="normal">Θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">G_f(\Theta)和G_b(\Theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.10764em">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">Θ</span><span class="mclose">)</span><span class="mord cjk_fallback">和</span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">Θ</span><span class="mclose">)</span></span></span></span>​</p><figure><img src="/ayene-no-blog/assets/image-20240505202049551.C2OC7kTc.png" alt="image-20240505202049551" loading="lazy" decoding="async"></figure><p>其中每个像素表示一个高斯，由位置、协方差、透明度和颜色构成。</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span>表示一个view direction map，来引入视角依赖</p></li><li><p>使用template mask，从pose-dependt Gaussian maps中提取3DGS，尽管只使用front&amp;back两个views，生成的点云也能覆盖侧面视图和手，因为这两个视图是正交的</p><figure><img src="/ayene-no-blog/assets/image-20240506143614332.BOkI7ODM.png" alt="image-20240506143614332" loading="lazy" decoding="async"></figure></li><li><p>将标准空间下的3DGS通过LBS变换到姿势空间</p><figure><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAANUAAABPCAIAAADz3xfxAAAMBUlEQVR4nO2dL3yjyhbHT9/nCa6jClaFp4orq8KqsCpcFVaVqrCquSpZldTFhavKqrKqrAqrSlVZVarCqlJVVpWrSlRwweWJ/P9Dmj8kJL3zVSkUmJP55czMmWHOQafTAQQiIf6TdAEQ/2qQ/hBJgvSHSBKkP0SSIP0hkgTpD5EkSH+IJEH6QyQJ0h8iSZD+EEmC9LcYvi4cvAJJc0JB0W0/6bLuE0h/i0GKRqfT6XRe6rnekVz9pdOn3Wq9PGkF3Pn25fTDO5KrWEGipY0BR+VK5uZ/Skh/cYDhOEnzJcOq5wCgef/3R162w6RLBaFrmM5qxQgD997bwo8I6S9OKFbsucdf51XdS7YwAI7xSTLd1S619ZgLMxukv1jBhh9/+vvbEfR0+by5lSf9dytPiSIMghAAMBzHZvy53+QoasbRrokDCyf+3Al8syKe3mzpYQn5v95w8o/Dw8PDQ0m3LUViKYoTBIGlD/84oPiCau2h+whss1dx6VpFIEdP2fLB0OKq6eolnqZoXhB45vCPA5IWZcNNfNDiqBxNvvvz718AAHBz+q4/thf0TdVGJxnardbLXfm4Vwji5Oqh1T/1cltOAwAQmVqjve5DlmGRh80e/3Y67dZz46qYIQCAOMpfPbWmrmy3Wk9X/YshXb596T+v9XR1QgAApPLXz2sZPEGjBkTtYZkr2q1Wq3Vb7ll49bTUd7MKSbW/GI6TNEMBPAJA2dAlZnCK5KtKzfxw/nh/LlRZT+ZWbJtsha8YS13BVs3Fn3Zz+u7gdPJgtnynVjlq1j0wHKdpFuAGAHJ1XeYH7hGnJUUxfpze/PP9U4F7NqVZDfd2wHAcAxwf/jX4vCk2pOtFGPiSWmPyVKNGdE/lr6c9SZJE+b9W4yLbLXIqX490Yo3arGvH7nt8sZTDmsvy/q9/WUQp4yfZ8UckFNXzFN9tVxPYpIvzOjhb0lXn8NN3+Of7qcjQdol5/aIRyL7Bj5brlZglPaCrSQVtOtwTetCEAmdOu2NKUjWJXu4hG2FH9Ydhfb/vej6w5Nx/3hFwhs/B9xsA+CWbTolZToAYdgTwGwBuPB9gSf3RomII04dt+VAC2ajMKAm26XZ1QXZUf2HYHwuyFAkQuJZpqFUFLymYUfn68x/i6KSqqwVm7rcYuLbjLx7+xyiGpdaplqGbaTqeD8xSv5ow/N39MIzbBK4uKzZOY65h+IyiKnykLGdHcHAcAGLow4W+YwcUR29AszuqP8/pxTHOGBoAMBLz9B+/m2kP1w1X9g2JPf3rfYi/GGJ0Jbu2aS8V/Hd8rCAysUTiXM8HWEZ/nmd3PxxzNAkAENoyXwgUW2ExCOjg8M8/S+yzISYyNAms6kev0qlsoB+0E/rzw3Bs6iC0dA0AANIXBR4HAAynKArgURB4CgOgRFU1f3z6rpqeGD1YpPkSzW+66KNg+EBxj17QN8lVeZnUtLFoYBBOuGXP0m4AAIgzuRsJ8PTqOVZ6ZjEAAJyXG3UOi3Z/8UJSOYAbAM/v/YhCAGK0fgJb0+yQEQrc2iXaifm3r6KkDYOvni6JX5sA6bNbPaoXjzN8DuB+i1NcYdAljD6Cs8JZb9gOutFbf+DZhkOTE67w/rNYGS4uCWxZ/PwTgMhemAqPAwD4tvETcGxQ5zgrivM7GzFCMXwaAODRdDwAAM/SvBI3HK04qvD5y5e/PkraanPLY2x8hB3NIOZQvm5cnKTTmUwmkzkigDjKnl3evcz432Gg5qWeg5WCC+uWdYqpOEw+nQIAII5OiuWz7NFoFH0Q2bi8rRezXYvTKYBUOl+7Ho1Zv9RzAJmrNaPRK8ZfumacHBEAkEqnU6n81dNo/LndqGUISKVSZ7drB8d2Qn99WbWjA+2T+nu4OAaifLepsPy6tGfPG0xF1toRBrfvigCQHRXgc/1q2emg1fXXL8a8mY9GLQb97UT72wfDX5mIH7TRjqU9ZuXCqlMjGwfDe7xSwKiVBxgr1o7h52exoDt+EPiOXqiGHLukuRgQ2Frf0LwK8VybJtfuEuzE+GNhNEnEqxLtq7JVeNATnKfaOBhbMa59qfD19P03gFTu0tAKS5vLVPyNrUH0LIvlxLVvk5D+Qt+xXc/p+TPXsayQZF4PMJU0vUQHIcYLpY0XMWYC13J8t9dhDxzbIimaZcg57okSFEuQgyDcreVZAADg6YojVqT1i3XQSWT/P98sibIzdkhUrEL0lIGvC+9Ob2qNjQShtoGjcqXxJcVMRVf4vZjZmcQ3KgZVLcQRKk1If0vj6cL/9lp/iJns1PgjCt9WVSPIZDKWpmimm/yLPYi42Bf/h3ib7IX/Q7xZkP4QSYL0h0gSpD9EkiD9IZIE6Q+RJEh/iCRB+kMkCdIfIkmQ/hBJsl/r/xDxE46+0zKbDS4AQ/r7V+PpkqSOrVENvXsPy9Dj68KYitF7MSpu0PoDxBi2fCBT23vRGPX/EEmC9IdIEqQ/RJJsR38LZG+ZQEh+9/gBva1z50GxnFTSzKiMBRuy/y0kxVn3BeLFaDdq6cEjiZPJHRq7r2s/P1xfnmVT3X+a3pMycfpvj48Vrt1qDbfeBZjcK6D/Xxu1f05SnNuLXPeGRKZ8t8jb4o0a5KI30Iyd7e1/8Fw/IQZVkI7e2rnVraptbL65JDP116fd6O9mPS2vTqezUfujNmXtPzj3+lMHvFn9veoEhjxcHMe6D21MzNVfp9MoD2ybvTHIxuyfr78RAY7v6DGTLetvm+MPjK3oAyfQ/HEqRuWoYoRSpruD2V4xCNA2XX9WRzAp+3c5Kc6Wx7+UqBkDJ/DrXJBmd7MpmgdvZh3uBQRLRbxXnrT9s5PiJMnWPO2AsWZooT7JjjC//X24mN//GxC//fPb39Z1folnPdSIbXa9k9l/bawv/lp9rcQGUs9E6q89HGamsrXbBSovZvtXT4qTOEnN/4a2zH04/9WrgfLd6mlmZjE9rf4apKTqryUksOWDD+eRZ4/yV7oiLbpJaaz2dzfHiTw9JylO8iQn/d6wbF6+lt0iwv+1n6/Pug0qkak1Fvcx8dm/XlKcRElMf61GLUNEVljr6e7hZdf6hXP6f8PO36J7gsZp/5z+30jnb/cCWp2k9j/1zAL/QSFly6ywsxqswFY+yvYejX8ZtrfTfvObtkDizu3Z392oHaCbFCeGG8bM9tefhq4mcZ894dZWoxIKhI75DRhpjb3xQt+xl0lnitPcculipu/Q/2C581N/bMX+IWslxdk8W9ZfYFUEUcMqD2Ypsqce2krlK+Tq3W8q9B3TUFXZ52XOrso/fjdT2aKiKsK8QFbgmJazlPtwgSxwsdRN0/ODETlOFGxp+wEAwLeUqu7TdGiqNllS1dW2flw6Kc422GJb/1zPp17pCbdf7mrdLnO/H9VuPdfz0EsR3G637mppADhOImo4L/436ACOpk1o3RZHe10r2d8N1vSnxB4ujgGI8sTj58X/Wrdng7ouDqYFny6z+euVo3ytxtXFxeVdHCOabfm/0FEE/svPZiZP+4aiTJ33Xct2Xff+d7N3gOplLcNwiqIBgBV4BscAuIp6ob//cq7bJXZb29/3XtEJBh41CIIggNEXcxhOSsOXXwAA97rlSRIFEDqmTrKV7i1WtT+01NIPUde67p4p6NekN0ht2C/YeFIcbLRkOCucEd++NQEAdMOWOQ7rJcWpruoKHVX4fN4EMLAna+0kmjFo+HWGo7BFGf0hN2rjBxpl2Or6mKHfm2Q8DvNUL2aPuonMs2fl4snRUT+f+Rr2N2qTzxkST1KcpXkr+WcWZUp/NQA4Wb352DARuWdWpFGD0YZzHeIt2BvMP7MYQeABZPldG8kNWDT3zGKQZGYkmxwAQGipqy0Oj7dg8eSf2SP99WogsPRvRLGSTCbS7UPxhRw0v4qCbHlBEHiWLJr03MH/dvAsi+ViyKAehyfeMI0aABCpbP7y+vaqmM0Wr3dzLmlTtB4ue4vogcjshvHP9WJxkYUWr7IP75/b8sGH81z9WefxcJN7Qew0YRDsjPEx5p/Zo/03sB359pNhl4wnBbkQ0632ov+3RxPBiOXYdf2FrqloTiaTCQxF1Xb3NVbEauxD/w/xdtl1/4d42/wfXsnNY75sOSIAAAAASUVORK5CYII=" alt="image-20240506143921696" loading="lazy" decoding="async"></figure><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mtext>&nbsp;</mtext><mi>a</mi><mi>n</mi><mi>d</mi><mtext>&nbsp;</mtext><mi>t</mi></mrow><annotation encoding="application/x-tex">R\ and\ t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal" style="margin-right:.00773em">R</span><span class="mspace">&nbsp;</span><span class="mord mathnormal">an</span><span class="mord mathnormal">d</span><span class="mspace">&nbsp;</span><span class="mord mathnormal">t</span></span></span></span>通过每个高斯上的蒙皮权重计算得到</p></li><li><p>渲染姿势空间下的3DGS</p></li></ul></li></ul><h4 id="loss-1" tabindex="-1">Loss <a class="header-anchor" href="#loss-1" aria-label="Permalink to &quot;Loss&quot;">​</a></h4><figure><img src="data:image/png;base64,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" alt="image-20240506145220712" loading="lazy" decoding="async"></figure><p>其中<img src="data:image/png;base64,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" alt="image-20240506145258916"></p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>O</mi><mo stretchy="false">(</mo><mi mathvariant="normal">Θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Delta O(\Theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord">Θ</span><span class="mclose">)</span></span></span></span>表示参数化模版上的offset map，为了保证预测出的高斯图上的位置属性近似于标准空间下的人体。</p><h4 id="pose-projection-strategy-pca-略" tabindex="-1">Pose Projection Strategy : PCA（略） <a class="header-anchor" href="#pose-projection-strategy-pca-略" aria-label="Permalink to &quot;Pose Projection Strategy : PCA（略）&quot;">​</a></h4><p>为了增强unseen视角的泛化性，对pose数据集（即T帧视频，每帧在posed position maps上的m个点组成的矩阵）用PCA进行数据增强</p><p>PCA的blog：<a href="https://zhuanlan.zhihu.com/p/77151308" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/77151308</a></p><figure><img src="/ayene-no-blog/assets/image-20240506151837089.CoP8WS8T.png" alt="image-20240506151837089" loading="lazy" decoding="async"></figure><h2 id="feature-splatting-language-driven-physics-based-scene-synthesis-and-editing" tabindex="-1">Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing <a class="header-anchor" href="#feature-splatting-language-driven-physics-based-scene-synthesis-and-editing" aria-label="Permalink to &quot;Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing&quot;">​</a></h2><p>website : <a href="https://feature-splatting.github.io" target="_blank" rel="noreferrer">https://feature-splatting.github.io</a></p><h3 id="motivation-17" tabindex="-1">motivation <a class="header-anchor" href="#motivation-17" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>这篇工作提出了Feature Splatting，半自动的将静态3D场景动态化，用自然语言来操纵外观和注册材质属性来控制动态交互。通过使用<strong>additional view-invariant features</strong>和视觉模型CLIP、DINOv2、SAM来增强3DGS实现。进一步地扩展到基于物理的动态场景，其中物体的类别和物理参数通过语言模型queries赋予。</p><p><strong>让3DGS携带特征</strong>和<strong>基于物理的动态</strong>都遇到了意想不到的技术挑战：</p><ul><li><p>feature map的提取：</p><p>高斯在geometry和radiance上有效地共享相同的interpolation kernal（？什么意思），但是从reference camera获得的2D特征图是低分辨率且有噪声的。直接采用来自Decomposing NeRF for Editing via Feature Field Distillation（NeRF的特征蒸馏）方法会导致低质量的结果，以及大量的高频噪声。我们解决这个问题通过提出一个新方法来提取feature map和一个蒸馏它们的步骤。</p></li><li><p>基于物理的动态：</p><p>为了适应volume-dependent physical effects, 提出in-fill现存的静态高斯的方法，以及在显著变形下如何将3DGS进行转换的方法，这些方法比以往表现的更好。</p></li></ul><p>这个场景建模和合成pipeline，feature splatting，包含了丰富的语义以致于可以使用自然语言来编辑，包括分解静态场景（成每个组件），并联系每个组件通过material properties和基于物理的动态。</p><p>贡献如下：</p><ul><li>提出feature splatting，增强静态场景，使其富有语义和余元驱动的物理真实movement</li><li>一种基于MPM的物理引擎，适用于基于高斯的表示，一种新颖的方法来融合来自多个基础视觉2D模型的特征以进行准确的分解</li><li>一个演示，来正面feature splatting是一个杰出的editing tool</li></ul><h3 id="related-work-2" tabindex="-1">Related work <a class="header-anchor" href="#related-work-2" aria-label="Permalink to &quot;Related work&quot;">​</a></h3><ul><li><p>Scene Editing with Distilled Feature Fields：许多工作提出NeRF的编辑方式，现存的工作都主要关注与如何操纵外观。例如</p><ul><li>Distiled Feature Fields（DFF）通过zero-shot open-text分割进行外观编辑，其中使用知识蒸馏将特征从2D基础视觉模型中嵌入。渲染过程中，DFF通过将语言查询与蒸馏特征关联起来分割受影响的体积，从而分解场景。然后，可以对分割出的对象执行外观编辑，例如颜色更改或移除。</li><li>NeRFShop:提出了一个允许用户输入以对NeRF进行几何修改的交互式流程</li><li>Instruct-NeRF2NeRF : 提出不在渲染过程中进行修改，而是使用现成的2D编辑方法来修改用于训练NeRF的图像</li><li>ClimateNeRF : 与本文关系最密切，提出在渲染过程中注入物理模拟以模拟不同的天气效果。然而，由于隐式场景表示的固有局限性，ClimateNeRF只能支持在神经渲染中修改光线行进过程，从而限制其仅能模拟天气效果中的光线反射、折射和衍射。相比之下，我们的方法使用显式表示来支持以对象为中心的物理模拟，具有更广泛的潜在应用。</li></ul></li><li><p>Concurrent Work: 列举一些3D场景理解的文章。然后提到</p><p>在物理仿真最相关的工作：PhysGaussian，表示同样使用了MPM，但不包括语义，并且PhysGaussian手动选择和分配高斯的物理属性。同时在处理高斯旋转时方式也有所不同。</p><p>在分割最相关的工作：Feature3DGS，fuse (2D reference features ) using (priors from multiple foundation models).我们将特征蒸馏视为整个pipeline中的一个组件，用系统优化的技术将训练技术提高30%</p></li></ul><h3 id="method-18" tabindex="-1">method <a class="header-anchor" href="#method-18" aria-label="Permalink to &quot;method&quot;">​</a></h3><p>三个关键组件：</p><ul><li>一个将丰富的semantic feature从vision-language models蒸馏到3DGS的方法</li><li>一个将场景通过open-text queries分解成key constituents的方法</li><li>作为物理真实动态场景合成的一部分，一种通过语言确定材料属性的方法</li></ul><h4 id="_3-1-differentiable-feature-splatting" tabindex="-1">3.1 Differentiable Feature Splatting <a class="header-anchor" href="#_3-1-differentiable-feature-splatting" aria-label="Permalink to &quot;3.1 Differentiable Feature Splatting&quot;">​</a></h4><h5 id="feature-splatting" tabindex="-1">Feature Splatting <a class="header-anchor" href="#feature-splatting" aria-label="Permalink to &quot;Feature Splatting&quot;">​</a></h5><p>对每个GS添加一个额外的vector<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">f_i∈R^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.1076em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.8491em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.00773em">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span>，与视角方向无关。</p><p>然后用volume rendering渲染特征图</p><figure><img src="/ayene-no-blog/assets/image-20240514173544908.7_KLaPEN.png" alt="image-20240514173544908" loading="lazy" decoding="async"></figure><h5 id="syatems-considerations" tabindex="-1">Syatems Considerations <a class="header-anchor" href="#syatems-considerations" aria-label="Permalink to &quot;Syatems Considerations&quot;">​</a></h5><p>直接光栅化高维度特征会导致昂贵的训练时间，深入分析后发现主要瓶颈在内存访问模式，通过设计了cuda kernal解决</p><h5 id="improving-reference-feature-quality-using-part-priors" tabindex="-1">Improving Reference Feature Quality Using Part-Priors <a class="header-anchor" href="#improving-reference-feature-quality-using-part-priors" aria-label="Permalink to &quot;Improving Reference Feature Quality Using Part-Priors&quot;">​</a></h5><p>Feature Splatting生成的特征取决于参考特征，但直接使用CLIP作为参考特征会导致质量降低，因为CLIP的特征比较粗糙，如下图</p><figure><img src="/ayene-no-blog/assets/image-20240514175837391.DGYcKTyN.png" alt="image-20240514175837391" loading="lazy" decoding="async"></figure><p>与基于NeRF的方法相比，在NeRF上这一问题并不显著，NeRF的隐式连续表示起到了正则化的效果。但3DGS没有这样的正则化，容易过拟合于粗糙reference feature map的噪声。</p><p>本文提出了一个改进GS Feature map质量的方法，通过使用DINOv2和SAM的object priors。</p><p>考虑一个输入图像，首先用SAM生成一个part-level masks集合<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">{M}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">M</span></span></span></span></span>, 对于一个给定的二值掩码M和粗糙的CLIP feature map <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">F_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>,本文使用一个Masked Average Pooling（MAP）来聚合一个single feature vector</p><figure><img src="/ayene-no-blog/assets/image-20240514191241457.Bld1AGk2.png" alt="image-20240514191241457" loading="lazy" decoding="async"></figure><p>其中i是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">F_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>​中的一个像素坐标，然后将w分配给分割部分内的所有像素，如果一个像素属于多个部分，则该像素特征通过平均所有相关部分的特征来获得，这样就得到了上图的(d) SAM-enhanced feat</p><blockquote><p>公式是将特征图<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">F_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>过一个分割掩码mask，比如说椅子就只剩下椅子那部分的feature map，然后将椅子对应像素的所有特征向量作平均池化，再重分配给椅子上对应的所有像素</p></blockquote><p>为了进一步降低过拟合的可能性，本文提出了一个shallow MLP，有两个output，并以3DGS的rendered features<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>F</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{F}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9468em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.9468em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span></span><span style="top:-3.2523em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.1667em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> 作为中间特征（不是输入么？），两个output分别为预测的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>F</mi><mo>^</mo></mover><mrow><mi>C</mi><mi>L</mi><mi>I</mi><mi>P</mi></mrow></msub><mo separator="true">,</mo><msub><mover accent="true"><mi>F</mi><mo>^</mo></mover><mrow><mi>D</mi><mi>I</mi><mi>N</mi><mi>O</mi><mi>v</mi><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\hat{F}_{CLIP}, \hat{F}_{DINOv2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1412em;vertical-align:-.1944em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.9468em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span></span><span style="top:-3.2523em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.1667em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.07153em">C</span><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight" style="margin-right:.07847em">I</span><span class="mord mathnormal mtight" style="margin-right:.13889em">P</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.9468em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span></span><span style="top:-3.2523em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.1667em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">D</span><span class="mord mathnormal mtight" style="margin-right:.07847em">I</span><span class="mord mathnormal mtight" style="margin-right:.02778em">NO</span><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>，用真实的CLIP以及DINOv2的输出做监督，pipeline如下<img src="/ayene-no-blog/assets/image-20240514200537791.CbYZ0nGg.png" alt="image-20240514200537791"></p><p>首先是Feature splatting渲染出一个feature map，每个像素上有一个d维的Latent Feature，通过一层MLP输出预测的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>F</mi><mo>^</mo></mover><mrow><mi>C</mi><mi>L</mi><mi>I</mi><mi>P</mi></mrow></msub><mo separator="true">,</mo><msub><mover accent="true"><mi>F</mi><mo>^</mo></mover><mrow><mi>D</mi><mi>I</mi><mi>N</mi><mi>O</mi><mi>v</mi><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\hat{F}_{CLIP}, \hat{F}_{DINOv2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1412em;vertical-align:-.1944em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.9468em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span></span><span style="top:-3.2523em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.1667em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.07153em">C</span><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight" style="margin-right:.07847em">I</span><span class="mord mathnormal mtight" style="margin-right:.13889em">P</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.9468em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span></span><span style="top:-3.2523em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.1667em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">D</span><span class="mord mathnormal mtight" style="margin-right:.07847em">I</span><span class="mord mathnormal mtight" style="margin-right:.02778em">NO</span><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>，用真实的CLIP以及DINOv2的输出做监督，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">F_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的监督权重更高，反向传播让3DGS能够通过render合成一个修正过的CLIP features map。</p><blockquote><p>换种方式解释，目的是为了让每个gs上有一个语义属性，因此需要render出一个feature map和gt feature map做监督，直接用CLIP生成的feature map监督效果不好，两个优化，第一个是用mask修正clip gt，第二个是用DINOv2作为正则在监督时加入。</p></blockquote><h4 id="_3-2-language-guided-scene-decomposition" tabindex="-1">3.2 Language-guided Scene Decomposition <a class="header-anchor" href="#_3-2-language-guided-scene-decomposition" aria-label="Permalink to &quot;3.2 Language-guided Scene Decomposition&quot;">​</a></h4><p>那么要怎么使用每个高斯上的feature，让它完成场景理解任务呢。</p><p>我们识别3DGS，通过查询其CLIP feature更接近正查询而非负查询（？）</p><p>具体来说，给定一个词汇作为正样本（比如bulldozer），用一个通用的词汇作为负样本（比如object，thing），使用frozen CLIP text encoder来获得以上词汇的text embeddings，遵循CLIP的标准实现，计算rasterized CLIP feature of every Gaussian（应该是每个高斯的CLIP feature，那rasterized这个前缀是不是有些误导性）和text embeddings的余弦相似度，选择那些相似度大于0.6作为前景object。在附录中包含了使用负文本查询的分割结果。</p><p>通过上述方式就选中了需要编辑或者进行仿真的高斯。</p><p>一些简单的编辑操作就容易实现了：</p><figure><img src="/ayene-no-blog/assets/image-20240515140219149.Dmc6eDVI.png" alt="image-20240515140219149" loading="lazy" decoding="async"></figure><h4 id="_3-3-language-driven-physics-synthesis" tabindex="-1">3.3 Language-Driven Physics Synthesis <a class="header-anchor" href="#_3-3-language-driven-physics-synthesis" aria-label="Permalink to &quot;3.3 Language-Driven Physics Synthesis&quot;">​</a></h4><p>feature splatting可以自动选择用于模拟的物理属性、估计碰撞表面、预测重力方向。核心方法在于使用Taichi扩展的MPM</p><h5 id="decoupling-objects-for-simulation" tabindex="-1">Decoupling Objects for Simulation <a class="header-anchor" href="#decoupling-objects-for-simulation" aria-label="Permalink to &quot;Decoupling Objects for Simulation&quot;">​</a></h5><p>对于常见的刚体创建了一组词汇（e.g. wood, ceramic, steel）,可以随用户的可选输入扩展。</p><p>给定一个selected multi-part object（比如一个插有花的花瓶），再执行一次CLIP相似度查询选择对象内与这些材料更接近的粒子。</p><p>在模拟过程中，这些被选定的粒子被认为是刚性的。</p><h5 id="language-grounded-collision-surface-estimation" tabindex="-1">Language-grounded Collision Surface Estimation <a class="header-anchor" href="#language-grounded-collision-surface-estimation" aria-label="Permalink to &quot;Language-grounded Collision Surface Estimation&quot;">​</a></h5><p>预定义一组规范查询（包括常见的平面物体，比如floor和tabletop），将这组查询输入到上述的场景分解pipeline中，获取这些物体的高斯表示，并用RANSAC来估计这些平面的几何形状，在物理模拟时作为碰撞表面。</p><p>重力方向是被估计为“floor”的平面几何的法向量。</p><h5 id="taichi-mpm-for-gaussians" tabindex="-1">Taichi MPM for gaussians <a class="header-anchor" href="#taichi-mpm-for-gaussians" aria-label="Permalink to &quot;Taichi MPM for gaussians&quot;">​</a></h5><p>可以直接将高斯中心μ视为点云使用MPM，但质量不佳，有两个问题：</p><ul><li>缺乏内部支撑，物体在与碰撞表面接触的时候回塌陷</li><li>当物体发生变形时，出现不期望的伪影</li></ul><p>本文的方法超越了简单的基于点的物理模拟，利用了特定于高斯的信息，如各向同性的不透明度和高斯协方差，在变形过程中进行体积保持和协方差修改，以解决上述两个挑战。</p><p>解决方案：</p><ul><li><p>Implicit Volume Preservation ： 在从少量真实世界图像中模拟物理现象时，体积保持是一个未解决的挑战。</p><p>例如，没有体积保持，模拟一个排球撞击地面时会在碰撞时塌陷。因此，我们提出了一种使用高斯的不透明度和协方差的隐式体积保持技术。具体来说，我们首先使用协方差和不透明度信息按照PhysGaussian的方法在表面高斯的圆盘上采样点以增加表面点的密度。通过增加表面点的密度，我们随后填充从对象中心到表面的透明支撑粒子。填充粒子的透明性旨在确保在T = 0时的渲染质量一致，从而实现从静态场景到动态场景的平滑连续过渡。</p></li><li><p>Estimating Rotation : 当物体发生变形的时候，需要校正协方差矩阵的旋转分量，否则可能会产生伪影。PhysGaussian也注意到了这个问题，尝试使用MPM的变形梯度F来更新高斯的协方差。然而变形梯度主要捕捉局部变化，当弹性物体发生大变形的时候，这种方法就会失效。因此我们提出用法线来估算弹性物体的旋转矩阵。</p><p>3DGS难以在稀疏重建时估计法线，因此本文采用了一种基于神经网络的方法。</p><p>具体来说，对于每个要模拟的高斯，我们找到其两个最近的邻居。这三个高斯的中心形成一个平面，我们使用该平面法线的旋转作为整个物体动态的代理。与从变形估算的旋转相比，我们的方法在物体发生大变形时产生的伪影更少。</p></li></ul><h4 id="experiments-11" tabindex="-1">Experiments <a class="header-anchor" href="#experiments-11" aria-label="Permalink to &quot;Experiments&quot;">​</a></h4><p>dataset : deep blending, Mip-NeRF360, custom dataset</p><h2 id="segment-any-3d-gaussians" tabindex="-1">Segment Any 3D Gaussians <a class="header-anchor" href="#segment-any-3d-gaussians" aria-label="Permalink to &quot;Segment Any 3D Gaussians&quot;">​</a></h2><p><a href="https://jumpat.github.io/SAGA" target="_blank" rel="noreferrer">https://jumpat.github.io/SAGA</a></p><h3 id="motivation-18" tabindex="-1">motivation <a class="header-anchor" href="#motivation-18" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>过去的方法：</p><ul><li>使用2D模型中提取2D特征，提升到3D空间中，用3D特征的相似性判断两个带你是否属于同一对象：速度较快，但分割粒度粗，因为缺乏解析嵌入特征的机制（e.g. 分割解码层）</li></ul><blockquote><p>related word中提到：仅仅依靠欧几里得距离或余弦距离时无法充分利用嵌入在高维视觉特征中的信息，因此此类方法的分割质量是有限的。</p></blockquote><ul><li>(segment anything nerf)直接将细粒度的2D分割结果投影到3Dmask grid上：产生精确的结果，但是由于多次执行基础模型和体渲染，导致过大时间开销</li></ul><p>3DGS绕过对广泛且空的空间的处理，提供丰富的显式信息，成为分割任务的理想选择</p><p>SAGA避免了推理期间多次前向传递 2D 分割模型的时间消耗。蒸馏是通过基于 SAM [23] 自动提取的掩码来训练高斯的 3D 特征实现的。在推理过程中，根据输入提示生成一组查询，然后通过高效的特征匹配检索预期的高斯。支持点、涂鸦和掩码在内的各种提示类型。</p><h3 id="method-19" tabindex="-1">method <a class="header-anchor" href="#method-19" aria-label="Permalink to &quot;method&quot;">​</a></h3><h4 id="sam" tabindex="-1">SAM <a class="header-anchor" href="#sam" aria-label="Permalink to &quot;SAM&quot;">​</a></h4><p>提供一个输入图片<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">I</span></span></span></span>和提示（可以是point，涂鸦或者mask）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span></span></span></span>，得到一个掩码<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>M</mi><mo>=</mo><mi>S</mi><mi>A</mi><mi>M</mi><mo stretchy="false">(</mo><mi>I</mi><mo separator="true">,</mo><mi>P</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">M = SAM(I,P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.07847em">I</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mclose">)</span></span></span></span></span></p><h4 id="overall-pipeline" tabindex="-1">Overall pipeline <a class="header-anchor" href="#overall-pipeline" aria-label="Permalink to &quot;Overall pipeline&quot;">​</a></h4><h5 id="训练阶段" tabindex="-1">训练阶段： <a class="header-anchor" href="#训练阶段" aria-label="Permalink to &quot;训练阶段：&quot;">​</a></h5><p>给定一个预训练完成的3DGS model <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal">G</span></span></span></span>，输入图片<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">I</span></span></span></span></p><p>对于一组图片，使用SAM得到一组2D特征图<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>F</mi><mi>I</mi><mrow><mi>S</mi><mi>A</mi><mi>M</mi></mrow></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><msup><mi>C</mi><mrow><mi>S</mi><mi>A</mi><mi>M</mi></mrow></msup><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><annotation encoding="application/x-tex">F^{SAM}_I∈R^{C^{SAM}×H×W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1167em;vertical-align:-.2753em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-2.4247em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.07847em">I</span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.05764em">S</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2753em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1.0064em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.00773em">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0064em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.07153em">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.9191em"><span style="top:-2.931em;margin-right:.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.05764em">S</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span></span></span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:.08125em">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:.13889em">W</span></span></span></span></span></span></span></span></span></span></span></span>，和一组不同粒度的掩码<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>M</mi><mi>I</mi><mrow><mi>S</mi><mi>A</mi><mi>M</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">M^{SAM}_I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1167em;vertical-align:-.2753em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-2.4247em;margin-left:-.109em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.07847em">I</span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.05764em">S</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2753em"><span></span></span></span></span></span></span></span></span></span></p><p>在每个高斯上训练一个低维特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>g</mi></msub><mo>∈</mo><msup><mi>R</mi><mi>C</mi></msup></mrow><annotation encoding="application/x-tex">f_g∈R^C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9805em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1076em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.00773em">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.07153em">C</span></span></span></span></span></span></span></span></span></span></span>，通过SAM-guidance loss做监督。</p><blockquote><p>没太看懂的correspondence loss的定义</p><p>To further enhance the feature compactness, we derive point-wise correspondences from extracted masks and distills them into the features (i.e., the correspondence loss).</p></blockquote><h5 id="推理阶段" tabindex="-1">推理阶段： <a class="header-anchor" href="#推理阶段" aria-label="Permalink to &quot;推理阶段：&quot;">​</a></h5><p>从给定的提示<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span></span></span></span>中生成一组查询<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span></span></span></span>，用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span></span></span></span>​通过特征匹配来检索3DGS，此后用3DGS提供的类似点云的丰富显式先验来做后处理</p><h4 id="training-features-for-gaussians" tabindex="-1">Training Features for Gaussians <a class="header-anchor" href="#training-features-for-gaussians" aria-label="Permalink to &quot;Training Features for Gaussians&quot;">​</a></h4><h5 id="sam-guidance-loss" tabindex="-1">SAM-guidance Loss <a class="header-anchor" href="#sam-guidance-loss" aria-label="Permalink to &quot;SAM-guidance Loss&quot;">​</a></h5><p>先将SAM的特征通过MLP <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>φ</mi></mrow><annotation encoding="application/x-tex">φ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.1944em"></span><span class="mord mathnormal">φ</span></span></span></span>降维到和高斯相同</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>F</mi><mi>I</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo>=</mo><mi>φ</mi><mo stretchy="false">(</mo><msubsup><mi>F</mi><mi>I</mi><mrow><mi>S</mi><mi>A</mi><mi>M</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F'_I=φ(F^{SAM}_I)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0489em;vertical-align:-.247em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8019em"><span style="top:-2.453em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.07847em">I</span></span></span><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-.25em"></span><span class="mord mathnormal">φ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8913em"><span style="top:-2.453em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.07847em">I</span></span></span><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.05764em">S</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p>对于每一个mask，做一个maked平均池化，比如一个pikachu的mask，将皮卡丘上所有像素点的特征平均池化成一个特征向量(论文中称为Query)<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mi>M</mi></msup><mo>∈</mo><msup><mi>R</mi><mi>C</mi></msup></mrow><annotation encoding="application/x-tex">T^M∈R^C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8804em;vertical-align:-.0391em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.00773em">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.07153em">C</span></span></span></span></span></span></span></span></span></span></span>，现在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mi>M</mi></msup></mrow><annotation encoding="application/x-tex">T^M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span></span></span></span></span></span></span></span></span></span></span>就代表pikachu</p><figure><img src="/ayene-no-blog/assets/image-20240528161206976.D8QENkwp.png" alt="image-20240528161206976" loading="lazy" decoding="async"></figure><figure><img src="/ayene-no-blog/assets/image-20240528161215385.DsoaQOwh.png" alt="image-20240528161215385" loading="lazy" decoding="async"></figure><p>代码对应如下：</p><figure><img src="/ayene-no-blog/assets/image-20240528165428089.NlkHRkqp.png" alt="image-20240528165428089" loading="lazy" decoding="async"></figure><p>然后将render出的特征图和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mi>M</mi></msup></mrow><annotation encoding="application/x-tex">T^M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span></span></span></span></span></span></span></span></span></span></span>做点积，相当于使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>M</mi></msub></mrow><annotation encoding="application/x-tex">T_M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>在feature map上做查询，询问哪些像素点是pikachu？如果是，该像素上的feature点乘<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mi>M</mi></msup></mrow><annotation encoding="application/x-tex">T^M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span></span></span></span></span></span></span></span></span></span></span>就会得到较高的值（余弦相似度高），从而得到分割图<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>M</mi></msub></mrow><annotation encoding="application/x-tex">P_M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>，再通过Sigmoid归一化成概率图</p><figure><img src="data:image/png;base64,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" alt="image-20240528172903491" loading="lazy" decoding="async"></figure><p>将查询到的分割图和真实的分割图做二值交叉熵进行训练</p><figure><img src="/ayene-no-blog/assets/image-20240528174904961.D6Xsgh_T.png" alt="image-20240528174904961" loading="lazy" decoding="async"></figure><h5 id="correspondence-loss" tabindex="-1">Correspondence Loss <a class="header-anchor" href="#correspondence-loss" aria-label="Permalink to &quot;Correspondence Loss&quot;">​</a></h5><p>TODO : 总之后面在看，先看推理</p><h3 id="inference" tabindex="-1">Inference <a class="header-anchor" href="#inference" aria-label="Permalink to &quot;Inference&quot;">​</a></h3><h5 id="sam-based-prompt" tabindex="-1">SAM-based Prompt <a class="header-anchor" href="#sam-based-prompt" aria-label="Permalink to &quot;SAM-based Prompt&quot;">​</a></h5><p>可以直接使用SAM低维特征做查询。</p><p>先将prompts给SAM生成一个精确的2D分割掩码<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>M</mi><mi>v</mi><mrow><mi>r</mi><mi>e</mi><mi>f</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">M_v^{ref}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0961em;vertical-align:-.247em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-2.453em;margin-left:-.109em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight" style="margin-right:.10764em">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span></p><p>在这个掩码上做平均池化得到一个查询<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mrow><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">Q^{mask}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0435em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span></span></span></span></span></span></span></span></span>[注: 查询应该是一个C维特征向量]</p><p>用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mrow><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">Q^{mask}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0435em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span></span></span></span></span></span></span></span></span>分割2D渲染特征图<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>F</mi><mi>v</mi><mi>r</mi></msubsup></mrow><annotation encoding="application/x-tex">F^r_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9303em;vertical-align:-.247em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.6644em"><span style="top:-2.453em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span>得到2D分割掩码<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>M</mi><mi>v</mi><mrow><mi>t</mi><mi>e</mi><mi>m</mi><mi>p</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">M_v^{temp}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0406em;vertical-align:-.247em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.7936em"><span style="top:-2.453em;margin-left:-.109em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span></p><p>如果<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>M</mi><mi>v</mi><mrow><mi>r</mi><mi>e</mi><mi>f</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">M_v^{ref}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0961em;vertical-align:-.247em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-2.453em;margin-left:-.109em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight" style="margin-right:.10764em">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>M</mi><mi>v</mi><mrow><mi>t</mi><mi>e</mi><mi>m</mi><mi>p</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">M_v^{temp}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0406em;vertical-align:-.247em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.7936em"><span style="top:-2.453em;margin-left:-.109em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span>的交集占据了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>M</mi><mi>v</mi><mrow><mi>r</mi><mi>e</mi><mi>f</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">M_v^{ref}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0961em;vertical-align:-.247em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-2.453em;margin-left:-.109em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight" style="margin-right:.10764em">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span>超过90%的空间，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mrow><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">Q^{mask}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0435em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span></span></span></span></span></span></span></span></span>就作为正式的查询。</p><p>否则，使用K-means算法来从低维特征中提取另一组查询<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Q</mi><mi>v</mi><mrow><mi>k</mi><mi>m</mi><mi>a</mi><mi>n</mi><mi>s</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">Q^{kmans}_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0961em;vertical-align:-.247em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-2.453em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">kman</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span></p><p>使用这个策略是因为分割目标可能包含许多components，不能简单的使用masked平均池化来捕获</p><hr><p>获得查询<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Q</mi><mi>v</mi><mrow><mi>S</mi><mi>A</mi><mi>M</mi></mrow></msubsup><mo>=</mo><msubsup><mi>Q</mi><mi>v</mi><mrow><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow></msubsup><mspace width="1em"><mi>o</mi><mi>r</mi><mspace width="1em"><msubsup><mi>Q</mi><mi>v</mi><mrow><mi>k</mi><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>s</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">Q^{SAM}_v=Q^{mask}_v\quad or\quad Q^{kmeans}_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-.247em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-2.453em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.05764em">S</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1.0961em;vertical-align:-.247em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-2.453em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:1em"></span><span class="mord mathnormal" style="margin-right:.02778em">or</span><span class="mspace" style="margin-right:1em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-2.453em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">km</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">an</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span>后，用点乘计算每个高斯点的特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">f_g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9805em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1076em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>和查询<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span></span></span></span>的相似度，如果查询分数大于自适应阈值（所有分数的均值和标准差之和）就认为是所要的点</p><h2 id="nerfing-it-offline-object-segmentation-through-implicit-modeling" tabindex="-1">NeRFing it: Offline Object Segmentation Through Implicit Modeling <a class="header-anchor" href="#nerfing-it-offline-object-segmentation-through-implicit-modeling" aria-label="Permalink to &quot;NeRFing it: Offline Object Segmentation Through Implicit Modeling&quot;">​</a></h2><p>ICRA2023 课程论文，按详细翻译的顺序读吧</p><h3 id="abstract" tabindex="-1">Abstract <a class="header-anchor" href="#abstract" aria-label="Permalink to &quot;Abstract&quot;">​</a></h3><ul><li>最近的机器人感知方法基于深度学习，需要非常大的数据集才表现良好，且模型准确性取决于数据分布，因此训练数据至关重要</li><li>收集和标注数据是一个重要瓶颈，需要高效的数据收集和标记pipeline</li><li>本文提出了一种使用<strong>最少的人工标记</strong>计算<strong>RGB-D视频序列</strong>的高质量对象<strong>分割图</strong>的方法。利用NeRF学习的<strong>密度</strong>来推断场景的几何形状，用它来使用<strong>用户提供的单个3D边界框</strong>计算密集分割图。</li><li>研究了计算分割图的准确性，并提出用NeRF生成新视角来合成新训练样本的方法（这不是NeRF自带的么…）</li><li>结果：能够计算出准确的分割图，还展示了使用合成的新样本提高下游目标检测任务的性能。</li></ul><h2 id="introduction" tabindex="-1">introduction <a class="header-anchor" href="#introduction" aria-label="Permalink to &quot;introduction&quot;">​</a></h2><ul><li>现有的感知方法大多基于监督学习[1-4]，需要大量的标注样本来拟合参数，其他方法效果不如监督学习好，而且往往也可以从标注数据中受益[6]。</li><li>对于语义分割，图像通常从单个image上绘制多边形来进行标注，这可能非常耗时，使得标注数据的成本超过了机器人application所能承受的水平。</li><li>LabelFusion通过构建环境的密集重建来快速标注RGB-D数据，可以计算具有已知mesh的物体的6D姿态标注，有已知mesh时效果很好，但大多数情况下这些mesh是不可用的。（可以提一嘴获取mesh的难度），如果有一个工具能在未知mesh的情况下快速提供真实标签，就可以在之前无法实现的地方部署更强大的监督学习算法。</li><li>为了克服上述难点，引入了一个<strong>生成语义分割图、6D姿态和3D边界框</strong>的管道，适用于RGB-D序列。利用NeRF来恢复场景的3D结构，并使用学习到的NeRF模型来计算每个RGB-D帧中物体分割图和边界框，NeRF应用RGB图形和已知的相机姿态来学习场景的隐式表示，本文呢还加入了深度来作为NeRF的额外监督信号</li></ul><p>贡献总结如下：</p><ul><li>提出了一个pipeline，通过深度监督的NeRF计算场景重建和高质量的语义分割图，适用于RGB-D图像序列</li><li>提出了一种方法，通过合成新视角并计算label来生成额外的训练样本</li></ul><p>在不同物体上评估了我们提出的pipeline，将生成的标签与逐帧标注的语义分割图（gt？）和另外两种pipeline比较，第一种是使用gt mesh model模型的，第二种是集成了TSDF但不需要mesh的。此外，还与SOTA pipeline Rapid Pose Labels进行了比较。最后评估了提出的数据增强方案在下游object detection 任务的有效性。</p><h3 id="related-work-3" tabindex="-1">related work <a class="header-anchor" href="#related-work-3" aria-label="Permalink to &quot;related work&quot;">​</a></h3><ul><li>2D标注和主动学习：[10]直接在图像上绘制，加速人工标注，[11]通过将创建semantic mask的问题简化为关键点标注任务来加速标注，[12,13,14]引入主动学习使用小部分训练数据加速创建真实数据集，这些方法可以和本文方法结合使用，利用本文在预处理步骤中恢复的不同视角进一步减少标注负担。</li><li>3D数据标注：主要提到当前的SOTA pipeline Rapid Pose Labels，为RGB-D视频标注物体姿态、分割掩码和边界框。提到Rapid Pose Labels需要对同一物体多次扫描，不能应用于关节或可变形物体，假定所有标注的2D关键点都有深度测量值，因此难以应用于反射或透明物体。（这里可以argue一下看看后面有没有反射或透明物体的描述）此外，该方法对于生成的物体点云进行后处理，需要额外的工作。我们的方法能够解决这些限制，在实验中与Rapid Pose Labels进行了比较。</li><li>神经辐射场：NeRF可以逼真的合成新视角，近年的工作[27,28]引入深度信息改善NeRF合成质量，[29, 30]扩展NeRF来推断几何信息。本文方法类似于[30]，用深度图来监督NeRF，以较少的人力生成高质量语义分割图，但我们的系统在离线方法中完成学习和处理，使用时不需要大量时间进行推理，支持机器人的线上运行。</li></ul><h3 id="method-20" tabindex="-1">method <a class="header-anchor" href="#method-20" aria-label="Permalink to &quot;method&quot;">​</a></h3><p>我们的目标是获得手持RGB-D图像序列汇总每一帧的高质量语义分割图、3D边界框和物体的6D姿势。RGB-D图像包含两种信息：RGB图像（红、绿、蓝三个颜色通道的标准彩色图像）和深度图像（每个像素点到摄像头的距离信息）。</p><p>Overview如下：</p><ul><li>计算每帧的相机位姿</li><li>计算场景的3D点云</li><li>学习一个深度监督的NeRF模型来表示场景</li><li>使用3D图形用户界面对物体进行边界框标注</li><li>使用学习到的NeRF模型和3D边界框计算密集的语义分割图</li></ul><h4 id="compute-camera-poses-for-each-frame" tabindex="-1">Compute camera poses for each frame <a class="header-anchor" href="#compute-camera-poses-for-each-frame" aria-label="Permalink to &quot;Compute camera poses for each frame&quot;">​</a></h4><p>目的：为了训练NeRF模型，在帧之间传bounding box labels</p><p>方法：在视频序列的所有帧上运行hloc（开源视觉工具包），github：<a href="https://github.com/cvg/Hierarchical-Localization%EF%BC%8C%E5%B0%B1%E6%8B%A5%E6%9C%89%E4%BA%86%E4%B8%80%E4%B8%AA%E5%88%9D%E6%AD%A5%E7%9A%84%E7%9B%B8%E6%9C%BA%E8%BD%A8%E8%BF%B9%EF%BC%8C%E7%8E%B0%E5%9C%A8%E8%BF%98%E8%A6%81%E5%B0%86%E8%BD%A8%E8%BF%B9%E5%92%8C%E7%9C%9F%E5%AE%9E%E7%9A%84%E7%89%A9%E7%90%86%E5%B0%BA%E5%BA%A6%E5%AF%B9%E9%BD%90%EF%BC%8C%E5%B0%B1%E9%9C%80%E8%A6%81%E5%BE%97%E5%88%B0%E4%B8%80%E4%B8%AA%E7%BC%A9%E6%94%BE%E5%9B%A0%E5%AD%90%EF%BC%8C%E6%9C%AC%E6%96%87%E9%80%9A%E8%BF%87%E4%BD%BF%E6%B5%8B%E9%87%8F%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%92%8C%E9%80%9A%E8%BF%87hloc%E4%B8%89%E8%A7%92%E6%B5%8B%E9%87%8F%E7%9A%84%E7%82%B9%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE%E5%BC%82%E6%9C%80%E5%B0%8F%E5%8C%96%EF%BC%8C%E5%9C%A8RANSAC%E5%BE%AA%E7%8E%AF%E4%B8%AD%E8%BF%87%E6%BB%A4%E6%8E%89%E7%A6%BB%E7%BE%A4%E5%80%BC%EF%BC%8C%E6%89%BE%E5%88%B0%E4%B8%80%E4%B8%AA%E6%AF%94%E4%BE%8B%E5%9B%A0%E5%AD%90%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%AF%B9%E7%BB%93%E6%9E%9C%E8%BD%A8%E8%BF%B9%E8%BF%9B%E8%A1%8C%E7%BC%A9%E6%94%BE%E3%80%82%E6%AD%A4%E5%90%8E%EF%BC%8C%E7%BB%99%E5%AE%9A%E7%9B%B8%E6%9C%BA%E5%A7%BF%E5%8A%BF%E5%92%8C" target="_blank" rel="noreferrer">https://github.com/cvg/Hierarchical-Localization，就拥有了一个初步的相机轨迹，现在还要将轨迹和真实的物理尺度对齐，就需要得到一个缩放因子，本文通过使测量的深度和通过hloc三角测量的点之间的差异最小化，在RANSAC循环中过滤掉离群值，找到一个比例因子，从而对结果轨迹进行缩放。此后，给定相机姿势和</a> RGB-D 帧，我们可以重建场景的点云。</p><h4 id="learning-a-nerf-model-of-the-scene" tabindex="-1">Learning a NeRF Model of the Scene <a class="header-anchor" href="#learning-a-nerf-model-of-the-scene" aria-label="Permalink to &quot;Learning a NeRF Model of the Scene&quot;">​</a></h4><p>在光度损失上和原始NeRF保持一致，这里额外引入了一个深度项，这是为了保证NeRF在各种类型的场景下都有比较好的结果，文中提到平面表面可能会被重建为不平整的，或密度会被分配到物体不存在的空闲空间，因此加入了深度损失来消除歧义</p><h4 id="computing-object-labels" tabindex="-1">Computing Object Labels <a class="header-anchor" href="#computing-object-labels" aria-label="Permalink to &quot;Computing Object Labels&quot;">​</a></h4><p>1、用户使用GUI在重建的场景点云中为感兴趣的物体放置3D bounding box</p><p>2、用NeRF为每一帧渲染完整的密集深度帧，使用相机内参将每个深度帧转换为点云</p><p>3、利用相机位姿和bouding box，根据物体类别对点云中的每个点进行分类，丢弃不属于任何物体的点。</p><p>4、将物体点投影回image frame，获得了每个物体的密集分割掩码</p><p>5、通过计算包含分割物体的最紧密bounding box来计算2D bounding box for detection</p><p>6、物体的姿态可以通过将3D bounding box 转换为相机坐标系来计算</p><h4 id="generating-synthetic-training-examples" tabindex="-1">Generating Synthetic Training Examples <a class="header-anchor" href="#generating-synthetic-training-examples" aria-label="Permalink to &quot;Generating Synthetic Training Examples&quot;">​</a></h4><p>文中还提到可以合成新视角来扩展原始数据集，这是NeRF原有的功能，本文在这节做的工作时寻找一个合适的相机位姿来合成新视角，这个位姿下看见的目标物体表面应该在其他已有的相机位姿下被看到过，本节提出了相机位姿采样策略，计算在该新视角下的颜色、深度、分割掩码： 1、计算object坐标系下相机姿势的边界框，在这个边界框内均匀采样相机位置</p><p>2、过滤掉太接近点云的点</p><p>3、计算一个viewpoint，指向场景中的一个物体</p><p>4、在x和y轴上从[-π/4, π/4]弧度和在z轴上从[-π, π]弧度（其中z轴向前，x轴向右，y轴向下）均匀采样的随机旋转来调整方向。</p><p>图示： <img src="/ayene-no-blog/assets/image-20240605201622421.ve3mT-Zo.png" alt="image-20240605201622421" style="zoom:80%"></p><h3 id="experiments-12" tabindex="-1">Experiments <a class="header-anchor" href="#experiments-12" aria-label="Permalink to &quot;Experiments&quot;">​</a></h3><p>数据收集设备：配备了飞行时间深度传感器的Apple iPhone 12 Pro</p><p>训练图像分辨率：960x720</p><p>深度帧分辨率：256x192，通过上采样匹配RGB图像</p><h4 id="a-label-accuracy" tabindex="-1">A. Label Accuracy <a class="header-anchor" href="#a-label-accuracy" aria-label="Permalink to &quot;A. Label Accuracy&quot;">​</a></h4><p>baseline：</p><p>1、用户给定bounding box，利用相机内参将深度图转换为点云，将点云分类为在bounding box和不在bounding box内的点，将它们投影到图像帧获得mask（实验表格中简称Depth）</p><p>2、使用TSDF表示场景，用marching cube算法提取网格，使用bounding box从网格中剪切出物体，并将物体渲染为分割图</p><p>（这里和introduction里不一样，可能是写作上的失误，可以注明一下）</p><p>同时与手动标注的分割图进行对比。</p><p>实验结果如下： <img src="/ayene-no-blog/assets/image-20240606163801438.CuULBg5x.png" alt="image-20240606163801438" style="zoom:50%"></p><p>第二列和第三列分别是Depth和TSDF的baseline，第四列是本文的实验结果，最后一列是新视角合成下的实验结果，数值是预测出的mask和手动标注的mask之间的mIOU</p><p>先看基于Depth的方法，实验结果普遍比基于TSDF的方法和基于NeRF的方法要差，这是因为RGB-D传感器捕获的深度图噪声大，且有许多像素没有测量值（前文有提到深度图是低分辨率的，依靠上采样来与彩色图像的分辨率对齐）</p><p>再看基于TSDF的方法，在某些情况下基于TSDF的方法与NeRF表现相似，特别是对于深度图质量好的简单形状物体，比如消防栓和公园长椅，在透明物体（酒杯 wine glass，茶壶teapot）和反光物体（car 汽车，wine bottles酒瓶）时也表现不佳。</p><p>最后是本文提出的方法，对于NeRF的方法仅在透明物体下表现不佳。</p><p>下面是一些定性结果，NeRF的分割示例，可以关注左下角的结果，分别是teapot和wine glass，teapot是因为茶柄中间也被认为是分割结果而iou低，酒瓶则是因为底部区域没有被判定为分割区域而导致iou低</p><figure><img src="/ayene-no-blog/assets/image-20240606171802142.BQiK3mOG.png" alt="image-20240606171802142" loading="lazy" decoding="async"></figure><p>还有一些失败样例</p><figure><img src="/ayene-no-blog/assets/image-20240606171912778.C3qO-kFI.png" alt="image-20240606171912778" loading="lazy" decoding="async"></figure><p>最后还与目前的SOTA方法Rapid Pose Labels进行比较，使用了oolong ice-tea bottle dataset，Rapid Pose Labels只取得了0.801的mIoU，而本文的方法达到了0.902的mIoU</p><h4 id="b-depth-supervision" tabindex="-1">B. Depth Supervision <a class="header-anchor" href="#b-depth-supervision" aria-label="Permalink to &quot;B. Depth Supervision&quot;">​</a></h4><p>在本节中，展示了为NeRF加入深度监督后带来的改进，以及不同的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的影响。具体结果如下图，观察第二列，可以看到没有深度监督的情况下会在接种产生伪影，最终导致影响分割掩码</p><figure><img src="/ayene-no-blog/assets/image-20240606173256851.Pcck_Otd.png" alt="image-20240606173256851" loading="lazy" decoding="async"></figure><p>下面是cup场景下不同<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的mIOU</p><figure><img src="/ayene-no-blog/assets/image-20240606173910426.BnAYk1Mr.png" alt="image-20240606173910426" loading="lazy" decoding="async"></figure><h4 id="c-novel-view-synthesis" tabindex="-1">C. Novel View Synthesis <a class="header-anchor" href="#c-novel-view-synthesis" aria-label="Permalink to &quot;C. Novel View Synthesis&quot;">​</a></h4><p>本节展示了使用新视角合成生成额外数据集的质量，可以看到大多数图像还是表现良好的，但一些图像存在视觉伪影，主要出现在原始数据集中不可见的部分以及从样本外的视角观察物体表面的时候，有伪影的图像可以参考第一行第三列的天空和地面以及第二行第五列的栅栏。</p><figure><img src="/ayene-no-blog/assets/image-20240606175852401.CKDNXwxe.png" alt="image-20240606175852401" loading="lazy" decoding="async"></figure><p>本节还定量比较了是否使用扩展数据集对语义分割和目标检测任务的影响，下图中x轴是使用合成数据集的比例，纵轴是准确度，可以发现分割任务更早达到峰值，这可能是因为对于分割来说，数据集的sharpness和texture质量更加重要，这是NeRF合成的新视角下做的不够好的部分。</p><figure><img src="/ayene-no-blog/assets/image-20240606185035033.DsYeCNzU.png" alt="image-20240606185035033" loading="lazy" decoding="async"></figure><h3 id="discussion-and-conclusions" tabindex="-1">DISCUSSION AND CONCLUSIONS <a class="header-anchor" href="#discussion-and-conclusions" aria-label="Permalink to &quot;DISCUSSION AND CONCLUSIONS&quot;">​</a></h3><p>本文提出了一个用NeRF创建语义分割label的pipeline，展示了NeRF恢复的几何信息可以生成高质量的分割掩码，并且优于现有的baseline。还展示了NeRF可以生成新视角来训练模型，减少收集数据的负担。</p><p>limitation</p><ul><li>依赖于精确的相机位姿</li><li>推断透明和完全透明物体的几何形状仍然是一个挑战</li><li>更复杂的场景中，bounding box里可能包含别的物体，从而产生噪声标签</li><li>场景几何可能未被完美推断（teapot示例），导致标签边缘和物体边界不匹配，可以加入让用户细化标签的功能。</li></ul><h2 id="physdreamer-physics-based-interaction-with-3d-objects-via-video-generation" tabindex="-1">PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation <a class="header-anchor" href="#physdreamer-physics-based-interaction-with-3d-objects-via-video-generation" aria-label="Permalink to &quot;PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation&quot;">​</a></h2><h3 id="motivation-19" tabindex="-1">motivation <a class="header-anchor" href="#motivation-19" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>物理材料很复杂，但是人却能很轻易的想象：例如一朵玫瑰花的摇曳，这来源于人类于物理世界交互所获得的先验知识，这启发了我们从视频生成模型中提炼动态先验，这些模型已在包含丰富多样物理世界视频的庞大数据集上进行了训练。</p><p>（其实就是认为视频生成模型的数据集都是物理世界的数据集，因此具备一定的物理先验）</p><h3 id="problem-formulation" tabindex="-1">problem formulation <a class="header-anchor" href="#problem-formulation" aria-label="Permalink to &quot;problem formulation&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20250108170515740.DuNHabaG.png" alt="image-20250108170515740" loading="lazy" decoding="async"></figure><p>使用mpm方法进行物理模拟需要的三个参数</p><p>粒子质量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">m_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7167em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>：预先计算为常数密度和粒子体积<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">V_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9694em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.2222em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>的乘积，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">V_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9694em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.2222em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>通过背景网格单元除以该单元所包含的粒子数估算。</p><p>泊松比<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">v_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7167em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>：实验表明对物体运动的影响忽略不计，假设泊松比为均匀的常数值</p><p>杨氏模量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">E_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9694em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0576em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>：通过查询杨氏模量场<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mi>p</mi></msub><mo>=</mo><mi>E</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>p</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E_p=E(x_p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9694em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0576em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mord mathnormal" style="margin-right:.05764em">E</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>获得</p><p>初速度：通过查询初速度场<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mn>0</mn></msub><mo>=</mo><msub><mi>v</mi><mn>0</mn></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v_0=v_0(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.0359em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></p><p>把对每个高斯的物理参数估计转换成了对一个杨氏模量场进行估计的问题</p><h3 id="method-21" tabindex="-1">method <a class="header-anchor" href="#method-21" aria-label="Permalink to &quot;method&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20241206155902475._2ViFTf0.png" alt="image-20241206155902475" loading="lazy" decoding="async"></figure><p>主要的pipeline就是这个</p><ul><li>先用diffusion视频生成模型生成物理真实的动态视频</li><li>用可微物理进行mpm模拟，优化初速度和杨氏模量场</li></ul><p>一些优化</p><ul><li>初速度场和杨氏模量场的参数转为2个三平面，用一个全局平滑正则</li><li>分阶段优化，第一阶段冻结杨氏模量优化初速度，只优化前三帧，第二阶段冻结初速度优化杨氏模量，梯度只流向前一帧</li><li>加速仿真，每8个粒子下采样为一个cluster，减少需要仿真的粒子数量</li></ul><figure><img src="/ayene-no-blog/assets/image-20241206160622293.CCDCjHE2.png" alt="image-20241206160622293" loading="lazy" decoding="async"></figure><h3 id="experiments-13" tabindex="-1">experiments <a class="header-anchor" href="#experiments-13" aria-label="Permalink to &quot;experiments&quot;">​</a></h3><p>数据集：alocasia、carnations、hat、telephone</p><h2 id="physics3d-learning-physical-properties-of-3d-gaussians-via-video-diffusion" tabindex="-1">Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion <a class="header-anchor" href="#physics3d-learning-physical-properties-of-3d-gaussians-via-video-diffusion" aria-label="Permalink to &quot;Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion&quot;">​</a></h2><p>简单来说在PhysGaussian的基础上引入了另外的物理参数</p><h2 id="dreamphysics-learning-physics-based-3d-dynamics-with-video-diffusion-priors" tabindex="-1">DreamPhysics: Learning Physics-Based 3D Dynamics with Video Diffusion Priors <a class="header-anchor" href="#dreamphysics-learning-physics-based-3d-dynamics-with-video-diffusion-priors" aria-label="Permalink to &quot;DreamPhysics: Learning Physics-Based 3D Dynamics with Video Diffusion Priors&quot;">​</a></h2><h3 id="motivation-20" tabindex="-1">motivation <a class="header-anchor" href="#motivation-20" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20241206162723094.1pttydyM.png" alt="image-20241206162723094" loading="lazy" decoding="async"></figure><p>a：physGaussian，无法估计物理参数，需要手动去设定</p><p>b：即使是最先进的视频生成模型，也不能很好的生成准确的物理结果</p><p>c：我们认为b方法提取物理先验的方法不当，并不是食品模型本身的局限性，因此本文的问题是如何挖掘和应用视频生成模型中的物理知识，以实现逼真的动态3D合成？</p><p>（与b的区别在于，b是要先生成一个物理真实的视频，然后进行可微渲染，本文是打算直接从视频生成模型中蒸馏出物理先验）</p><h3 id="method-22" tabindex="-1">method <a class="header-anchor" href="#method-22" aria-label="Permalink to &quot;method&quot;">​</a></h3><p>TODO</p><h2 id="sim-anything-automated-3d-physical-simulation-of-open-world-scene-with-gaussian-splatting" tabindex="-1">Sim Anything: Automated 3D Physical Simulation of Open-world Scene with Gaussian Splatting <a class="header-anchor" href="#sim-anything-automated-3d-physical-simulation-of-open-world-scene-with-gaussian-splatting" aria-label="Permalink to &quot;Sim Anything: Automated 3D Physical Simulation of Open-world Scene with Gaussian Splatting&quot;">​</a></h2><figure><img src="/ayene-no-blog/assets/image-20241206172424151.Dx1yYK5c.png" alt="image-20241206172424151" loading="lazy" decoding="async"></figure><p>pipeline</p><ul><li><p>分割</p></li><li><p>将分割目标渲染，输入BLIP（一个VQA，视觉问答模型，可以描述这幅图像），然后将描述和图像输入给MLLM（多模态大语言模型），让它输出k个候选材料、是否刚性，然后通过计算图像与k个候选材料的CLIP相似度，得到最match的材料，最后将材料、图像、文本描述输入MLLM，得到物理属性：密度、杨氏模量和泊松比。</p></li><li><p>随后</p></li><li><p>需要将得到的物理属性通过一个分布分配到每个高斯核中，这个分布训练一个网络<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">D_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.0278em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>表示，通过Physics3D的预测结果作为监督，将粒子位置和平均值作为输入，即理属性在粒子之间的几何感知概率分布<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span></span></span></span>定义为：</p></li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo>=</mo><msub><mi>D</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P=D_\theta(X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.0278em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mclose">)</span></span></span></span></span></p><p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span>为粒子位置，然后把上一节得到的物理属性平均值按分布分配给粒子</p><p>除了分配分布，还和PhysDreamer一样进行了粒子的降采样和cluster</p><h2 id="pugs-zero-shot-physical-understanding-with-gaussian-splatting" tabindex="-1">PUGS: Zero-shot Physical Understanding with Gaussian Splatting <a class="header-anchor" href="#pugs-zero-shot-physical-understanding-with-gaussian-splatting" aria-label="Permalink to &quot;PUGS: Zero-shot Physical Understanding with Gaussian Splatting&quot;">​</a></h2><h3 id="motivation-21" tabindex="-1">motivation <a class="header-anchor" href="#motivation-21" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>在使用3DGS估计物理参数的问题上，依次遇到了如下困难</p><ul><li><p>原始3DGS的几何表示仅考虑颜色，不符合物体的真实分布（这些在之前的文章也有体现，例如需要内部填充粒子）：对此提出了geometryaware regularization loss（几何感知正则化）</p></li><li><p>对于材料的理解缺乏局部区域关联性：提出region-aware feature contrastive loss（区域感知特征）：这里有点问题，原始GS又没有材料理解功能，是在说NeRF2Phyics吗</p><figure><img src="/ayene-no-blog/assets/image-20250317160307161.tfFzriIu.png" alt="image-20250317160307161" loading="lazy" decoding="async"></figure></li><li><p>物理参数估计的问题：对重建结果使用VLM模型，利用上述的region-aware feature将物理属性分布到3D上（2D-&gt;3D）</p></li><li><p>对于物体级的属性（例如质量，就是一整个物体的属性而不是一个杨氏模量场等等），提出基于高斯的体积分模块（虽然没看后面盲猜一个类渲染魔改cuda）</p></li></ul><h3 id="method-23" tabindex="-1">method <a class="header-anchor" href="#method-23" aria-label="Permalink to &quot;method&quot;">​</a></h3><h4 id="geometryaware-regularization-loss" tabindex="-1">geometryaware regularization loss: <a class="header-anchor" href="#geometryaware-regularization-loss" aria-label="Permalink to &quot;geometryaware regularization loss:&quot;">​</a></h4><p>一个是来自TVCG 2025 PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity Surface Reconstruction的loss</p><figure><img src="/ayene-no-blog/assets/image-20250317164034961.CDAE8FFv.png" alt="image-20250317164034961" loading="lazy" decoding="async"></figure><p>就是一个通过深度预测的法线和真实法线的一致性，本篇文章引入这个loss的目的是给训练引入法线和深度信息，来约束几何信息的正确性。</p><p>N是体渲染法线，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">N_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.109em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>是深度图中，选择点p，然后选取上下左右四个像素点，投影到三维空间，计算其局部法线得到的深度推导的法线图</p><figure><img src="/ayene-no-blog/assets/image-20250317164139900.CqNnish-.png" alt="image-20250317164139900" loading="lazy" decoding="async"></figure><p>一个是来自point-nerf：这个loss比较常见，就是希望透明度趋向于0 / 1</p><figure><img src="/ayene-no-blog/assets/image-20250317164801223.CNARSZvh.png" alt="image-20250317164801223" loading="lazy" decoding="async"></figure><h4 id="region-aware-feature-contrastive-loss" tabindex="-1">Region-Aware Feature Contrastive Loss <a class="header-anchor" href="#region-aware-feature-contrastive-loss" aria-label="Permalink to &quot;Region-Aware Feature Contrastive Loss&quot;">​</a></h4><p>来自SAGA的loss，试图通过无监督训练给高斯训练一个场景特化特征，用SAM获得一系列2d掩码，然后如果任意两个像素p1,p2属于一个掩码，则coor=1，此时loss为负，余弦相似度越大，loss越小，即保证同一掩码的两个pix具有相似的特征</p><figure><img src="/ayene-no-blog/assets/image-20250317172720807.VeO7tstI.png" alt="image-20250317172720807" loading="lazy" decoding="async"></figure><h4 id="vlm-based-physical-property-prediction" tabindex="-1">VLM Based Physical Property Prediction <a class="header-anchor" href="#vlm-based-physical-property-prediction" aria-label="Permalink to &quot;VLM Based Physical Property Prediction&quot;">​</a></h4><p>随机选择多视角输入中的一张，从VLM中获得一个字典，材料名称 : 物理属性值</p><p>这里也有个奇怪的点，难道VLM不能获得某块区域对应的材料是什么吗？为什么最后还要用CLIP获得对齐，这样不是丢失了材料位置的2D信息吗</p><h4 id="feature-based-property-propagation" tabindex="-1">Feature Based Property Propagation <a class="header-anchor" href="#feature-based-property-propagation" aria-label="Permalink to &quot;Feature Based Property Propagation&quot;">​</a></h4><p>总之这一阶段的核心作用肯定是将材料（2D）和高斯点（3D）对齐，有很多方式，但是反投影肯定是最直接的。</p><p>对所有高斯进行体素下采样，获得一组源点（代表点），然后在每个输入视图下，先判断源点是否是表面点（利用深度测试），然后在该相机视角下进行反投影，得到源点-2D输入图像像素点的对应关系，得到源点对应的投影点后，以投影点为中心提取一个p*p patch，得到图像的CLIP特征，通过比较每个材料(text)和patch(p * p pixel)之间的CLIP相似度，将材料分配到源点上，再由源点分配到高斯核上（这里用了之前的Region-Aware Feature进行分配，s是一组源点），每个高斯核有材料之后，从之前的字典中获得物理属性。</p><h4 id="gaussian-volume-integration" tabindex="-1">Gaussian Volume Integration <a class="header-anchor" href="#gaussian-volume-integration" aria-label="Permalink to &quot;Gaussian Volume Integration&quot;">​</a></h4><p>根据代码，用VLM预测了一个密度，乘上高斯计算的体积来得到质量，但实际上众所周知高斯只有表面点，特别是经过前面的几何正则化之后，由深度和法线约束的高斯更只有表面点了，这时候算出来的体积肯定有问题，这时候作者又提出希望VLM预测该物体的“纯体积”，也就是能够预测内部结构，比如同样体积，气球的纯体积小，实心水球的纯体积就大，以此来计算真正的质量。</p><figure><img src="/ayene-no-blog/assets/image-20250317190526135.D0IwPAcV.png" alt="image-20250317190526135" loading="lazy" decoding="async"></figure><h4 id="experiments-14" tabindex="-1">Experiments <a class="header-anchor" href="#experiments-14" aria-label="Permalink to &quot;Experiments&quot;">​</a></h4><p>定性结果：材质分配</p><figure><img src="/ayene-no-blog/assets/image-20250318140355313.C_sMPQa-.png" alt="image-20250318140355313" loading="lazy" decoding="async"></figure><p>定量结果：在一个质量数据集上，比较质量估计结果的准确性</p><figure><img src="/ayene-no-blog/assets/image-20250318140602171.BEPJTWDJ.png" alt="image-20250318140602171" loading="lazy" decoding="async"></figure><p>应用：</p><p>部署了一个机器人应用，这个方法估计的杨氏模量场比较准确，所以机器人推测出抓这个棉帽子需要更小的机器人手臂开口大小，成功抓取。<img src="/ayene-no-blog/assets/image-20250318140914212.CTXbj_Ut.png" alt="image-20250318140914212"></p><h2 id="omniphysgs-3d-constitutive-gaussians-forgeneral-physics-based-dynamics-generation" tabindex="-1">OMNIPHYSGS: 3D CONSTITUTIVE GAUSSIANS FORGENERAL PHYSICS-BASED DYNAMICS GENERATION <a class="header-anchor" href="#omniphysgs-3d-constitutive-gaussians-forgeneral-physics-based-dynamics-generation" aria-label="Permalink to &quot;OMNIPHYSGS: 3D CONSTITUTIVE GAUSSIANS FORGENERAL PHYSICS-BASED DYNAMICS GENERATION&quot;">​</a></h2><h3 id="motivation-22" tabindex="-1">motivation <a class="header-anchor" href="#motivation-22" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>这篇文章的核心目的是生产物理真实的动态</p><ul><li>PhysGaussian需要手动调整物理属性</li><li>DreamPhysics、PhysDreamer：限于纯弹性材料</li><li>Physics3D：限于粘弹性材料</li></ul><p>以上方法的这些缺点，导致了无法成为一个<strong>通用、自动</strong>的解决方案</p><p>这篇文章的任务是合成通用的、基于物理的3D动态，通用是指为广泛的材料类型生成动态，包括纯弹性、粘弹性、塑性以及流体，同时还能处理不同材料之间的交互，因此应该具备一下特点</p><ul><li>强有力的物理约束</li><li>能够灵活处理各种材料和物体</li><li>能够无需手动建模</li></ul><p>因此本文提出了使用本构模型（Constitutive Models）来描述高斯粒子，将高斯扩展为本构高斯（<strong>Constitutive Gaussians</strong>）。具体来说，先使用原始3DGS建模，再使用mpm模拟动态，每个高斯被扩展为本构高斯，其扩展特征被提取并通过物理感知的decoder处理，以预测应变和应力。这些预测结果与一组专家设计的本构模型相结合，以处理各种材料并增强动态场景的物理合理性。本构高斯粒子的模拟状态被渲染为视频帧，并通过文本提示输入到预训练的视频扩散模型中。最后，可学习的本构高斯粒子通过分数蒸馏采样SDS进行优化。</p><blockquote><p>本构模型（Constitutive Models）（Gonzalez &amp; Stuart, 2008；Jiang等，2016）是描述材料在不同机械条件下响应的物理模型，提供了两个物理量（如应力和应变）之间的关系。这些模型通常基于专家对材料机制的观察推导而来，是决定不同材料动态的关键因素。为了增强动态合成的灵活性和自动化程度，关键在于推广本构模型，而不是像以往的工作（Xie等，2024；Zhang等，2024b；Liu等，2024；Huang等，2024）那样手动设置本构模型。</p></blockquote><h3 id="method-24" tabindex="-1">method <a class="header-anchor" href="#method-24" aria-label="Permalink to &quot;method&quot;">​</a></h3><h4 id="_3-2-1-constitutive-gaussian-overview" tabindex="-1">3.2.1 constitutive gaussian (overview) <a class="header-anchor" href="#_3-2-1-constitutive-gaussian-overview" aria-label="Permalink to &quot;3.2.1 constitutive gaussian (overview)&quot;">​</a></h4><figure><img src="/ayene-no-blog/assets/image-20250415152701597.Cxwvz_P0.png" alt="image-20250415152701597" loading="lazy" decoding="async"></figure><p>通过mpm模拟场景，渲染出视频片段，与diffusion的模型用SDS优化。</p><p>可训练的参数是本构高斯，即材质属性，同一场景不同材质的物体可训练参数不一样。</p><p>具体来说，有三个可训练参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mrow><mi>e</mi><mi>l</mi></mrow></msub><mo separator="true">,</mo><msub><mi>θ</mi><mrow><mi>p</mi><mi>l</mi></mrow></msub><mo separator="true">,</mo><msub><mi>θ</mi><mrow><mi>p</mi><mi>h</mi><mi>y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\theta_{el}, \theta_{pl}, \theta_{phy}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9805em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.0278em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:.01968em">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.0278em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.01968em">pl</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.0278em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">h</span><span class="mord mathnormal mtight" style="margin-right:.03588em">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>，分别作为超弹性能量密度函数（Hyperelastic Energy Density Function）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ψ</mi></mrow><annotation encoding="application/x-tex">\psi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.03588em">ψ</span></span></span></span>, 塑性返回函数（Plasticity Return Function）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ψ</mi></mrow><annotation encoding="application/x-tex">\Psi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord">Ψ</span></span></span></span> ，以及物理属性<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal">λ</span></span></span></span></p><p>本文的本构模型，指的就是不同的材质，实际上会有不同的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ψ</mi><mo separator="true">,</mo><mi mathvariant="normal">Ψ</mi></mrow><annotation encoding="application/x-tex">\psi, \Psi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.03588em">ψ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">Ψ</span></span></span></span> ,这些函数的排列组合，形成了不同的材质。</p><h5 id="物理属性-lambda" tabindex="-1">物理属性<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal">λ</span></span></span></span> <a class="header-anchor" href="#物理属性-lambda" aria-label="Permalink to &quot;物理属性$\lambda$&quot;">​</a></h5><p>其中，物理属性在所有本构模型中是相同的，只是用与不用的差别，物理属性<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal">λ</span></span></span></span>包含下述4种（但不是选一种使用，而是都使用）：</p><figure><img src="/ayene-no-blog/assets/image-20250415165641745.5A_gzyBP.png" alt="image-20250415165641745" loading="lazy" decoding="async"></figure><h5 id="超弹性能量密度函数-psi" tabindex="-1">超弹性能量密度函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ψ</mi></mrow><annotation encoding="application/x-tex">\psi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.03588em">ψ</span></span></span></span> <a class="header-anchor" href="#超弹性能量密度函数-psi" aria-label="Permalink to &quot;超弹性能量密度函数$\psi$&quot;">​</a></h5><p>用来计算cauchy应力张量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">σ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span></span></span></span>，包括下述3种，选择一种使用：</p><figure><img src="/ayene-no-blog/assets/image-20250415170301142.BiFvUi_Q.png" alt="image-20250415170301142" loading="lazy" decoding="async"></figure><h5 id="塑性返回函数-psi" tabindex="-1">塑性返回函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ψ</mi></mrow><annotation encoding="application/x-tex">\Psi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord">Ψ</span></span></span></span> <a class="header-anchor" href="#塑性返回函数-psi" aria-label="Permalink to &quot;塑性返回函数$\Psi$&quot;">​</a></h5><p>用来计算修正的变形梯度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span></span></span></span>包括下述4种，选择1种使用</p><figure><img src="/ayene-no-blog/assets/image-20250415170711621.0u8fipnn.png" alt="image-20250415170711621" loading="lazy" decoding="async"></figure><p>因此就可以组合出3×4×1=12种材质，那么要如何选择对应的函数呢？</p><h4 id="_3-2-2-physics-guided-constitutive-network" tabindex="-1">3.2.2 physics guided constitutive network <a class="header-anchor" href="#_3-2-2-physics-guided-constitutive-network" aria-label="Permalink to &quot;3.2.2 physics guided constitutive network&quot;">​</a></h4><h5 id="encoder" tabindex="-1">encoder <a class="header-anchor" href="#encoder" aria-label="Permalink to &quot;encoder&quot;">​</a></h5><p>首先，将训练好的GS进行特征提取，这边用到的是类似pointBert的3D主干结构，特征编码器首先利用最远点采样（Farthest Point Sampling, FPS）和k近邻（kNN）算法将粒子划分为一组局部邻域，然后通过一个小型PointNet将每个邻域内的高斯信息（包括位置、协方差矩阵、球谐函数和透明度）编码为特征向量</p><h5 id="decoder" tabindex="-1">decoder <a class="header-anchor" href="#decoder" aria-label="Permalink to &quot;decoder&quot;">​</a></h5><p>用一个MLP，输入encoder得到的某一邻域的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:-.1076em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>, 输出就是该领域所用的本构模型（MLP+hardmax+STE），即所用的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ψ</mi><mtext>和</mtext><mi mathvariant="normal">Ψ</mi></mrow><annotation encoding="application/x-tex">\psi和\Psi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.03588em">ψ</span><span class="mord cjk_fallback">和</span><span class="mord">Ψ</span></span></span></span>，用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mtext>和</mtext><mi>k</mi></mrow><annotation encoding="application/x-tex">j和k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mord cjk_fallback">和</span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span>表示</p><figure><img src="/ayene-no-blog/assets/image-20250415194930333.CU3_iyc0.png" alt="image-20250415194930333" loading="lazy" decoding="async"></figure><h2 id="pixie-fast-and-generalizable-supervised-learning-of-3d-physics-from-pixels" tabindex="-1">Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels <a class="header-anchor" href="#pixie-fast-and-generalizable-supervised-learning-of-3d-physics-from-pixels" aria-label="Permalink to &quot;Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels&quot;">​</a></h2><h3 id="motivation-23" tabindex="-1">motivation <a class="header-anchor" href="#motivation-23" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>过去的物理参数可以分为两种</p><ul><li>仅物理模拟（需要手动标注参数&amp;材质）：PhysGaussian、Physically Compatible 3D Object Modeling from a Single Image</li><li>视频直接作为监督、或蒸馏：pac-nerf、<strong>Dreamphysics</strong>、omniphysgs</li></ul><p>提出：从稀疏信号（单张渲染图像or蒸馏loss）提取百万粒子的物理参数优化缓慢且无法泛化，每个场景都要从头优化。</p><p>比较核心的贡献：</p><ul><li>提供了一个物理材质标注的数据集</li><li>提出了新的管线，精度更高，速度更快，<strong>可零样本泛化</strong></li></ul><h3 id="method-25" tabindex="-1">method <a class="header-anchor" href="#method-25" aria-label="Permalink to &quot;method&quot;">​</a></h3><p>提出一个观点：三维视觉外观（多视角物体的纹理、阴影、形状）与物理属性存在内在关联。即</p><p>三维视觉外观-&gt;物理属性</p><p>而-&gt;的过程怎么做？神经网络</p><p>对上述过程做一个形式化描述，多视角图像<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">I</span></span></span></span>,对应的相机参数Π，能够映射到一个连续三维材质场M。</p><figure><img src="data:image/png;base64,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" alt="image-20250921160512514" loading="lazy" decoding="async"></figure><p>材质场M可定义为<img src="/ayene-no-blog/assets/image-20250921160556703.BRp-tEkH.png" alt="image-20250921160556703"></p><p>即场景中每个点的（材质类别、杨氏模量、泊松比、密度值）</p><hr><p>从图片到体素化特征</p><p>但是直接这样做，gap还是太大了，因此作者希望</p><ol><li>引入视觉先验（正如之前的文章认为difussion通过大量的真实世界物理视频已经学习到了物理先验一样），作者这里引入了预训练的CLIP作为中间表示</li><li>从2D直接到3D材质场有一定跨度，因此采用NeRF方法将CLIP Feature提升到3D空间。（NeRF从图片-&gt;辐射场，模拟视觉，这边将监督换成CLIP特征即可）</li></ol><blockquote><p>这里其实有个方法上的差异，构建的是材质场，因此要将feature embedding进三维空间</p><p>要不试试嵌入gaussian中</p></blockquote><p>现在，空间中有一个特征辐射场了，为了能够输入神经网络，将辐射场进行体素化，变成一个N * N * N * D的规范网格，D是CLIP特征维度。这个规范网格就是很标准的神经网络输入了</p><hr><p>从体素化特征到体素化物理参数</p><p>从一个维度到相同的维度，会让人想起什么，语义分割？风格迁移？无论哪种领域，unet都会出现。</p><p>在实验中，作者发现了一些需要解决的事情</p><ul><li>维度太高了，本来就是三维，因此需要特征降维，相当于一次预处理，这边采用了卷积神经网络。</li><li>体素太稀疏了，空间中大部分其实是空气，在NeRF中就是透明度&lt;0.01，这边做一个mask。</li></ul><p>这里就是本篇文章里需要训练的神经网络了，一个unet，从体素化的CLIP特征，映射到体素化的物理属性，需要物理属性做约束。</p><hr><p>最后是应用，其实物理参数已经预测好了，这里用最近邻插值将网格的物理参数映射到高斯上，然后做了个mpm模拟</p><h3 id="experiments-15" tabindex="-1">experiments <a class="header-anchor" href="#experiments-15" aria-label="Permalink to &quot;experiments&quot;">​</a></h3><p>这边评估指标比较特别，采用的是gpt作为评估器，让gpt打一个分；当然还有定量指标PSNR、SSIM，数据集采用本文提出来的PIXIEVERSE数据集</p><p>数据标注流程</p><ol><li><p>从Objaverse（一个有很多3D物体的数据集）中选择候选asset，通过关键词和asset title的余弦相似度检索topk=500的，举例现在要搜索500个tree，就用关键词(tree | ficus | fern…)检索资产库。</p><p>检索出来后，先用Gemini过滤掉低质量（曝光明显 过于dark 严重遮挡等等），再人工进行过滤明显不符合要求的资产。</p></li></ol><figure><img src="/ayene-no-blog/assets/image-20251029191953366.Gsgf0Lhz.png" alt="image-20251029191953366" loading="lazy" decoding="async"></figure><ol start="2"><li><p>对所有剩下的资产（1624个）进行CLIP监督的NeRF训练，构建一个CLIP特征场。让VLM生成一个候选部件词，分别用这些词做3D语义分割。举例，VLM看见一棵树后，生成三个候选词{leves, pot, trunk},然后用余弦相似度检索CLIP特征场中相似度大于阈值的点，将该候选词分配给这些点。</p><p>为了精确度会生成好几组候选词，然后让另一个VLM做1~10分的评判，选择最高分的候选词。</p><figure><img src="/ayene-no-blog/assets/image-20251029192534286.CvrZD7bl.png" alt="image-20251029192534286" loading="lazy" decoding="async"></figure></li><li><p>VLM在提供每个asset的部件候选词时，也会提供该部件的物理参数，所以输入输出大概是这样的，输入NeRF渲染的5张图片，被要求输出他们的部件候选词，以及每个部件可能的物理参数。</p><figure><img src="/ayene-no-blog/assets/image-20251029193623729.004p23N7.png" alt="image-20251029193623729" loading="lazy" decoding="async"></figure><p>也是为了增加物理参数估计的准确性，提供了每种类别的参数示例：</p><figure><img src="/ayene-no-blog/assets/image-20251029193752475.CH5OwFIF.png" alt="image-20251029193752475" loading="lazy" decoding="async"></figure></li></ol><h2 id="gaussianproperty-integrating-physical-properties-to-3d-gaussians-with-lmms" tabindex="-1">GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs <a class="header-anchor" href="#gaussianproperty-integrating-physical-properties-to-3d-gaussians-with-lmms" aria-label="Permalink to &quot;GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs&quot;">​</a></h2><h3 id="motivation-24" tabindex="-1">motivation <a class="header-anchor" href="#motivation-24" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><p>没太讲清楚为什么提，以前的方法为什么不好；说明了方法的pipeline是先2D分割，GPT-4V来估计物理参数，通过投影和投票得到物理参数，最后以机器人实验验证。</p><h3 id="method-26" tabindex="-1">method <a class="header-anchor" href="#method-26" aria-label="Permalink to &quot;method&quot;">​</a></h3><figure><img src="/ayene-no-blog/assets/image-20251029174918854.CnjI6Z6l.png" alt="image-20251029174918854" loading="lazy" decoding="async"></figure><p>看pipeline就可以了，</p><p>在2D上：其实就是先SAM进行部件级分割，然后对每个部件使用LLM进行物理属性预测。</p><p>然后通过反投影+投票机制投影到3DGS上。</p><p>几个细节</p><ol><li><p>预测的时候采用的是整个物体的图片+mask，这样可以有整个物体的信息的同时预测部件的参数</p><img src="/ayene-no-blog/assets/image-20251029175039905.BVnaoL1x.png" alt="image-20251029175039905" style="zoom:50%"></li><li><p>为了更精确的估计材质，提出了一个候选材料库，包含常见的15种材料和600个case，让LLM限定在这些材料的范围内进行选择，以提高输出的稳定性。</p></li></ol><h2 id="phys4dgen-physics-compliant-4d-generation-with-multi-material-composition-perception-acm-mm-2025" tabindex="-1">Phys4DGen: Physics-Compliant 4D Generation with Multi-Material Composition Perception（ACM MM 2025） <a class="header-anchor" href="#phys4dgen-physics-compliant-4d-generation-with-multi-material-composition-perception-acm-mm-2025" aria-label="Permalink to &quot;Phys4DGen: Physics-Compliant 4D Generation with Multi-Material Composition Perception（ACM MM 2025）&quot;">​</a></h2><h3 id="motivation-25" tabindex="-1">motivation <a class="header-anchor" href="#motivation-25" aria-label="Permalink to &quot;motivation&quot;">​</a></h3><ol><li>现有文章把物体当做均匀实体，<strong>拥有单一材质</strong>，因此具有不同部件的物体表现出相同的材质，因而物理不真实。</li></ol><blockquote><p>其实已经有文章，比如PIXIE和GaussianProperty已经发现了这点，并且在物理材质估计之前做了部件级分割，所以这点其实不是很成立</p></blockquote><ol start="2"><li><p>3DGS<strong>缺乏对内部结构</strong>的建模能力，但是物体内部可能有多种材料甚至和表面材质不同 。这会使得物体在大形变下容易结构崩塌。</p></li><li><p>现有方法比较依赖于初始化的值，10^4和10^6的杨氏模量的初始化可能导致完全不一样的结果（如左图），因此认为现有方法<strong>缺乏物理先验</strong></p></li></ol><p>提出解决</p><ol><li>部件级3D分割。</li><li>提出内部物理结构发现模块，对物体内部进行建模</li><li>LLM识别表面和内部材质属性</li></ol><h3 id="method-27" tabindex="-1">Method <a class="header-anchor" href="#method-27" aria-label="Permalink to &quot;Method&quot;">​</a></h3><ol><li><p>部件级3D分割。几乎和我的方法一模一样</p><p>使用SAM2进行视频追踪得到帧间一致的id，修改CUDA确定表面深度，基于深度进行id反投影，投过投票机制确定每个GS最终的id。</p></li></ol><img src="/ayene-no-blog/assets/image-20251126183446784.Obw_eKej.png" alt="image-20251126183446784" style="zoom:80%"><ol start="2"><li><p>内部物理结构发现模块，其实就是粒子填充，具体做法：先生成高斯模型的BBX，然后对BBX的空间均匀采样生成粒子。</p><p>现在可以将粒子分为</p><p><strong>外部粒子</strong>：凡是深度在表面深度以外的，直接去除</p><p><strong>表面粒子</strong>：被表面GS包含的</p><p><strong>内部粒子</strong>：上述粒子的补集</p><p>确认表面粒子和内部粒子后，还要将高斯上的物理属性分配上去，表面粒子则由最近邻的GS粒子分配，内部粒子则通过6个主轴发射射线，通过触碰到的表面高斯的材质加权分配。</p></li><li><p>LLM物理属性增强。</p><figure><img src="/ayene-no-blog/assets/image-20251126194656109.DlyuOz3m.png" alt="image-20251126194656109" loading="lazy" decoding="async"></figure><p>从之前分割图像中，把物体和物体的部件级分割给LLM，提示其输出不同部件的物理参数。</p></li></ol><p>现在有了3D分割分组，有了2D SAM-&gt;LLM的物理属性，接下来是做一个分配，本篇是分别渲染成2D图，转换为CLIP特征做余弦相似度匹配。</p><blockquote><p>为什么不直接反投影，3DGS分组不就是SAM得到的嘛</p></blockquote><!--]--><!--]--><!----><!----></article><!-- </Transition> --><!--]--><!--[--><!--[--><!--[--><!----><ul class="post-copyright" m="y-4"><li class="post-copyright-author"><strong>本文作者：</strong><span>水沢绫音</span></li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://pat-chou-li.github.io/ayene-no-blog/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0" target="_blank" title="本文链接">https://pat-chou-li.github.io/ayene-no-blog/posts/论文阅读笔记</a></li><li class="post-copyright-license"><strong>版权声明：</strong><span>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 ">CC BY-NC-SA</a> 许可协议。</span></li></ul><!--]--><!--]--><!--]--></div><!--]--><!----></div><!--[--><!--]--><!--[--><div class="post-nav"><div class="post-nav-item"><a href="/ayene-no-blog/posts/%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95" class="post-nav-prev" title="实验记录"><div class="icon" i-ri-arrow-left-s-line></div><span class="title truncate" text="sm">实验记录</span></a></div><div class="post-nav-item"><a href="/ayene-no-blog/posts/nerf/AI葵/nerf与它的快乐伙伴们" class="post-nav-next" title="nerf和他的快乐伙伴们（AI葵）"><span class="title truncate" text="sm">nerf和他的快乐伙伴们（AI葵）</span><div class="icon" i-ri-arrow-right-s-line></div></a></div></div><!--]--><!--[--><!--]--><!----><!--[--><!-- eslint-disable-next-line vue/no-lone-template empty --><template class="mt-4"></template><!--]--><!--[--><!--]--><!--[--><!--]--></div><!--]--></main><!--[--><button class="xl:hidden toc-btn shadow-md fixed yun-icon-btn z-20 bg-$va-c-bg-soft" opacity="75" right="4" bottom="19"><div i-ri-file-list-line></div></button><!----><aside flex="~ col" class="va-card yun-aside sticky top-0 lg:top-$yun-margin-top min-h-sm" text="center" overflow="auto"><div class="w-full" flex="~ col" pb-2 style="display:none"><!--[--><h2 m="t-6 b-2" font="serif black">文章目录</h2><div style="display:none" data-v-6dc67c73><div class="content" data-v-6dc67c73><div class="outline-title" data-v-6dc67c73>本页</div><div class="outline-marker" data-v-6dc67c73></div><nav aria-labelledby="doc-outline-aria-label" data-v-6dc67c73><span id="doc-outline-aria-label" class="visually-hidden" data-v-6dc67c73>Table of Contents for current page</span><ul class="root va-toc relative z-1 css-i18n-toc" data-v-6dc67c73 data-v-699db71a><!--[--><!--]--></ul></nav></div></div><!--]--><div class="flex-grow"></div><!----></div></aside><!--]--><!--]--></div><footer flex="~ col" class="relative yun-footer va-footer px-4 py-4 pt-0 text-$va-c-text-light w-full mt-14" bg="white dark:$va-c-bg-soft" text="center sm"><div class="yun-cloud absolute top--10 left-0 right-0"><svg class="waves" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z" fill="var(--yun-c-cloud)"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><!----><div class="copyright flex justify-center items-center gap-2" p="1"><span>©<!--[--> 2022 -<!--]--> 2025</span><a class="animate-pulse inline-flex" href="https://www.yunyoujun.cn/sponsors/" target="_blank" title="Sponsor YunYouJun"><div class="i-ri-cloud-line"></div></a><span>水沢绫音</span></div><div class="powered" m="2"><span>由 <a href="git+https://github.com/YunYouJun/valaxy.git" target="_blank" rel="noopener">Valaxy</a> <span class="op-60">v0.26.10</span> 驱动</span><span mx-1>|</span><span><span>主题</span><span mx-1>-</span><a href="git+https://github.com/YunYouJun/valaxy/tree/main/packages/valaxy-theme-yun.git" title="valaxy-theme-yun" target="_blank">Yun</a><span class="ml-1 op-60">v0.26.10</span></span></div><!--[--><!--]--><div class="yun-footer-gradient" style="--gradient-from:161 196 253;--gradient-to:194 233 251"></div></footer><!--]--><!--]--></div><script>window.__INITIAL_STATE__='{"pinia":{"yun-app":{},"app":{"showLoading":true},"site":{},"routerStore":{}}}'</script><script type="application/ld+json" data-hid="schema-org-graph">{"@context":"https://schema.org","@graph":[{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#identity","@type":"Person","name":"水沢绫音","url":"https://pat-chou-li.github.io/ayene-no-blog/","image":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#/schema/image/658d95f"},"sameAs":["/atom.xml","https://github.com/pat-chou-li","https://www.zhihu.com/people/shui-ze-ling-yin-46","https://space.bilibili.com/8929945?spm_id_from=333.1007.0.0","patchouli13@163.com"]},{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#website","@type":"WebSite","dateModified":"2023.11.1","datePublished":"2023.11.1","inLanguage":"en","name":"论文阅读笔记，各种各样的","url":"https://pat-chou-li.github.io/ayene-no-blog/","publisher":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#identity"}},{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#webpage","@type":"WebPage","dateModified":"2023-11-01T00:00:00.000Z","datePublished":"2023-11-01T00:00:00.000Z","description":"我从来没有觉得学图形学开心过。","name":"论文阅读笔记，各种各样的","url":"https://pat-chou-li.github.io/ayene-no-blog/","about":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#identity"},"isPartOf":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#website"},"potentialAction":[{"@type":"ReadAction","target":["https://pat-chou-li.github.io/ayene-no-blog/"]}]},{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#article","dateModified":"2023-11-01T00:00:00.000Z","datePublished":"2023-11-01T00:00:00.000Z","description":"我从来没有觉得学图形学开心过。","headline":"论文阅读笔记，各种各样的","inLanguage":"en","thumbnailUrl":"https://pat-chou-li.github.io/ayene-no-blog/favicon.svg","@type":["Article","BlogPosting"],"author":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#/schema/person/6678b6b"},"image":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#/schema/image/19d7713"},"isPartOf":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#webpage"},"mainEntityOfPage":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#webpage"},"publisher":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#identity"}},{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#/schema/person/6678b6b","@type":"Person","name":"水沢绫音","url":"https://valaxy.site"},{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#/schema/image/658d95f","@type":"ImageObject","contentUrl":"https://raw.githubusercontent.com/pat-chou-li/ayene-no-blog/main/resource/avatar.jpg","inLanguage":"en","url":"https://raw.githubusercontent.com/pat-chou-li/ayene-no-blog/main/resource/avatar.jpg"},{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#/schema/image/19d7713","@type":"ImageObject","contentUrl":"https://pat-chou-li.github.io/ayene-no-blog/favicon.svg","inLanguage":"en","url":"https://pat-chou-li.github.io/ayene-no-blog/favicon.svg"}]}</script></body></html>