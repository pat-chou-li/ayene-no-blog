<!DOCTYPE html><html lang="en" data-beasties-container><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="/favicon.svg"><script>!function(){const e=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,t=localStorage.getItem("vueuse-color-scheme")||"auto";("dark"===t||e&&"light"!==t)&&document.documentElement.classList.toggle("dark",!0)}()</script><style type="text/css">:root{color-scheme:light dark;--va-c-bg:#fff}html{background-color:var(--va-c-bg)}</style><script>const locale=localStorage.getItem("valaxy-locale")||"zh-CN";document.documentElement.setAttribute("lang",locale)</script><script type="module" async crossorigin src="/ayene-no-blog/assets/app.5Pc5wqE0.js"></script><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/framework.BWK1Zvdg.js"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/chunks/@vueuse/motion.CDGXpttC.js"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/chunks/vue-router.DlUPz4t8.js"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/chunks/dayjs.BldX5ftQ.js"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/chunks/vue-i18n.lF0SUo7_.js"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/chunks/pinia.BYsrTFz1.js"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/chunks/nprogress.DxEoPAls.js"><link rel="stylesheet" crossorigin href="/ayene-no-blog/assets/app.B3NQSKti.css"><link rel="stylesheet" crossorigin href="/ayene-no-blog/assets/group-icons.CNuSYZDE.css"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/post.BIZmRJsi.js"><link rel="stylesheet" href="/ayene-no-blog/assets/group-icons.CNuSYZDE.css"><link rel="modulepreload" crossorigin href="/ayene-no-blog/assets/（AI葵）cuda教学.DPH8AtXi.js"><title>（AI葵）cuda教学 - ayane no blog</title><script id="check-mac-os" async>document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap"><meta property="og:locale:alternate" content="en"><meta name="description" content="我从来没有觉得学图形学开心过。"><meta property="og:description" content="我从来没有觉得学图形学开心过。"><meta property="og:locale" content="zh-CN"><meta property="og:site_name" content="ayane no blog"><meta property="og:title" content="（AI葵）cuda教学"><meta property="og:image" content="/favicon.svg"><meta property="og:type" content="website"><meta property="og:url" content="https://pat-chou-li.github.io/ayene-no-blog/"><link rel="icon" href="/favicon.svg" type="image/svg+xml"><meta name="generator" content="Valaxy 0.26.3"><meta name="theme-color" content="#fff"><meta name="msapplication-TileColor" content="#fff"><meta name="twitter:card" content="summary_large_image"></head><body><div id="app" data-server-rendered="true"><!--[--><!--[--><!--[--><!----><!----><div class="yun-page-header-gradient" style="--gradient-from:161 196 253;--gradient-to:194 233 251"></div><!----><!----><!----><canvas class="fireworks"></canvas><!--[--><div class="yun-bg"></div><!--]--><div class="yun-page-loading" absolute left-0 right-0 bottom-0 top-0 flex justify="center" items-center z-10 bg="$va-c-bg" data-v-673bc094><div class="spinner" data-v-673bc094></div></div><a href="#" class="back-to-top yun-icon-btn bg-$va-c-bg-soft shadow-md"><div class="size-8" i-ri-arrow-up-s-line></div><svg class="progress-circle-container" viewBox="0 0 100 100"><circle stroke-dasharray="301.59289474462014 301.59289474462014" stroke-dashoffset="301.59289474462014" stroke="currentColor" stroke-width="2" stroke-linecap="round" class="progress-circle" cx="50" cy="50" r="48" fill="none"/></svg></a><!-- TODO --><!-- <YunDock /> --><!--]--><!--[--><!--]--><!----><!--]--><!--[--><div flex="~" class="w-full m-auto justify-center items-start gap-4 mt-12 md:mt-24"><!--[--><!----><main class="yun-main lt-md:w-full" flex="~ center"><!--[--><div class="content w-full md:w-3xl lg:w-2xl xl:w-2xl 2xl:w-4xl" flex="~ col grow" p="lt-md:0"><div class="yun-card flex-center rounded-2 relative" flex="col" min-h="100px" bg="$va-c-bg-light" m="0" style><!----><!----><!--[--><div class="mt-8 mb-4"><!--[--><header class="post-header"><h1 p="2" text="2xl center" font="serif black" style="" class="post-title flex-center"><!----><span inline-flex class="leading-none">（AI葵）cuda教学</span></h1></header><!--]--></div><!--[--><!--[--><!--[--><!--[--><!----><!----><!----><div flex="~ center" text="sm" class="flex-col gap-2! post-meta gap-4"><div class="post-time flex items-center gap-4"><span class="posted-time inline-flex-center gap-1" title="发表于2024-03-03 00:00:00"><div class="inline-block" i-ri-calendar-line></div><time class="op-80">2024-03-03</time></span><span class="edited-time inline-flex-center gap-1" title="更新于2025-08-22 09:02:38"><div i-ri-calendar-2-line></div><time class="op-80">2025-08-22</time></span></div><!--[--><!--]--><div class="inline-flex-center gap-4"><!----><!----></div></div><!--]--><div class="inline-flex mt-2" text="sm" py="1"><a href="/ayene-no-blog/categories?category=CUDA/CPP" class="transition post-category inline-flex-center text-xs border-$va-c-divider" px-2 h="7" border rounded-full hover="bg-blue-500 text-white"><div m="x-1" inline-flex i-ri-folder-2-line></div><span>CUDA/CPP</span></a><span mx="2"></span><div class="post-tags inline-flex" items="center" gap="1" flex="wrap 1" justify="end"><!--[--><a href="/ayene-no-blog/tags/?tag=%E5%9B%BE%E5%BD%A2%E5%AD%A6" class="transition post-tag inline-flex-center text-xs border-$va-c-divider" px-2 h="7" rounded-full border hover="bg-blue-500 text-white"><span>图形学</span></a><a href="/ayene-no-blog/tags/?tag=NeRF" class="transition post-tag inline-flex-center text-xs border-$va-c-divider" px-2 h="7" rounded-full border hover="bg-blue-500 text-white"><span>NeRF</span></a><a href="/ayene-no-blog/tags/?tag=3DGS" class="transition post-tag inline-flex-center text-xs border-$va-c-divider" px-2 h="7" rounded-full border hover="bg-blue-500 text-white"><span>3DGS</span></a><!--]--></div></div><!--]--><!--]--><!--]--><div p="x-4 b-8" class="sm:px-6 lg:px-12 xl:px-16" w="full"><!--[--><!--]--><!--[--><!-- <Transition appear> --><article class="markdown-body"><!--[--><!----><!----><!--[--><!--]--><!--[--><h2 id="tutorial-1-setup" tabindex="-1">Tutorial 1 setup <a class="header-anchor" href="#tutorial-1-setup" aria-label="Permalink to &quot;Tutorial 1 setup&quot;">​</a></h2><ul><li><p>使用方法：在python中调用cpp，在cpp中调用cuda</p></li><li><p>哪些时候要用到CUDA，pytorch无法自动实现？</p><ul><li><p>非平行运算，例如体渲染，采样点不一样</p></li><li><p>大量的串行运算</p><img src="/ayene-no-blog/assets/image-20240303133950997.jJ9o6vj3.png" alt="image-20240303133950997" style="zoom:50%"></li></ul></li></ul><p>windos下cpp中添加torch头文件</p><blockquote><p>"C:/Users/Patchouli/anaconda3/envs/cppcuda/include",</p><p>"C:/Users/Patchouli/anaconda3/envs/cppcuda/Lib/site-packages/torch/include",</p><p>"C:/Users/Patchouli/anaconda3/envs/cppcuda/Lib/site-packages/torch/include/torch/csrc/api/include"</p></blockquote><p>pybind：从python中调用cpp</p><div class="language-"><button title="Copy code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span>PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){</span></span>
<span class="line"><span>    m.def("_trilinear_interpolation", &amp;_trilinear_interpolation) </span></span>
<span class="line"><span>    #前者定义python中的函数名称，后者是cpp中调用的函数</span></span>
<span class="line"><span>}</span></span></code></pre><button class="code-block-unfold-btn"></button></div><p>CPP文件需要编译才能运行，这里由python进行注册，在setup.py文件中</p><div class="language-"><button title="Copy code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span>import glob</span></span>
<span class="line"><span>import os.path as osp</span></span>
<span class="line"><span>from setuptools import setup</span></span>
<span class="line"><span>from torch.utils.cpp_extension import CUDAExtension, BuildExtension</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>ROOT_DIR = osp.dirname(osp.abspath(__file__))</span></span>
<span class="line"><span>include_dirs = [osp.join(ROOT_DIR, "include")]</span></span>
<span class="line"><span></span></span>
<span class="line"><span>sources = glob.glob('*.cpp')+glob.glob('*.cu')</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>setup(</span></span>
<span class="line"><span>    name='cppcuda_tutorial', #import时的名称</span></span>
<span class="line"><span>    version='1.0',</span></span>
<span class="line"><span>    author='kwea123',</span></span>
<span class="line"><span>    author_email='kwea123@gmail.com',</span></span>
<span class="line"><span>    description='cppcuda_tutorial',</span></span>
<span class="line"><span>    long_description='cppcuda_tutorial',</span></span>
<span class="line"><span>    ext_modules=[</span></span>
<span class="line"><span>        CUDAExtension(</span></span>
<span class="line"><span>            name='cppcuda_tutorial',</span></span>
<span class="line"><span>            sources=sources,        # 有哪些cpp/cu文件需要build</span></span>
<span class="line"><span>            include_dirs=include_dirs,</span></span>
<span class="line"><span>            extra_compile_args={'cxx': ['-O2'],</span></span>
<span class="line"><span>                                'nvcc': ['-O2']}</span></span>
<span class="line"><span>        )</span></span>
<span class="line"><span>    ],</span></span>
<span class="line"><span>    cmdclass={</span></span>
<span class="line"><span>        'build_ext': BuildExtension</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span>)</span></span></code></pre><button class="code-block-unfold-btn"></button></div><p>写好setup.python后，执行以下代码编译cpp</p><blockquote><p>python setup.py install</p></blockquote><p>在python中就可以使用cpp代码了</p><blockquote><p>import torch</p><p>import cppcuda_tutorial</p><p>feats = torch.ones(2)</p><p>points = torch.zeros(2)</p><p>out = cppcuda_tutorial._trilinear_interpolation(feats, points)</p><p>print(out)</p></blockquote><h2 id="tutorial-2-input-check" tabindex="-1">Tutorial 2 input check <a class="header-anchor" href="#tutorial-2-input-check" aria-label="Permalink to &quot;Tutorial 2 input check&quot;">​</a></h2><p>grid-&gt;block-&gt;thread</p><p>三线性插值中有哪些能平行运算？</p><div class="language-"><button title="Copy code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span>#include &lt;torch/extension.h&gt;</span></span>
<span class="line"><span>torch::Tensor _trilinear_interpolation(</span></span>
<span class="line"><span>    torch::Tensor feats, //(N, 8, F) N: 正方体数量， 8个顶点， F个feature</span></span>
<span class="line"><span>    torch::Tensor point  //(N, 3) 三维坐标</span></span>
<span class="line"><span>){</span></span>
<span class="line"><span>    return feats;</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span></span></span>
<span class="line"><span>PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){</span></span>
<span class="line"><span>    m.def("_trilinear_interpolation", &amp;_trilinear_interpolation);</span></span>
<span class="line"><span>}</span></span></code></pre><button class="code-block-unfold-btn"></button></div><p>N个正方体之间可以平行</p><p>F个feature之间可以平行</p><blockquote><p>8个顶点不也能平行运算吗</p></blockquote><p>在调用cuda，将tensor传入前，必须做的检测</p><div class="language-cpp"><button title="Copy code" class="copy"></button><span class="lang">cpp</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">    CHECK_INPUT</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(feats);</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">    CHECK_INPUT</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(points);</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">#define</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> CHECK_CUDA</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">x</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">) </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">TORCH_CHECK</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(x.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">is_cuda</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(), #x </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">" must be a CUDA tensor"</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">#define</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> CHECK_CONTIGUOUS</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">x</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">) </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">TORCH_CHECK</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(x.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">is_contiguous</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(), #x </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">" must be contiguous"</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">#define</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> CHECK_INPUT</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">x</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">) </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">CHECK_CUDA</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(x); </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">CHECK_CONTIGUOUS</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(x)</span></span></code></pre><button class="code-block-unfold-btn"></button></div><p>大概就是检测tensor是不是在cuda上，和内存上是否连续，注意floater之类的参数就不要检测了，主要是tensor</p><h2 id="tutorial-3-定义thread" tabindex="-1">Tutorial 3 定义thread <a class="header-anchor" href="#tutorial-3-定义thread" aria-label="Permalink to &quot;Tutorial 3 定义thread&quot;">​</a></h2><img src="/ayene-no-blog/assets/image-20240303160824172.DRg7TDlB.png" alt="image-20240303160824172" style="zoom:50%"><p>一个block中有多少thread由我们自行定义，有几个block一般由一个公式固定计算</p><p>例如有20个立方体，每个立方体要计算10个feature，我们定义的线程中行表示不同立方体，列表示不同feature，用1个含有16*16个thread的block，则需要两个block才能将20*10包含住，2行1列block。</p><div class="language-c++"><button title="Copy code" class="copy"></button><span class="lang">c++</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    // torch.zeros(N, F, dtype=torch.float32, device='cuda:0')</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">    torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::Tensor feat_interp </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">zeros</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">({N, F}, feats.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">options</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">());</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    // torch::zeros({N, F}, torch::dtype(torch::kInt32).device(feats.device));</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> dim3</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> threads</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">16</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">16</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">);</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D"> // N and F平行运算，各16个，共有256个线程</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> dim3</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> blocks</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">((N</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">threads.x</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">/</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">threads</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">.</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">x</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, (F</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">threads.y</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">/</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">threads</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">.</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">y</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">);</span></span></code></pre><button class="code-block-unfold-btn"></button></div><p>接下来要启动一个kernal,让数据在GPU上运算，举例代码如下</p><div class="language-c++"><button title="Copy code" class="copy"></button><span class="lang">c++</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">    AT_DISPATCH_FLOATING_TYPES</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(feats.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(), </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">"trilinear_fw_cu"</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    ([</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">] {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">      </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        trilinear_fw_kernel</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">scalar_t</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&gt;&lt;&lt;&lt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">blocks, threads</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&gt;&gt;&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">            feats.packed_accessor</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::RestrictPtrTraits, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">size_t&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">            points.packed_accessor</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::RestrictPtrTraits, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">size_t&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">            feat_interp.packed_accessor</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::RestrictPtrTraits, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">size_t&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        );</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    }));</span></span></code></pre><button class="code-block-unfold-btn"></button></div><h2 id="tutorial-4-调用kernal" tabindex="-1">Tutorial 4 调用kernal <a class="header-anchor" href="#tutorial-4-调用kernal" aria-label="Permalink to &quot;Tutorial 4 调用kernal&quot;">​</a></h2><p>核函数的写法：</p><p>step1：计算出thread的编号，之后写每个thread要做什么</p><div class="language-c++"><button title="Copy code" class="copy"></button><span class="lang">c++</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">// 二维thread，计算编号    </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">const</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583"> int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> n </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> blockIdx.x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> blockDim.x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> threadIdx.x;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">const</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583"> int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> f </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> blockIdx.y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> blockDim.y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> threadIdx.y;</span></span></code></pre><button class="code-block-unfold-btn"></button></div><p>step2：去除不必要的thread，图中橙色部分就是不必要的thread</p><img src="/ayene-no-blog/assets/image-20240303170057260.DXDhnvVm.png" alt="image-20240303170057260" style="zoom:50%"><div class="language-c++"><button title="Copy code" class="copy"></button><span class="lang">c++</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> (n</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&gt;=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">feats.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">size</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">||</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> f</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&gt;=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">feats.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">size</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">;</span></span></code></pre><button class="code-block-unfold-btn"></button></div><p>最后cuda代码如下：</p><div class="language-c++"><button title="Copy code" class="copy"></button><span class="lang">c++</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">#include</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF"> &lt;torch/extension.h&gt;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">template</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> &lt;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">typename</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">&gt;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">// __global__ ：在CPU上调用，在GPU上执行</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">// __host__ : 在CPU上执行</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">// __device__ : 在GPU上调用，在GPU上执行</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">__global__ </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">void</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> trilinear_fw_kernel</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::PackedTensorAccessor</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::RestrictPtrTraits, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">size_t&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> feats,</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::PackedTensorAccessor</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::RestrictPtrTraits, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">size_t&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> points,</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">    torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::PackedTensorAccessor</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::RestrictPtrTraits, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">size_t&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> feat_interp</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">){</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    // 计算出thread的编号，接下来写每个thread要做的事</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583"> int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> n </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> blockIdx.x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> blockDim.x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> threadIdx.x;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583"> int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> f </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> blockIdx.y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> blockDim.y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> threadIdx.y;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    // 去除不必要的thread</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> (n</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&gt;=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">feats.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">size</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">||</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> f</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&gt;=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">feats.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">size</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    // point -1~1，normalization</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> u </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> (points[n][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">/</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> v </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> (points[n][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">/</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> w </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> (points[n][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">/</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">v)</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">w);</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">v)</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">w;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> c </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> v</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">w);</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> d </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> 1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">a</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">b</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">c;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    feat_interp[n][f] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">u)</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(a</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">feats[n][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">][f] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">                               b</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">feats[n][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">][f] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">                               c</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">feats[n][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">][f] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">                               d</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">feats[n][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">][f]) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">                            u</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(a</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">feats[n][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">][f] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">                               b</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">feats[n][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">][f] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">                               c</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">feats[n][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">][f] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">                               d</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">feats[n][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">7</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">][f]);</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">}</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">Tensor</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> trilinear_fw_cu</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">    torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">Tensor</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70"> feats</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">,</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D"> //(N, 8, F) N: 正方体数量， 8个顶点， F个feature</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">    torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">Tensor</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70"> points</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">  //(N, 3) 三维坐标</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">){</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583"> int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> N </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> feats.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">size</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">), F </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> feats.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">size</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">);</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    // torch.zeros(N, F, dtype=torch.float32, device='cuda:0')</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">    torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::Tensor feat_interp </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">zeros</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">({N, F}, feats.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">options</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">());</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    // torch::zeros({N, F}, torch::dtype(torch::kInt32).device(feats.device));</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> dim3 </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">threads</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">16</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">16</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">);</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D"> // N and F平行运算，各16个，共有256个线程</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    const</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> dim3 </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">blocks</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">((N</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">threads.x</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">threads.x, (F</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">threads.y</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">threads.y);</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    // CPU上调用核函数，进行GPU计算</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">    AT_DISPATCH_FLOATING_TYPES</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(feats.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(), </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">"trilinear_fw_cu"</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    ([</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">] {</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">        // trilinear_fw_kernel : 自定义的kernal函数的名字</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">        // scalar_t : 数据类型</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        trilinear_fw_kernel</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">scalar_t</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&gt;&lt;&lt;&lt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">blocks, threads</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&gt;&gt;&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">            // 要丢进kernal的三个数据，input和output都要丢进去，kernal函数没有回传</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">            // .packed_accessor 将tensor转化为cuda认识的形态</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">            // first parametre  : 数据类型</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">            // second parametre : tensor的维度，比如说feats是（N, 8, F），所以有3维</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">            // thrid parameter  : 指定各个参数内存上没有交集（大概是 用到再查）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">            feats.packed_accessor</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::RestrictPtrTraits, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">size_t&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">            points.packed_accessor</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::RestrictPtrTraits, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">size_t&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">            feat_interp.packed_accessor</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">scalar_t</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">::RestrictPtrTraits, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">size_t&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        );</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    }));</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> feat_interp;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">}</span></span></code></pre><button class="code-block-unfold-btn"></button></div><h2 id="tutorial-5-python验证" tabindex="-1">Tutorial 5 python验证 <a class="header-anchor" href="#tutorial-5-python验证" aria-label="Permalink to &quot;Tutorial 5 python验证&quot;">​</a></h2><p>可以用python写一遍，然后用来对比cuda程序的正确性，以及加速效率</p><div class="language-"><button title="Copy code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span>import torch</span></span>
<span class="line"><span>import cppcuda_tutorial</span></span>
<span class="line"><span>import time</span></span>
<span class="line"><span></span></span>
<span class="line"><span>def trilinear_interpolation_py(feats, points):</span></span>
<span class="line"><span>    """</span></span>
<span class="line"><span>    Inputs:</span></span>
<span class="line"><span>        feats: (N, 8, F)</span></span>
<span class="line"><span>        points: (N, 3) local coordinates in [-1, 1]</span></span>
<span class="line"><span>    </span></span>
<span class="line"><span>    Outputs:</span></span>
<span class="line"><span>        feats_interp: (N, F)</span></span>
<span class="line"><span>    """</span></span>
<span class="line"><span>    u = (points[:, 0:1]+1)/2</span></span>
<span class="line"><span>    v = (points[:, 1:2]+1)/2</span></span>
<span class="line"><span>    w = (points[:, 2:3]+1)/2</span></span>
<span class="line"><span>    a = (1-v)*(1-w)</span></span>
<span class="line"><span>    b = (1-v)*w</span></span>
<span class="line"><span>    c = v*(1-w)</span></span>
<span class="line"><span>    d = 1-a-b-c</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    feats_interp = (1-u)*(a*feats[:, 0] +</span></span>
<span class="line"><span>                          b*feats[:, 1] +</span></span>
<span class="line"><span>                          c*feats[:, 2] +</span></span>
<span class="line"><span>                          d*feats[:, 3]) + \</span></span>
<span class="line"><span>                       u*(a*feats[:, 4] +</span></span>
<span class="line"><span>                          b*feats[:, 5] +</span></span>
<span class="line"><span>                          c*feats[:, 6] +</span></span>
<span class="line"><span>                          d*feats[:, 7])</span></span>
<span class="line"><span>    </span></span>
<span class="line"><span>    return feats_interp</span></span>
<span class="line"><span></span></span>
<span class="line"><span>if __name__ == '__main__':</span></span>
<span class="line"><span>    N = 65536 </span></span>
<span class="line"><span>    F = 256</span></span>
<span class="line"><span>    feats = torch.rand(N, 8, F, device='cuda')</span></span>
<span class="line"><span>    points = torch.rand(N, 3, device='cuda') * 2 - 1</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    t = time.time()</span></span>
<span class="line"><span>    out_cuda = cppcuda_tutorial._trilinear_interpolation(feats, points)</span></span>
<span class="line"><span>    torch.cuda.synchronize()</span></span>
<span class="line"><span>    print('   cuda fw time', time.time()-t, 's')</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    t = time.time()</span></span>
<span class="line"><span>    out_py = trilinear_interpolation_py(feats, points)</span></span>
<span class="line"><span>    torch.cuda.synchronize()</span></span>
<span class="line"><span>    print('pytorch fw time', time.time()-t, 's')</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>    out_cuda = cppcuda_tutorial._trilinear_interpolation(feats, points)</span></span>
<span class="line"><span>    out_py = trilinear_interpolation_py(feats, points)</span></span>
<span class="line"><span>    # 判断是否相等</span></span>
<span class="line"><span>    print(torch.allclose(out_py, out_cuda))</span></span></code></pre><button class="code-block-unfold-btn"></button></div><h2 id="tutorial-6-反向传播" tabindex="-1">Tutorial 6 反向传播 <a class="header-anchor" href="#tutorial-6-反向传播" aria-label="Permalink to &quot;Tutorial 6 反向传播&quot;">​</a></h2><p>检查能否自动计算梯度</p><div class="language-python"><button title="Copy code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">torch.requires_grad(out_cuda)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">&gt;&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> false</span></span></code></pre><button class="code-block-unfold-btn"></button></div><p>因此我们需要手写反向传播</p><p>step1：计算所有<strong>output</strong>对所有<strong>trainable input</strong>的偏微分</p><p>step2： 用torch.autograd.Function包装fw及bw处理</p><div class="language-c++"><button title="Copy code" class="copy"></button><span class="lang">c++</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> Trilinear_interpolation_cuda</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">torch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">autograd</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">.</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">Function</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    @staticmethod</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">ctx</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">feats</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">points</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        feat_interp </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> cppcuda_tutorial.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">trilinear_interpolation_fw</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(feats, points)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        ctx.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">save_for_backward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(feats, points)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> feat_interp</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    # 前向传播后，dL_dfeat_interp已知，其他需要手写反向传播</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    @staticmethod</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0"> backward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">ctx</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">dL_dfeat_interp</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        feats, points </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> ctx.saved_tensors</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        dL_dfeats </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> cppcuda_tutorial.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">trilinear_interpolation_bw</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(dL_dfeat_interp.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">contiguous</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(), feats, points)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> dL_dfeats, None</span></span></code></pre><button class="code-block-unfold-btn"></button></div><p>写好的时候，就用apply方法，就可以保存梯度了</p><div class="language-"><button title="Copy code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code"><code v-pre><span class="line"><span>out_cuda = Trilinear_interpolation_cuda.apply(feats2, points)</span></span></code></pre><button class="code-block-unfold-btn"></button></div><!--]--><!--]--><!----><!----></article><!-- </Transition> --><!--]--><!--[--><!--[--><!--[--><!----><ul class="post-copyright" m="y-4"><li class="post-copyright-author"><strong>本文作者：</strong><span>水沢绫音</span></li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://pat-chou-li.github.io/ayene-no-blog/posts/%EF%BC%88AI%E8%91%B5%EF%BC%89cuda%E6%95%99%E5%AD%A6" target="_blank" title="本文链接">https://pat-chou-li.github.io/ayene-no-blog/posts/（AI葵）cuda教学</a></li><li class="post-copyright-license"><strong>版权声明：</strong><span>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 ">CC BY-NC-SA</a> 许可协议。</span></li></ul><!--]--><!--]--><!--]--></div><!--]--><!----></div><!--[--><!--]--><!--[--><div class="post-nav"><div class="post-nav-item"><a href="/ayene-no-blog/posts/%E5%88%86%E5%89%B2%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E8%AE%B0%E5%BD%95" class="post-nav-prev" title="分割论文复现记录"><div class="icon" i-ri-arrow-left-s-line></div><span class="title truncate" text="sm">分割论文复现记录</span></a></div><div class="post-nav-item"><a href="/ayene-no-blog/posts/%E9%9A%8F%E7%AC%94/虚拟人前置知识" class="post-nav-next" title="虚拟人前置知识"><span class="title truncate" text="sm">虚拟人前置知识</span><div class="icon" i-ri-arrow-right-s-line></div></a></div></div><!--]--><!--[--><!--]--><!----><!--[--><!-- eslint-disable-next-line vue/no-lone-template empty --><template class="mt-4"></template><!--]--><!--[--><!--]--><!--[--><!--]--></div><!--]--></main><!--[--><button class="xl:hidden toc-btn shadow-md fixed yun-icon-btn z-20 bg-$va-c-bg-soft" opacity="75" right="4" bottom="19"><div i-ri-file-list-line></div></button><!----><aside flex="~ col" class="va-card yun-aside sticky top-0 lg:top-$yun-margin-top min-h-sm" text="center" overflow="auto"><div style="display:none" class="w-full" flex="~ col" pb-2><!--[--><h2 m="t-6 b-2" font="serif black">文章目录</h2><div style="display:none" data-v-6dc67c73><div class="content" data-v-6dc67c73><div class="outline-title" data-v-6dc67c73>本页</div><div class="outline-marker" data-v-6dc67c73></div><nav aria-labelledby="doc-outline-aria-label" data-v-6dc67c73><span id="doc-outline-aria-label" class="visually-hidden" data-v-6dc67c73>Table of Contents for current page</span><ul class="root va-toc relative z-1 css-i18n-toc" data-v-6dc67c73 data-v-699db71a><!--[--><!--]--></ul></nav></div></div><!--]--><div class="flex-grow"></div><!----></div></aside><!--]--><!--]--></div><footer flex="~ col" class="relative yun-footer va-footer px-4 py-4 pt-0 text-$va-c-text-light w-full mt-14" bg="white dark:$va-c-bg-soft" text="center sm"><div class="yun-cloud absolute top--10 left-0 right-0"><svg class="waves" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z" fill="var(--yun-c-cloud)"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><!----><div class="copyright flex justify-center items-center gap-2" p="1"><span>©<!--[--> 2022 -<!--]--> 2025</span><a class="animate-pulse inline-flex" href="https://www.yunyoujun.cn/sponsors/" target="_blank" title="Sponsor YunYouJun"><div class="i-ri-cloud-line"></div></a><span>水沢绫音</span></div><div class="powered" m="2"><span>由 <a href="git+https://github.com/YunYouJun/valaxy.git" target="_blank" rel="noopener">Valaxy</a> <span class="op-60">v0.26.3</span> 驱动</span><span mx-1>|</span><span><span>主题</span><span mx-1>-</span><a href="git+https://github.com/YunYouJun/valaxy/tree/main/packages/valaxy-theme-yun.git" title="valaxy-theme-yun" target="_blank">Yun</a><span class="ml-1 op-60">v0.26.3</span></span></div><!--[--><!--]--><div class="yun-footer-gradient" style="--gradient-from:161 196 253;--gradient-to:194 233 251"></div></footer><!--]--><!--]--></div><script>window.__INITIAL_STATE__='{"pinia":{"yun-app":{},"app":{"showLoading":true},"site":{},"routerStore":{}}}'</script><script type="application/ld+json" data-hid="schema-org-graph">{"@context":"https://schema.org","@graph":[{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#identity","@type":"Person","name":"水沢绫音","url":"https://pat-chou-li.github.io/ayene-no-blog/","image":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#/schema/image/658d95f"},"sameAs":["/atom.xml","https://github.com/pat-chou-li","https://www.zhihu.com/people/shui-ze-ling-yin-46","https://space.bilibili.com/8929945?spm_id_from=333.1007.0.0","patchouli13@163.com"]},{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#website","@type":"WebSite","dateModified":"2025-08-22T09:02:38.876Z","datePublished":"2024.3.3","inLanguage":"en","name":"（AI葵）cuda教学","url":"https://pat-chou-li.github.io/ayene-no-blog/","publisher":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#identity"}},{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#webpage","@type":"WebPage","dateModified":"2025-08-22T09:02:38.876Z","datePublished":"2024-03-03T00:00:00.000Z","description":"我从来没有觉得学图形学开心过。","name":"（AI葵）cuda教学","url":"https://pat-chou-li.github.io/ayene-no-blog/","about":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#identity"},"isPartOf":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#website"},"potentialAction":[{"@type":"ReadAction","target":["https://pat-chou-li.github.io/ayene-no-blog/"]}]},{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#article","dateModified":"2025-08-22T09:02:38.876Z","datePublished":"2024-03-03T00:00:00.000Z","description":"我从来没有觉得学图形学开心过。","headline":"（AI葵）cuda教学","inLanguage":"en","thumbnailUrl":"https://pat-chou-li.github.io/ayene-no-blog/favicon.svg","@type":["Article","BlogPosting"],"author":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#/schema/person/6678b6b"},"image":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#/schema/image/19d7713"},"isPartOf":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#webpage"},"mainEntityOfPage":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#webpage"},"publisher":{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#identity"}},{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#/schema/person/6678b6b","@type":"Person","name":"水沢绫音","url":"https://valaxy.site"},{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#/schema/image/658d95f","@type":"ImageObject","contentUrl":"https://raw.githubusercontent.com/pat-chou-li/ayene-no-blog/main/resource/avatar.jpg","inLanguage":"en","url":"https://raw.githubusercontent.com/pat-chou-li/ayene-no-blog/main/resource/avatar.jpg"},{"@id":"https://pat-chou-li.github.io/ayene-no-blog/#/schema/image/19d7713","@type":"ImageObject","contentUrl":"https://pat-chou-li.github.io/ayene-no-blog/favicon.svg","inLanguage":"en","url":"https://pat-chou-li.github.io/ayene-no-blog/favicon.svg"}]}</script></body></html>